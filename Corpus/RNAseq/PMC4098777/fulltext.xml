<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="product-review" xml:lang="en"><?DTDIdentifier.IdentifierValue http://www.biomedcentral.com/xml/article.dtd?><?DTDIdentifier.IdentifierType system?><?SourceDTD.DTDName article.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName bmc2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id><journal-title-group><journal-title>BMC Bioinformatics</journal-title></journal-title-group><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4098777</article-id><article-id pub-id-type="publisher-id">1471-2105-15-116</article-id><article-id pub-id-type="pmid">24766777</article-id><article-id pub-id-type="doi">10.1186/1471-2105-15-116</article-id><article-categories><subj-group subj-group-type="heading"><subject>Software</subject></subj-group></article-categories><title-group><article-title>ShrinkBayes: a versatile R-package for analysis of count-based sequencing data in complex study designs</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="A1"><name><surname>van de Wiel</surname><given-names>Mark A</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>mark.vdwiel@vumc.nl</email></contrib><contrib contrib-type="author" id="A2"><name><surname>Neerincx</surname><given-names>Maarten</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>m.neerincx@vumc.nl</email></contrib><contrib contrib-type="author" id="A3"><name><surname>Buffart</surname><given-names>Tineke E</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>T.Buffart@vumc.nl</email></contrib><contrib contrib-type="author" id="A4"><name><surname>Sie</surname><given-names>Daoud</given-names></name><xref ref-type="aff" rid="I4">4</xref><email>d.sie@vumc.nl</email></contrib><contrib contrib-type="author" id="A5"><name><surname>Verheul</surname><given-names>Henk MW</given-names></name><xref ref-type="aff" rid="I3">3</xref><email>h.verheul@vumc.nl</email></contrib></contrib-group><aff id="I1"><label>1</label>Department of Epidemiology and Biostatistics, VU University medical center, De Boelelaan 1117, 1081 HV Amsterdam, The Netherlands</aff><aff id="I2"><label>2</label>Department of Mathematics, VU University, De Boelelaan 1081a, 1081 HV Amsterdam, The Netherlands</aff><aff id="I3"><label>3</label>Department of Medical Oncology, VU University medical center, De Boelelaan 1117, 1081 HV Amsterdam, The Netherlands</aff><aff id="I4"><label>4</label>Department of Pathology, VU University medical center, De Boelelaan 1117, 1081 HV Amsterdam, The Netherlands</aff><pub-date pub-type="collection"><year>2014</year></pub-date><pub-date pub-type="epub"><day>26</day><month>4</month><year>2014</year></pub-date><volume>15</volume><fpage>116</fpage><lpage>116</lpage><history><date date-type="received"><day>24</day><month>9</month><year>2013</year></date><date date-type="accepted"><day>11</day><month>4</month><year>2014</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2014 van de Wiel et al.; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>van de Wiel et al.; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><self-uri xlink:href="http://www.biomedcentral.com/1471-2105/15/116"/><abstract><sec><title>Background</title><p>Complex designs are common in (observational) clinical studies. Sequencing data for such studies are produced more and more often, implying challenges for the analysis, such as excess of zeros, presence of random effects and multi-parameter inference. Moreover, when sample sizes are small, inference is likely to be too liberal when, in a Bayesian setting, applying a non-appropriate prior or to lack power when not carefully borrowing information across features.</p></sec><sec><title>Results</title><p>We show on microRNA sequencing data from a clinical cancer study how our software <monospace>ShrinkBayes</monospace> tackles the aforementioned challenges. In addition, we illustrate its comparatively good performance on multi-parameter inference for groups using a data-based simulation. Finally, in the small sample size setting, we demonstrate its high power and improved FDR estimation by use of Gaussian mixture priors that include a point mass.</p></sec><sec><title>Conclusion</title><p><monospace>ShrinkBayes</monospace> is a versatile software package for the analysis of count-based sequencing data, which is particularly useful for studies with small sample sizes or complex designs.</p></sec></abstract><kwd-group><kwd>Differential expression</kwd><kwd>Shrinkage</kwd><kwd>Sequencing</kwd><kwd>Bayesian analysis</kwd></kwd-group></article-meta></front><body><sec><title>Background</title><p>Following the surge of count-based sequencing data, a plethora of software packages for differential expression analysis of such data has emerged [<xref ref-type="bibr" rid="B1">1</xref>]. Many of these methods are limited in use due to restrictions on the study design, the model and inference like a) 2- or <italic>K </italic>- group comparisons only; b) no random effects; c) no explicit solution for excess of zeros and d) no multi-parameter inference. We introduced <monospace>ShrinkBayes</monospace> as a versatile analysis method which allows generalized linear mixed models and zero-inflation and with, due to its multi-parameter shrinkage options, good reproducibility and power characteristics [<xref ref-type="bibr" rid="B2">2</xref>]. This paper illustrates the R-package <monospace>ShrinkBayes</monospace> on a challenging microRNA sequencing (miRseq) colon tumor-plus-metastasis study. In addition, we automated the use of mixture priors containing a spike, leading to improved FDR-based inference. Finally, we extend the class of admitted priors with mixtures of a multivariate point mass and a Gaussian product density to allow for powerful multi-parameter inference.</p></sec><sec><title>Implementation</title><sec><title>Shrinkage</title><p><monospace>ShrinkBayes</monospace> applies Integrated Nested Laplace Approximation, <monospace>INLA</monospace>[<xref ref-type="bibr" rid="B3">3</xref>], in combination with Empirical Bayes principles to provide shrunken parameter estimates and inference. In a Bayesian setting, multi-parameter shrinkage is effectuated by estimating hyper-parameters of priors. The core of <monospace>ShrinkBayes</monospace> is iterative estimation of priors: each prior is fit to the point-wise empirical mean of the <italic>marginal</italic> posteriors of those parameters <italic>&#x003b8;</italic><sub><italic>i</italic>
</sub>,<italic>i </italic>= 1,&#x02026;,<italic>p </italic>= # features, that correspond to the prior [<xref ref-type="bibr" rid="B2">2</xref>]. Shrinkage is known to be potentially beneficial for dispersion parameters, but may be as important for parameters of interest to accomplish better inference [<xref ref-type="bibr" rid="B2">2</xref>] and for nuisance parameters to reduce their impact when unimportant [<xref ref-type="bibr" rid="B4">4</xref>].</p><p>A typical <monospace>ShrinkBayes</monospace> analysis consists of the following modules: a) Iterative Empirical Bayes estimation of multiple priors which need to obey the parametric forms included in <monospace>INLA</monospace>; b) Fitting of the full model and the null model; c) Updating one prior resulting from a) to a non-parametric or mixture prior to allow for for more flexibility and/or better inferential properties; d) Updating the posteriors of the corresponding parameters; e) Computing summary statistics including estimates of lfdr and (B)FDR. The steps are detailed in the Example section. Below we discuss novel implementations and methods with respect to [<xref ref-type="bibr" rid="B2">2</xref>].</p></sec><sec><title>Setting</title><p>The setting is a generalized linear model. Let <italic>j </italic>= 1,&#x02026;,<italic>n </italic>denote independent samples, <italic>Y</italic><sub>
<italic>ij </italic>
</sub>be the data for feature <italic>i </italic>and sample <italic>j</italic>, <italic>F </italic>be the likelihood model (e.g. (zero-inflated) negative binomial) with mean <italic>&#x003bc;</italic><sub>
<italic>ij </italic>
</sub>and hyper-parameters <bold>
<italic>&#x003b3;</italic>
</bold><sub>
<italic>i </italic>
</sub>and <italic>g </italic>() a link-function. Here, <bold>
<italic>&#x003b3;</italic>
</bold><sub>
<italic>i </italic>
</sub>contains distribution parameters that are not linked to covariates, e.g. zero-inflation and over-dispersion. Then, </p><p><disp-formula id="bmcM1"><label>(1)</label><mml:math id="M1" name="1471-2105-15-116-i1" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mo>=</mml:mo></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo>&#x003bc;</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="bold-italic">&#x003b3;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo>&#x003bc;</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x003b1;</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo mathvariant="bold-italic">&#x003b1;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x003b2;</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i </italic>
</sub>= (<italic>&#x003b2;</italic><sub>
<italic>i</italic>1</sub>,&#x02026;,<italic>&#x003b2;</italic><sub>
<italic>iK </italic>
</sub>) denotes the parameter(s) for which (joint) inference is desired, while <bold>
<italic>&#x003b1;</italic>
</bold><sub>
<italic>i </italic>
</sub>contains all the other regression parameters, including the intercept. In addition, <inline-formula><mml:math id="M2" name="1471-2105-15-116-i2" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x003b1;</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> (<inline-formula><mml:math id="M3" name="1471-2105-15-116-i3" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x003b2;</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>) denotes the <italic>j</italic>th row of the design matrix restricted to those columns of this matrix that are relevant for <bold>
<italic>&#x003b1;</italic>
</bold><sub>
<italic>i </italic>
</sub>(<bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i</italic>
</sub>).</p></sec><sec><title>Priors</title><p><monospace>ShrinkBayes </monospace>inherits much of its flexibility from the <monospace>INLA</monospace> R-package, including its ability to deal with arbitrary designs and random effects. <monospace>INLA</monospace>, however, requires use of specific parametric priors. Since the prior may be crucial for inference in a multiple testing setting, we extended the class of admissible priors to non-parametric and parametric mixture priors [<xref ref-type="bibr" rid="B2">2</xref>].</p><p><monospace>ShrinkBayes</monospace> was praised for its power and versatility, but also criticized for its poor FDR estimation in case of a point null-hypothesis for one parameter (so <bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i </italic>
</sub>= <italic>&#x003b2;</italic><sub>
<italic>i</italic>
</sub>), <italic>H</italic><sub>0 <italic>i </italic>
</sub>: <italic>&#x003b2;</italic><sub>
<italic>i </italic>
</sub>= 0 against <italic>H</italic><sub>1 <italic>i </italic>
</sub>: <italic>&#x003b2;</italic><sub>
<italic>i </italic>
</sub>&#x02260; 0 [<xref ref-type="bibr" rid="B1">1</xref>]. Here, we resolve this issue. In [<xref ref-type="bibr" rid="B1">1</xref>], a smooth non-parametric prior was used for <italic>&#x003b2;</italic><sub>
<italic>i</italic>
</sub>, which does not suit <italic>H </italic><sub>0 <italic>i</italic>
</sub>. To promote more suitable priors, we simplified application of parametric mixture priors with a spike on zero by automating multi-grid parameter estimation of such priors, and increased their flexibility by allowing non-equal mixture proportions for negative and positive effects. Moreover, we implemented a mixture of a spike and a smooth non-parametric component (SpNP prior). For the Results, we focus on the Spike-Gauss-Gauss (SPGG) and SpNP priors: </p><p><disp-formula id="bmcM2"><label>(2)</label><mml:math id="M4" name="1471-2105-15-116-i4" overflow="scroll"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mtext>SpGG</mml:mtext></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#x003b4;</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mo>-</mml:mo><mml:mo>&#x003bc;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>&#x003c4;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mo>&#x003bc;</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mo>&#x003c4;</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mspace width="1em"/><mml:mtext>&#x02009;Spike-Gauss-Gauss&#x02019;</mml:mtext><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p><disp-formula id="bmcM3"><label>(3)</label><mml:math id="M5" name="1471-2105-15-116-i5" overflow="scroll"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mtext>SpNP</mml:mtext></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#x003b4;</mml:mo><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mtext>NP</mml:mtext></mml:mrow></mml:msub><mml:mspace width="2em"/></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mspace width="1em"/><mml:mtext>&#x02009;Spike-Nonparametric&#x02019;</mml:mtext><mml:mo>,</mml:mo><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <italic>&#x003b4; </italic>() is the dirac delta function, i.e. a spike. The spike is essential, because it allows the posteriors to have non-zero mass on the null-hypothesis, <italic>&#x003b2;</italic><sub>
<italic>i </italic>
</sub>= 0, hence accommodating selection. The smooth parts of both these priors allow asymmetry between under and overexpression. All parameters are determined by maximizing the total (log-) marginal likelihood (i.e. the sum of marginal likelihoods over all features). This maximization is explicit for the parametric SpGG prior, whereas <italic>F</italic><sub>NP</sub> is obtained by the iterative marginal procedure [<xref ref-type="bibr" rid="B2">2</xref>] with the restriction that it contains maximally one mode on both the negative and positive half-plane. The restriction helps to identify <italic>F</italic><sub>NP </sub>together with <italic>p</italic><sub>0</sub>. In words, given a current proposal for <italic>p</italic><sub>0 </sub>and <italic>F</italic><sub>NP </sub>the iterative procedure proposes a new estimate of <italic>p</italic><sub>0 </sub>and <italic>F</italic><sub>NP </sub>by fitting the SpNP prior to the point-wise empirical mean (over features <italic>i</italic>=1,&#x02026;,<italic>p</italic>) of the current posteriors <italic>&#x003c0;</italic>(<italic>&#x003b2;</italic><sub>
<italic>i</italic>
</sub>|<bold>Y</bold><sub>
<italic>i</italic>
</sub>), where the fit needs to respect the aforementioned restriction. Any reasonable starting value of <italic>p</italic><sub>0 </sub>(we use 0.8) and <italic>F</italic><sub>NP </sub>(we use a sufficiently vague central Gaussian, e.g. <italic>N</italic>(0,5)) can be used and convergence is checked by assessing the total (log-)marginal likelihood.</p><p><monospace>ShrinkBayes</monospace> allows for other parametric priors, such as the &#x02009;Spike-Gauss&#x02019; (SpG) and the &#x02009;Spike-and-Slab&#x02019; (SpSlab). Both are mixtures of a point mass and a central Gaussian distribution, but the first has a data-adaptive variance fitted with the same direct maximization procedure as for the SpGG prior, whereas the latter has a prescribed large variance. Both alternatives are discussed in more detail in the Additional file&#x000a0;<xref ref-type="supplementary-material" rid="S1">1</xref>.</p></sec><sec><title>Multi-parameter inference</title><p>Multi-parameter inference is desirable when the parameters represent multiple groups or covariates with a similar interpretation. In a frequentist setting, this is often done by likelihood-ratio tests. Below we discuss the Bayesian counterpart. Suppose one aims at testing <italic>H</italic><sub>0<italic>i </italic>
</sub>: <bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i </italic>
</sub>= <bold>0 </bold>against <italic>H</italic><sub>1 <italic>i </italic>
</sub>: <bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i </italic>
</sub>&#x02260; <bold>0 </bold>in a linear model <italic>M </italic>(<bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i</italic>
</sub>), which also includes response <bold>Y</bold><sub>
<italic>i</italic>
</sub>, covariates <bold>X </bold>and, possibly, additional parameters <bold>
<italic>&#x003bb;</italic>
</bold><sub>
<italic>i</italic>
</sub>. Refer to the full model <inline-formula><mml:math id="M6" name="1471-2105-15-116-i6" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> when <bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i </italic>
</sub>is unconstrained and the null model <inline-formula><mml:math id="M7" name="1471-2105-15-116-i7" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>(</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>. Traditionally, comparison of two models is done by computation of the Bayes Factor (BF). However, in a multiple testing setting a good threshold for BF requires knowing <italic>p</italic><sub>0</sub>, the proportion of true null models (see [<xref ref-type="bibr" rid="B5">5</xref>], Ch. 5). Then, thresholding for BF is directly linked to local fdr, which simply equals </p><p><disp-formula id="bmcM4"><label>(4)</label><mml:math id="M8" name="1471-2105-15-116-i8" overflow="scroll"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>lfdr</mml:mtext></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>&#x003c0;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="M9" name="1471-2105-15-116-i9" overflow="scroll"><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="M10" name="1471-2105-15-116-i10" overflow="scroll"><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> are the marginal likelihoods under <inline-formula><mml:math id="M11" name="1471-2105-15-116-i11" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M12" name="1471-2105-15-116-i12" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, respectively. On its turn, lfdr determines BFDR(<italic>t</italic>,<bold>Y</bold><sub>
<italic>i</italic>
</sub>) = <italic>E</italic>[lfdr|lfdr &#x0003c; <italic>t</italic>] : the mean of all local fdrs smaller than <italic>t</italic>. Given its analogous interpretation to ordinary FDR [<xref ref-type="bibr" rid="B6">6</xref>] we prefer to define threshold <italic>t </italic>using BFDR(<italic>t</italic>,<bold>Y</bold><sub>
<italic>i</italic>
</sub>) rather than lfdr. In any case, we need to compute <inline-formula><mml:math id="M13" name="1471-2105-15-116-i13" overflow="scroll"><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> and <italic>p</italic><sub>0</sub>.</p><p>The marginal likelihoods <inline-formula><mml:math id="M14" name="1471-2105-15-116-i14" overflow="scroll"><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> and <inline-formula><mml:math id="M15" name="1471-2105-15-116-i15" overflow="scroll"><mml:mtext>ML</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> are conveniently supplied by INLA from the two separate fits of the models <inline-formula><mml:math id="M16" name="1471-2105-15-116-i16" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M17" name="1471-2105-15-116-i17" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Finally, <italic>p</italic><sub>0 </sub>is determined by our <italic>iterative joint procedure</italic>[<xref ref-type="bibr" rid="B2">2</xref>], which determines the value of <italic>p</italic><sub>0 </sub>(along with other parameters) that maximizes the total (log-)marginal likelihood with respect to prior: </p><p><disp-formula id="bmcM5"><label>(5)</label><mml:math id="M18" name="1471-2105-15-116-i18" overflow="scroll"><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>&#x003b4;</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:munderover><mml:mrow><mml:mo mathsize="big">&#x0220f;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mo>&#x003b2;</mml:mo></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ik</mml:mtext></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>hence a mixture of a multivariate point-mass (<italic>&#x003b4;</italic>(<bold>
<italic>&#x003b2; </italic>
</bold>= <bold>0</bold>)) and a Gaussian product density for the regression parameters <bold>
<italic>&#x003b2;</italic>
</bold><sub>
<italic>i </italic>
</sub>= (<italic>&#x003b2;</italic><sub>
<italic>i </italic>1</sub>,&#x02026;,<italic>&#x003b2;</italic><sub>
<italic>iK </italic>
</sub>). In particular when the true <italic>p</italic><sub>0</sub> is large, the total (log-)marginal likelihood may contain ridges and/or multiple modalities with respect to the parameters of (5). For example, when the true <italic>p</italic><sub>0 </sub>is large a prior (5) with <italic>small</italic><inline-formula><mml:math id="M19" name="1471-2105-15-116-i19" overflow="scroll"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and small values of <italic>&#x003c3;</italic><sub>
<italic>k </italic>
</sub>may also fit rather well. To counter this, we use the constraint <italic>p</italic><sub>0 </sub>&#x02265; 0.5 (which is realistic in most cases) and use a large default starting value of <italic>p</italic><sub>0 </sub>(0.8). Moreover, iteration is stopped when the total (log-)marginal likelihood decreases by less than 0.1% to avoid &#x02009;walking on a ridge&#x02019;.</p></sec><sec><title>Additional changes</title><p>In addition to the improved implementation of spike-priors and the multi-parameter inference, <monospace>ShrinkBayes</monospace> versions 2.3 and higher contain a number of novelties and changes compared to version 1.6, which corresponds to [<xref ref-type="bibr" rid="B2">2</xref>]. In particular, it is faster, because convergence of the parameters of the prior(s) is assessed in terms of total marginal likelihood instead of on the separate parameters. The new version also allows to approximate marginal likelihood for a null model from the results of the full model using the Savage-Dickey approximation [<xref ref-type="bibr" rid="B7">7</xref>]. This is particulary convenient for contrasts for which a null-model can not be defined without the use of constraints. Additional file <xref ref-type="supplementary-material" rid="S1">1</xref>, Section 2, contains more details and a full list of changes.</p></sec></sec><sec sec-type="results"><title>Results</title><sec><title>Priors</title><p>To study which of the priors performs best in terms of FDR estimation and power, we compared them on simulated data sets, including those in [<xref ref-type="bibr" rid="B1">1</xref>].</p><sec><title><bold>
<italic>Results on simulations for various effect size distributions</italic>
</bold></title><p>The true effect size distribution, i.e. the true generating distribution of the parameter of interest, may have impact on what prior performs best. Hence, we study several effect size distributions, including a Gamma, <italic>t</italic>, Uniform and Gaussian mixture (see Additional file&#x000a0;<xref ref-type="supplementary-material" rid="S1">1</xref>, Section 1). We compared performance of the SpGG, SpNP, SpG and SpSlab priors in terms of accuracy of FDR estimation, area-under-the-curve (AUC), number of detections and absence of detections when <italic>H</italic><sub>0<italic>i </italic>
</sub>is true for all features (<italic>p</italic><sub>0 </sub>= 1). From the results (Additional file&#x000a0;<xref ref-type="supplementary-material" rid="S1">1</xref>, Section 1) we conclude that SpGG and SpNP lead to accurate estimates of FDR and are very competitive in terms of power, whereas SpSlab is often too conservative; SpG generally performs well except for the (asymmetric) Gamma distribution for which it is less powerful than SpGG and SpNP. In the case <italic>p</italic><sub>0 </sub>= 1, none of the prior returns a significant result at BFDR&#x02264;0.1, but the SpGG prior performs best in the sense that it produces the highest BFDRs.</p></sec><sec><title><bold>
<italic>Results on simulations in </italic>
</bold>[<xref ref-type="bibr" rid="B1">1</xref>]</title><p>Next, we report results of <monospace>ShrinkBayes</monospace> with the SpGG and SpNP priors on simulations in [<xref ref-type="bibr" rid="B1">1</xref>], which compared several methods, including <monospace>ShrinkBayes</monospace> (referred to as <monospace>ShrinkSeq</monospace>), on a variety of data sets. <monospace>ShrinkBayes</monospace> was used with a smooth non-parametric prior (NP), so not containing a spike. The number of features equals 12500. We focus on data sets where counts are exclusively generated from the negative binomial. Moreover, we report results on the symmetric cases (in terms of up- and down-regulation) only (<inline-formula><mml:math id="M20" name="1471-2105-15-116-i20" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, <italic>p</italic><sub>0 </sub>= 0.64 and <inline-formula><mml:math id="M21" name="1471-2105-15-116-i21" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>, <italic>p</italic><sub>0 </sub>= 0.9), because for the asymmetric cases the normalization procedure used in [<xref ref-type="bibr" rid="B1">1</xref>] introduces artificial differential signal for the non-differential features. We do include a case with outliers which contains, on average, 5% outliers for 10% of the features (<inline-formula><mml:math id="M22" name="1471-2105-15-116-i22" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>S</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></inline-formula>). For sample sizes we focus on <italic>n </italic>= <italic>N</italic>/2 = 5,10, because the <monospace>ShrinkBayes</monospace> results reported in [<xref ref-type="bibr" rid="B1">1</xref>] were relatively worse for those sample sizes.</p><p>Table&#x000a0;<xref ref-type="table" rid="T1">1</xref> contains the results on FDR estimation. Note that the target FDR equals 0.05 here, in order to be consistent with [<xref ref-type="bibr" rid="B1">1</xref>]. We observe that <monospace>ShrinkBayes</monospace> with SpGG or SpNP is still liberal, but the results are much better than those for the NP prior. In fact, when comparing the results of Table&#x000a0;<xref ref-type="table" rid="T1">1</xref> with those in Figure four of [<xref ref-type="bibr" rid="B1">1</xref>], we observe that <monospace>ShrinkBayes</monospace> has improved from the worse to at least average in terms of FDR estimation. In particular, for the data sets with outliers it outperforms 5/6 (4/6) [ <italic>n </italic>= 5(10)] of the other methods that are based on count distributions.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>FDR results for target FDR=0.05</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/></colgroup><thead valign="top"><tr><th align="right"><bold>Data set</bold></th><th align="right"><bold>
<italic>n </italic>
</bold><bold>= </bold><bold>
<italic>N </italic>
</bold><bold>/2</bold></th><th align="right"><bold>SpGG</bold></th><th align="right"><bold>SpNP</bold></th><th align="right"><bold>NP</bold><sup>
<bold>&#x02217;</bold>
</sup></th></tr></thead><tbody valign="top"><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M23" name="1471-2105-15-116-i23" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">5<hr/></td><td align="right" valign="bottom">0.085<hr/></td><td align="right" valign="bottom">0.078<hr/></td><td align="right" valign="bottom">0.29<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M24" name="1471-2105-15-116-i24" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">10<hr/></td><td align="right" valign="bottom">0.079<hr/></td><td align="right" valign="bottom">0.071<hr/></td><td align="right" valign="bottom">0.29<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M25" name="1471-2105-15-116-i25" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">5<hr/></td><td align="right" valign="bottom">0.115<hr/></td><td align="right" valign="bottom">0.115<hr/></td><td align="right" valign="bottom">0.37<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M26" name="1471-2105-15-116-i26" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">10<hr/></td><td align="right" valign="bottom">0.083<hr/></td><td align="right" valign="bottom">0.081<hr/></td><td align="right" valign="bottom">0.38<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M27" name="1471-2105-15-116-i27" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>S</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">5<hr/></td><td align="right" valign="bottom">0.111<hr/></td><td align="right" valign="bottom">0.108<hr/></td><td align="right" valign="bottom">0.38<hr/></td></tr><tr><td align="right"><disp-formula><mml:math id="M28" name="1471-2105-15-116-i28" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>S</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula></td><td align="right">10</td><td align="right">0.119</td><td align="right">0.117</td><td align="right">0.40</td></tr></tbody></table><table-wrap-foot><p><sup>&#x02217;</sup>: as reported in [<xref ref-type="bibr" rid="B1">1</xref>].</p></table-wrap-foot></table-wrap><p>Table&#x000a0;<xref ref-type="table" rid="T2">2</xref> contains the results on AUC. Again, we observe a uniform improvement when using <monospace>ShrinkBayes</monospace> with SpGG or SpNP instead of NP. Strikingly, <monospace>ShrinkBayes</monospace> with both SpGG and SpNP generally outperforms all the other methods reported in [<xref ref-type="bibr" rid="B1">1</xref>] when it comes to AUC.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Area-under-the-curves</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/><col align="center"/></colgroup><thead valign="top"><tr><th align="right"><bold>Data set</bold></th><th align="right"><bold>
<italic>n</italic>
</bold><bold>= </bold><bold>
<italic>N </italic>
</bold><bold>/2</bold></th><th align="right"><bold>SpGG</bold></th><th align="right"><bold>SpNP</bold></th><th align="right"><bold>NP</bold><sup>
<bold>&#x02217;</bold>
</sup></th><th align="right"><bold>Best</bold><sup>
<bold>&#x02217;</bold>
</sup></th></tr></thead><tbody valign="top"><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M29" name="1471-2105-15-116-i29" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">5<hr/></td><td align="right" valign="bottom">0.897<hr/></td><td align="right" valign="bottom">0.898<hr/></td><td align="right" valign="bottom">0.85<hr/></td><td align="right" valign="bottom">0.87<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M30" name="1471-2105-15-116-i30" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow><mml:mrow><mml:mn>2000</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">10<hr/></td><td align="right" valign="bottom">0.949<hr/></td><td align="right" valign="bottom">0.951<hr/></td><td align="right" valign="bottom">0.91<hr/></td><td align="right" valign="bottom">0.93<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M31" name="1471-2105-15-116-i31" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">5<hr/></td><td align="right" valign="bottom">0.874<hr/></td><td align="right" valign="bottom">0.879<hr/></td><td align="right" valign="bottom">0.82<hr/></td><td align="right" valign="bottom">0.87<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M32" name="1471-2105-15-116-i32" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>B</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">10<hr/></td><td align="right" valign="bottom">0.937<hr/></td><td align="right" valign="bottom">0.940<hr/></td><td align="right" valign="bottom">0.88<hr/></td><td align="right" valign="bottom">0.93<hr/></td></tr><tr><td align="right" valign="bottom"><disp-formula><mml:math id="M33" name="1471-2105-15-116-i33" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>S</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula><hr/></td><td align="right" valign="bottom">5<hr/></td><td align="right" valign="bottom">0.866<hr/></td><td align="right" valign="bottom">0.871<hr/></td><td align="right" valign="bottom">0.81<hr/></td><td align="right" valign="bottom">0.85<hr/></td></tr><tr><td align="right"><disp-formula><mml:math id="M34" name="1471-2105-15-116-i34" overflow="scroll"><mml:msubsup><mml:mrow><mml:mtext>S</mml:mtext></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow><mml:mrow><mml:mn>625</mml:mn></mml:mrow></mml:msubsup></mml:math></disp-formula></td><td align="right">10</td><td align="right">0.923</td><td align="right">0.927</td><td align="right">0.87</td><td align="right">0.92</td></tr></tbody></table><table-wrap-foot><p><sup>&#x02217;</sup>: as reported in [<xref ref-type="bibr" rid="B1">1</xref>]. Best<sup>&#x02217;</sup>: Highest AUC of all other methods reported in [<xref ref-type="bibr" rid="B1">1</xref>].</p></table-wrap-foot></table-wrap></sec></sec><sec><title>Multi-parameter inference: data-based simulation</title><p>We compare our solution for multi-parameter inference to the likelihood-ratio tests that are implemented in the popular RNAseq data analysis programs <monospace>edgeR</monospace>[<xref ref-type="bibr" rid="B8">8</xref>] and <monospace>DESeq</monospace>[<xref ref-type="bibr" rid="B9">9</xref>]. We believe such a comparison is most meaningful and fair when the data is simulated in a relevant and realistic way, preferably avoiding distributional assumptions as much as possible. Therefore, we generated the data in three steps. First, we create a realistic null data set: we simply re-sample 3*5 &#x02009;observations&#x02019; from our miRseq data set, independently for each of the 2060 features. Hence, per feature 5 observations are generated from the same empirical distribution for each of the 3 groups. Next, modest filtering on the number of non-zeros is applied, because this is recommended for the use of edgeR and DESeq: at least 3 non-zeros should be present. Finally, we need a realistic effect size distribution for the features. To avoid parametric assumptions this is estimated by <italic>F</italic><sub>NP</sub>, the smooth component of the SpNP prior (3), for the groups in the miRseq study (organs). We create 20% differential features by sampling independently from <italic>F</italic><sub>NP</sub> for groups 2 and 3 and multiplying the respective counts by the exponentiated sampled effect sizes. This entire simulation was repeated 10 times.</p><p>We analyzed the simulated data sets using <monospace>ShrinkBayes</monospace>, <monospace>edgeR</monospace>, <monospace>DESeq</monospace> and a simple nonparametric Kruskal-Wallis test. In addition, the old version of <monospace>ShrinkBayes</monospace> was applied with a smooth nonparametric prior and an <italic>a posteriori</italic> multiple comparison of the 3 groups, as suggested in [<xref ref-type="bibr" rid="B2">2</xref>]. Figure&#x000a0;<xref ref-type="fig" rid="F1">1</xref> shows the ROC curves, as averaged over the 10 repeats, for False Positive Rate (FPR) smaller than 0.05. We focus on this FPR range, because when using <inline-formula><mml:math id="M35" name="1471-2105-15-116-i35" overflow="scroll"><mml:mover accent="false"><mml:mrow><mml:mtext>FDR</mml:mtext></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>&#x02264;</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula> as a selection criterion, all 5 methods produce sets of significant features with FPR&#x02264;0.05. <monospace>ShrinkBayes</monospace> seems somewhat superior to <monospace>edgeR</monospace> across the entire range, while it is competitive with <monospace>DESeq</monospace>. Possibly due to the smoothness of the prior <monospace>ShrinkBayes,Old</monospace> performs a little bit better in terms of ranking than <monospace>ShrinkBayes</monospace> for very small FPR, but becomes inferior for larger values. The latter may be caused by loss of power when using a multiple comparison approach in a <italic>K</italic>-group setting. Surprisingly, the Kruskal-Wallis test seems to be very competitive, although it also loses power for larger values of the FPR.</p><fig id="F1" position="float"><label>Figure 1</label><caption><p>ROC curves for multi-parameter inference: mean False Positive Rate (FPR; x-axis) vs mean True Positive Rate (TPR; y-axis), as averaged over 10 repeats of the data-based simulation, which consists of 3 groups with 5 counts for &#x02248;2000 features.</p></caption><graphic xlink:href="1471-2105-15-116-1"/></fig><p>ROC curves, however, only allow comparison of the rankings. In practice, the actual selection is most important. Table&#x000a0;<xref ref-type="table" rid="T3">3</xref> shows the results summarized over the 10 repeats when using <inline-formula><mml:math id="M36" name="1471-2105-15-116-i36" overflow="scroll"><mml:mover accent="false"><mml:mrow><mml:mtext>FDR</mml:mtext></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>&#x02264;</mml:mo><mml:mn>0.1</mml:mn></mml:math></inline-formula> as selection criterion. Note that for all p-value-based methods we use the Benjamini-Hochberg FDR correction, which is appropriate here given the independent sampling per feature in our simulated data set. BFDR is used as an estimate of FDR in the <monospace>ShrinkBayes</monospace> setting. True FDR is evaluated on the selected sets by simply dividing the number of false positives by the total number of positives. Here, the differences are much clearer: the Kruskal-Wallis test is useless in this setting, because it does not select anything. <monospace>ShrinkBayes,Old</monospace> selects too much at a too high true FDR, probably due to the smooth prior, as discussed before. <monospace>DESeq</monospace> and <monospace>ShrinkBayes</monospace> produce better true FDRs (with the DESeq ones more variable), but, on average, <monospace>ShrinkBayes</monospace> detects almost four times as many features. <monospace>edgeR</monospace> selects more, but is both more liberal and more variable. In fact, as can be inferred from the ROC curves, <monospace>ShrinkBayes</monospace> would achieve a smaller true FDR with the same number of detections as <monospace>edgeR</monospace>.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Number of detections (mean and standard deviation) at target FDR = 0.1 and true FDR for the set of detections (median and IQR: interquartile range)</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/></colgroup><thead valign="top"><tr><th align="right" valign="bottom"><bold>Method</bold><hr/></th><th align="right" valign="bottom"><bold># Detections</bold><hr/></th><th align="right" valign="bottom"><bold>True FDR</bold><hr/></th></tr><tr><th align="left">&#x000a0;</th><th align="right"><bold>mean (sd)</bold></th><th align="right"><bold>Median (IQR)</bold></th></tr></thead><tbody valign="top"><tr><td align="right" valign="bottom"><monospace>ShrinkBayes</monospace><hr/></td><td align="right" valign="bottom">37.4 (4.60)<hr/></td><td align="right" valign="bottom">0.171 (0.072)<hr/></td></tr><tr><td align="right" valign="bottom"><monospace>ShrinkBayes, Old</monospace><hr/></td><td align="right" valign="bottom">132.1 (15.3)<hr/></td><td align="right" valign="bottom">0.509 (0.038)<hr/></td></tr><tr><td align="right" valign="bottom"><monospace>edgeR</monospace><hr/></td><td align="right" valign="bottom">58.8 (12.9)<hr/></td><td align="right" valign="bottom">0.258 (0.120)<hr/></td></tr><tr><td align="right" valign="bottom"><monospace>DESeq</monospace><hr/></td><td align="right" valign="bottom">10.4 (3.75)<hr/></td><td align="right" valign="bottom">0.191 (0.178)<hr/></td></tr><tr><td align="right"><monospace>Kruskal-Wallis</monospace></td><td align="right">0 (0)</td><td align="right">NA</td></tr></tbody></table><table-wrap-foot><p>Results are summaries from 10 repeats of the simulated data sets.</p></table-wrap-foot></table-wrap><p>Note that <monospace>ShrinkBayes</monospace> is still liberal in the sense that it underestimates true FDR. This is probably due to the data not being generated from a specific parametric distribution. In particular, we observed that the data contains outliers for some features. Dedicated detection of such outliers can certainly reduce the number of false positives. A simple, heuristic, practical alternative is to additionally require for selection the corresponding <italic>uncorrected </italic>Kruskal-Wallis <italic>p</italic>-value to be smaller than 0.05. Then, power of a parametric approach like <monospace>ShrinkBayes</monospace>, which is essential in a multiple testing setting, is combined with the robustness of a nonparametric test. In this case, the median true FDR drops from 0.171 to 0.134 (target equals 0.1), while detecting 32 features on average instead of 37.4.</p></sec><sec><title>Example: analysis of miRseq count data</title><sec><title><bold>
<italic>Data</italic>
</bold></title><p>We applied <monospace>ShrinkBayes</monospace> to a challenging data set. The data set contains miRseq counts of 2060 miRNAs (3p- and 5p-variants) for 55 resections from primary colon tumors (P) and corresponding metastases (M) coded by the covariate <monospace>PM</monospace>. In addition, several other covariates are available: <monospace>indiv</monospace>: most individuals correspond to 2 samples (one for P, M), but some have multiple measurements for M, because the metastasis occurred at multiple locations; <monospace>organ</monospace>: organ where the metastasis occurred; <monospace>time</monospace>: binary, indicating whether resections of the primary tumor and the metastasis were at different dates; <monospace>chemo</monospace>: binary, indicating whether chemotherapy was applied in between the resections. In addition to other software, <monospace>ShrinkBayes</monospace> provides two important extra features to correctly analyze these data: it explicitly accounts for excess of zeros and allows for random effects (here <monospace>indiv</monospace>). Both are important for appropriate inference. In addition, we demonstrate here that joint inference for related parameters like those corresponding to <monospace>organ</monospace> is feasible. Note that separate inference for each organ has limited power due to the small number of samples per organ. We focus on the statistical analysis. Preprocessing is described in the Additional file <xref ref-type="supplementary-material" rid="S1">1</xref>, Section 3, which also contains annotated R-code for the entire analysis, including inferences for <monospace>organ</monospace> and the P-M contrast.</p></sec><sec><title><bold>
<italic>Analysis</italic>
</bold></title><p>The analysis consists of the following steps: 1) Likelihood specification for the counts, here the zero-inflated negative binomial one; 2a) Specification of the regression model. Here, the model <inline-formula><inline-graphic xlink:href="1471-2105-15-116-i37.gif"/></inline-formula> is the linear model with fixed effects <monospace>PM</monospace>, <monospace>time</monospace>, <monospace>chemo</monospace> and <monospace>organ</monospace> plus random effect <monospace>indiv</monospace>; 2b) Specification of the null-model <inline-formula><mml:math id="M37" name="1471-2105-15-116-i38" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>: as <inline-formula><inline-graphic xlink:href="1471-2105-15-116-i39.gif"/></inline-formula>, without <monospace>organ</monospace>; 3) Choice of parameters to shrink. Here, all fixed parameters plus the over-dispersion parameter of the negative binomial.</p><p>4) Estimation of priors for the purpose of shrinkage. Standard priors (Gaussian and inverse-Gamma) are used for all parameters, except for the inferential variable, <monospace>organ</monospace>, for which the multivariate mixture prior (5) is used; 5) Computation of posteriors under models <inline-formula><inline-graphic xlink:href="1471-2105-15-116-i40.gif"/></inline-formula> and <inline-formula><mml:math id="M38" name="1471-2105-15-116-i41" overflow="scroll"><mml:msub><mml:mrow><mml:mo mathvariant="script">&#x02133;</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, given the prior parameters; 6) Combination of the two posteriors to one given the parameters of the mixture prior; 7) Compute local and Bayesian false discovery rates (lfdr; BFDR). The most complex steps, 4) to 7), are completely automated including setting of tested defaults, which allows users with little experience in Bayesian computing to apply <monospace>ShrinkBayes</monospace>. The joint mixture prior is discussed above; other technical details are given in [<xref ref-type="bibr" rid="B2">2</xref>].</p></sec><sec><title><bold>
<italic>Discoveries</italic>
</bold></title><p>At BFDR = 0.10, we discovered 43 miRs for which <monospace>organ</monospace> is associated to expression in the metastasis. Figure&#x000a0;<xref ref-type="fig" rid="F2">2</xref> shows two posteriors of contrasts <italic>&#x003b2;</italic><sub>
<italic>ik </italic>
</sub>- <italic>&#x003b2;</italic><sub>
<italic>i </italic>
<italic>&#x02113;</italic>
</sub>,<italic>k </italic>&#x0003e; <italic>&#x02113;</italic>, which help to explain differential or non-differential miR expression. For example, for the significant differential miR, which corresponds to the left display of Figure&#x000a0;<xref ref-type="fig" rid="F2">2</xref>, differences are largest between organs 0 and 3 on one side and organs 1 and 2 on the other. To accommodate users, <monospace>ShrinkBayes</monospace> contains functions to easily produce such posterior plots and also summary tables. Importantly, the estimate of <italic>p</italic><sub>0 </sub>in (5) is large, <inline-formula><mml:math id="M39" name="1471-2105-15-116-i42" overflow="scroll"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.92</mml:mn></mml:math></inline-formula>, which implies strong shrinkage of organ effects towards zero, rendering more &#x02009;degrees of freedom&#x02019; and hence more power for other inferences. This is another strong aspect of <monospace>ShrinkBayes</monospace>: in studies with relatively few samples, multi-parameter shrinkage helps to increase power for a particular parameter of interest [<xref ref-type="bibr" rid="B4">4</xref>]. The idea of jointly shrinking multiple parameters was recently also adopted in [<xref ref-type="bibr" rid="B10">10</xref>], although their approach currently applies to <italic>K</italic>-group comparisons only.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>Posterior densities and joint null-probability (pi0All) of 6 contrasts</bold><bold>
<italic>&#x003b2;</italic>
</bold><sub>
<bold>
<italic>i</italic>
</bold>
<bold>
<italic>k </italic>
</bold>
</sub><bold>- </bold><bold>
<italic>&#x003b2;</italic>
</bold><sub>
<bold>
<italic>i </italic>
</bold>
<bold>
<italic>&#x02113;</italic>
</bold>
</sub><bold>, </bold><bold>
<italic>k</italic>
</bold><bold>&#x0003e;</bold><bold>
<italic>&#x02113;</italic>
</bold><bold>, representing log</bold><bold>
<italic>e</italic>
</bold><bold>-fold expression differences (x-axis) between 4 organs, for a significant miR (left) and non-significant miR (right).</bold></p></caption><graphic xlink:href="1471-2105-15-116-2"/></fig></sec></sec></sec><sec sec-type="discussion"><title>Discussion</title><p>For the choice of prior, we recommend to use the SpGG prior when inference on a parameter equalling zero is desired, because of its uniformly good performance in terms of FDR estimation and power. The SpNP prior is a good alternative which may be attractive in extremely small sample size settings for which the flexible shape of the non-parametric component is important (see also [<xref ref-type="bibr" rid="B4">4</xref>]). When using an interval null-hypothesis, <italic>H</italic><sub>0<italic>i </italic>
</sub>: |<italic>&#x003b2;</italic><sub>
<italic>i</italic>
</sub>| &#x0003c; <italic>&#x003b4;</italic>, inclusion of a spike is less relevant, so smooth (non-parametric) priors generally suffice.</p><p>Given the good performance of the SpGG prior in a univariate setting, it may be good to extend (5) to the multivariate analogue of the SpGG prior: a mixture of a multivariate point mass and a two-component Gaussian mixture product density. However, while this is conceptually feasible, it may be computationally cumbersome, because it would require combining several different fits from <monospace>INLA</monospace> under combinations of the components of the mixture.</p><p>Although <monospace>ShrinkBayes</monospace> is much more efficient that MCMC-based methods, it is computationally more demanding than frequentist counterparts like <monospace>edgeR</monospace>[<xref ref-type="bibr" rid="B8">8</xref>] and <monospace>DESeq</monospace>[<xref ref-type="bibr" rid="B9">9</xref>]. As an indication: the data example above (on approx 2,000 features) runs in approximately 30 minutes on 6 cpus of a Linux-cluster, whereas approximately 6 hours would be required for 100,000 features. For extremely large data sets, <monospace>ShrinkBayes</monospace> provides quick pre-screen functions, application of which potentially reduces computing time by a large factor.</p><p>We focused on sequencing count data for fairly complex designs. To our knowledge, extensively validated data are still not available for such studies, which hampers a thorough comparison between methods. Even when such a data set would be available, it is uncertain to what extent conclusions from one data set could be extrapolated to others, because the relative performance of a method may depend on many aspects such as the proportion of outliers and zero counts and/or the presence of multiple noise levels (e.g. within and between individuals). We emphasize that <monospace>ShrinkBayes</monospace> is currently the only RNAseq analysis method that can deal with the latter, by allowing random and mixed effects models, concepts that are widely accepted and used in other fields of statistical data analysis.</p><p>For simple designs, <monospace>ShrinkBayes</monospace> can be useful as well, in particular due to its good reproducibility, as shown for publicly available RNAseq data in [<xref ref-type="bibr" rid="B2">2</xref>]. <monospace>ShrinkBayes</monospace> also applies to Gaussian data, like mRNA microarray data or high-throughput RNA interference screens [<xref ref-type="bibr" rid="B4">4</xref>]. Use is similar, as illustrated in the <monospace>ShrinkBayes</monospace> R-vignette, which also contains additional examples on count data.</p></sec><sec sec-type="conclusions"><title>Conclusion</title><p>We illustrated the versatility of <monospace>ShrinkBayes</monospace> on a data set which reflects a level of complexity that is common in clinical practice. With the decrease of costs for sequencing, we are likely to encounter such complex data sets frequently in the near future and <monospace>ShrinkBayes</monospace> provides the means and power to analyze these.</p></sec><sec><title>Availability and requirements</title><p><bold>Project name: </bold><monospace>ShrinkBayes </monospace><bold>Project home page: </bold><ext-link ext-link-type="uri" xlink:href="http://www.few.vu.nl/~mavdwiel/ShrinkBayes.html">http://www.few.vu.nl/~mavdwiel/ShrinkBayes.html</ext-link><bold>Operating system: </bold>Platform independent (developed on Linux)<bold>Programming language: </bold>R<bold>Other requirements: </bold>R (&#x0003e;= 3.0.1); R-packages: <monospace>INLA</monospace>, from <ext-link ext-link-type="uri" xlink:href="http://www.r-inla.org">http://www.r-inla.org</ext-link> and <monospace>snowfall</monospace>, <monospace>VGAM</monospace>, <monospace>mclust</monospace>, <monospace>logcondens</monospace>, <monospace>Iso</monospace> from CRAN<bold>Licence: </bold>GNU GPL</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors&#x02019; contributions</title><p>MAvdW developed the methodology and the software. MAvdW and MN wrote the manuscript. HMWV and TEB conceived of the miRseq study, which was carried out by MN. DS facilitated the sequencing and performed the initial analysis of the data. All authors read, discussed and approved the manuscript.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="S1"><caption><title>Additional file 1</title><p><bold>Supplementary Material. </bold>It contains: additional simulation results, a list of changes of the software with respect to previous versions, details on the Savage-Dickey approximation for marginal likelihood and extensive R-code for the miRseq example.</p></caption><media xlink:href="1471-2105-15-116-S1.pdf"><caption><p>Click here for file</p></caption></media></supplementary-material></sec></body><back><sec><title>Acknowledgements</title><p>We thank Charlotte Soneson for providing simulated data, Andrea Riebler for discussions and Paul Eijk for his help with library preparation.</p></sec><ref-list><ref id="B1"><mixed-citation publication-type="journal"><name><surname>Soneson</surname><given-names>C</given-names></name><name><surname>Delorenzi</surname><given-names>M</given-names></name><article-title><bold>A comparison of methods for differential expression analysis of RNA-seq data</bold></article-title><source>BMC Bioinformatics</source><year>2013</year><volume>14</volume><fpage>91</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-14-91</pub-id><?supplied-pmid 23497356?><pub-id pub-id-type="pmid">23497356</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="journal"><name><surname>Van de Wiel</surname><given-names>MA</given-names></name><name><surname>Leday</surname><given-names>GGR</given-names></name><name><surname>Pardo</surname><given-names>L</given-names></name><name><surname>Rue</surname><given-names>H</given-names></name><name><surname>van der Vaart</surname><given-names>AW</given-names></name><name><surname>van Wieringen</surname><given-names>WN</given-names></name><article-title><bold>Bayesian analysis of RNA sequencing data by estimating multiple shrinkage priors</bold></article-title><source>Biostatistics</source><year>2012</year><volume>14</volume><fpage>113</fpage><lpage>128</lpage><?supplied-pmid 22988280?><pub-id pub-id-type="pmid">22988280</pub-id></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><name><surname>Rue</surname><given-names>H</given-names></name><name><surname>Martino</surname><given-names>S</given-names></name><name><surname>Chopin</surname><given-names>N</given-names></name><article-title><bold>Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations (with discussion)</bold></article-title><source>J R Stat Soc B</source><year>2009</year><volume>71</volume><fpage>319</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2008.00700.x</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><name><surname>Van de Wiel</surname><given-names>MA</given-names></name><name><surname>de Menezes</surname><given-names>RX</given-names></name><name><surname>Siebring-van Olst</surname><given-names>E</given-names></name><name><surname>van Beusechem</surname><given-names>VW</given-names></name><article-title><bold>Analysis of small-sample clinical genomics studies using multi-parameter shrinkage: application to high-throughput RNA interference screening</bold></article-title><source>BMC Med Genom</source><year>2013</year><volume>6</volume><fpage>1</fpage><pub-id pub-id-type="doi">10.1186/1755-8794-6-1</pub-id></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="book"><name><surname>Efron</surname><given-names>B</given-names></name><source>Large-scale Inference. Institute of Mathematical Statistics Monographs</source><year>2010</year><publisher-name>Cambridge: Cambridge University Press</publisher-name></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><name><surname>Ventrucci</surname><given-names>M</given-names></name><name><surname>Scott</surname><given-names>EM</given-names></name><name><surname>Cocchi</surname><given-names>D</given-names></name><article-title><bold>Multiple testing on standardized mortality ratios: a Bayesian hierarchical model for FDR estimation</bold></article-title><source>Biostatistics</source><year>2011</year><volume>12</volume><fpage>51</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxq040</pub-id><?supplied-pmid 20577014?><pub-id pub-id-type="pmid">20577014</pub-id></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><name><surname>Wetzels</surname><given-names>R</given-names></name><name><surname>Grasman</surname><given-names>RPPP</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name><article-title><bold>An encompassing prior generalization of the Savage-Dickey density ratio</bold></article-title><source>Comp Stat Data Anal</source><year>2010</year><volume>54</volume><fpage>2094</fpage><lpage>2102</lpage><pub-id pub-id-type="doi">10.1016/j.csda.2010.03.016</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><name><surname>Robinson</surname><given-names>MD</given-names></name><name><surname>McCarthy</surname><given-names>DJ</given-names></name><name><surname>Smyth</surname><given-names>GK</given-names></name><article-title><bold>edgeR: a Bioconductor package for differential expression analysis of digital gene expression data</bold></article-title><source>Bioinformatics</source><year>2010</year><volume>26</volume><fpage>139</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btp616</pub-id><?supplied-pmid 19910308?><pub-id pub-id-type="pmid">19910308</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><name><surname>Anders</surname><given-names>S</given-names></name><name><surname>Huber</surname><given-names>W</given-names></name><article-title><bold>Differential expression analysis for sequence count data</bold></article-title><source>Genome Biol</source><year>2010</year><volume>11</volume><fpage>106</fpage><pub-id pub-id-type="doi">10.1186/gb-2010-11-2-106</pub-id><?supplied-pmid 20236492?><pub-id pub-id-type="pmid">20236492</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><name><surname>Si</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>P</given-names></name><article-title><bold>An optimal test with maximum average power while controlling FDR with application to RNA-seq data</bold></article-title><source>Biometrics</source><year>2013</year><volume>69</volume><fpage>594</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1111/biom.12036</pub-id><?supplied-pmid 23889143?><pub-id pub-id-type="pmid">23889143</pub-id></mixed-citation></ref></ref-list></back></article>