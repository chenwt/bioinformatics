<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d2 20140930//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Biol Direct</journal-id><journal-id journal-id-type="iso-abbrev">Biol. Direct</journal-id><journal-title-group><journal-title>Biology Direct</journal-title></journal-title-group><issn pub-type="epub">1745-6150</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4539931</article-id><article-id pub-id-type="publisher-id">71</article-id><article-id pub-id-type="doi">10.1186/s13062-015-0071-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>Experiences with workflows for automating data-intensive bioinformatics</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Spjuth</surname><given-names>Ola</given-names></name><address><email>ola.spjuth@farmbio.uu.se</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Bongcam-Rudloff</surname><given-names>Erik</given-names></name><address><email>Erik.Bongcam@slu.se</email></address><xref ref-type="aff" rid="Aff2"/></contrib><contrib contrib-type="author"><name><surname>Hern&#x000e1;ndez</surname><given-names>Guillermo Carrasco</given-names></name><address><email>guillermo.carrasco@scilifelab.se</email></address><xref ref-type="aff" rid="Aff3"/></contrib><contrib contrib-type="author"><name><surname>Forer</surname><given-names>Lukas</given-names></name><address><email>lukas.forer@i-med.ac.at</email></address><xref ref-type="aff" rid="Aff4"/></contrib><contrib contrib-type="author"><name><surname>Giovacchini</surname><given-names>Mario</given-names></name><address><email>mario.giovacchini@scilifelab.se</email></address><xref ref-type="aff" rid="Aff3"/></contrib><contrib contrib-type="author"><name><surname>Guimera</surname><given-names>Roman Valls</given-names></name><address><email>brainstorm@nopcode.org</email></address><xref ref-type="aff" rid="Aff3"/></contrib><contrib contrib-type="author"><name><surname>Kallio</surname><given-names>Aleksi</given-names></name><address><email>aleksi.kallio@csc.fi</email></address><xref ref-type="aff" rid="Aff5"/></contrib><contrib contrib-type="author"><name><surname>Korpelainen</surname><given-names>Eija</given-names></name><address><email>eija.korpelainen@csc.fi</email></address><xref ref-type="aff" rid="Aff5"/></contrib><contrib contrib-type="author"><name><surname>Ka&#x00144;du&#x00142;a</surname><given-names>Maciej M</given-names></name><address><email>maciej.kandula@boku.ac.at</email></address><xref ref-type="aff" rid="Aff6"/></contrib><contrib contrib-type="author"><name><surname>Krachunov</surname><given-names>Milko</given-names></name><address><email>wfxp@milko.3mhz.net</email></address><xref ref-type="aff" rid="Aff7"/></contrib><contrib contrib-type="author"><name><surname>Kreil</surname><given-names>David P</given-names></name><address><email>david.kreil@boku.ac.at</email></address><xref ref-type="aff" rid="Aff6"/></contrib><contrib contrib-type="author"><name><surname>Kulev</surname><given-names>Ognyan</given-names></name><address><email>okulev@fmi.uni-sofia.bg</email></address><xref ref-type="aff" rid="Aff7"/></contrib><contrib contrib-type="author"><name><surname>&#x00141;abaj</surname><given-names>Pawe&#x00142; P.</given-names></name><address><email>pawel.labaj@boku.ac.at</email></address><xref ref-type="aff" rid="Aff6"/></contrib><contrib contrib-type="author"><name><surname>Lampa</surname><given-names>Samuel</given-names></name><address><email>samuel.lampa@it.uu.se</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Pireddu</surname><given-names>Luca</given-names></name><address><email>luca.pireddu@crs4.it</email></address><xref ref-type="aff" rid="Aff8"/></contrib><contrib contrib-type="author"><name><surname>Sch&#x000f6;nherr</surname><given-names>Sebastian</given-names></name><address><email>sebastian.schoenherr@i-med.ac.at</email></address><xref ref-type="aff" rid="Aff4"/></contrib><contrib contrib-type="author"><name><surname>Siretskiy</surname><given-names>Alexey</given-names></name><address><email>alexey.siretskiy@it.uu.se</email></address><xref ref-type="aff" rid="Aff9"/></contrib><contrib contrib-type="author"><name><surname>Vassilev</surname><given-names>Dimitar</given-names></name><address><email>jim6329@gmail.com</email></address><xref ref-type="aff" rid="Aff10"/></contrib><aff id="Aff1"><label/>Department of Pharmaceutical Biosciences and Science for Life Laboratory, Uppsala University, SE-75124, Uppsala, P.O. Box 591 Sweden </aff><aff id="Aff2"><label/>SLU-Global Bioinformatics Centre, Department of Animal Breeding and Genetics, Swedish University of Agricultural Sciences, Uppsala, Sweden </aff><aff id="Aff3"><label/>Science for Life Laboratory, Karolinska Institutet, SE-17121, Stockholm, P.O. Box 1031 Sweden </aff><aff id="Aff4"><label/>Division of Genetic Epidemiology, Medical University of Innsbruck, Innsbruck, 6020 Austria </aff><aff id="Aff5"><label/>CSC - IT Center for Science Ltd., FI-02101, Espoo, P.O. Box 405 Finland </aff><aff id="Aff6"><label/>Chair of Bioinformatics Research Group, Boku University, Vienna, Austria </aff><aff id="Aff7"><label/>Faculty of Mathematics and Informatics, Sofia University, Sofia, Bulgaria </aff><aff id="Aff8"><label/>CRS4 Polaris, Pula, Italy </aff><aff id="Aff9"><label/>Department of Information Technology, Uppsala University, SE-75105, Uppsala, P.O. Box 337 Sweden </aff><aff id="Aff10"><label/>AgroBioInstitute and Joint Genomic Centre, Sofia, Bulgaria </aff></contrib-group><pub-date pub-type="epub"><day>19</day><month>8</month><year>2015</year></pub-date><pub-date pub-type="pmc-release"><day>19</day><month>8</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>10</volume><elocation-id>43</elocation-id><history><date date-type="received"><day>26</day><month>2</month><year>2015</year></date><date date-type="accepted"><day>3</day><month>8</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; Spjuth et al. 2015</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><p>High-throughput technologies, such as next-generation sequencing, have turned molecular biology into a data-intensive discipline, requiring bioinformaticians to use high-performance computing resources and carry out data management and analysis tasks on large scale. Workflow systems can be useful to simplify construction of analysis pipelines that automate tasks, support reproducibility and provide measures for fault-tolerance. However, workflow systems can incur significant development and administration overhead so bioinformatics pipelines are often still built without them. We present the experiences with workflows and workflow systems within the bioinformatics community participating in a series of hackathons and workshops of the EU COST action SeqAhead. The organizations are working on similar problems, but we have addressed them with different strategies and solutions. This fragmentation of efforts is inefficient and leads to redundant and incompatible solutions. Based on our experiences we define a set of recommendations for future systems to enable efficient yet simple bioinformatics workflow construction and execution.</p><p><bold>Reviewers</bold></p><p>This article was reviewed by Dr Andrew Clark.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Workflow</kwd><kwd>Automation</kwd><kwd>Data-intensive</kwd><kwd>High-performance computing</kwd><kwd>Big data</kwd><kwd>Reproducibility</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2015</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p>High-throughput technologies such as next-generation sequencing (NGS) have revolutionized molecular biology and transformed it into a data-intensive discipline [<xref ref-type="bibr" rid="CR1">1</xref>]. Bioinformaticians are nowadays required to interact with e-infrastructure consisting of high-performance computing (HPC) resources, large-scale storage, and a vibrant ecosystem of bioinformatics tools. It is common that analyses consist of multiple software tools applied in a sequential fashion on input data; and these analysis steps are usually executed on a server or a computer cluster given the significant data size and computation time requirements. Such a multi-step procedure is commonly referred to as a workflow. In order to efficiently carry out such analysis it can be beneficial to use Scientific Workflow Management Systems that can streamline the design and execution of workflows and pipelines in high-performance computing settings such as local clusters or distributed computing clouds [<xref ref-type="bibr" rid="CR2">2</xref>].</p><p>There exist a number of workflow systems for use in bioinformatics. Taverna [<xref ref-type="bibr" rid="CR3">3</xref>] pioneered integration of web services in bioinformatics; Galaxy [<xref ref-type="bibr" rid="CR4">4</xref>&#x02013;<xref ref-type="bibr" rid="CR6">6</xref>] is a workflow system that has been used in sequence analysis and other bioinformatics applications; Kepler [<xref ref-type="bibr" rid="CR7">7</xref>] and Chipster [<xref ref-type="bibr" rid="CR8">8</xref>] are other examples of such systems that are used for next-generation sequencing and gene expression data analysis. All of the abovementioned systems have graphical user interfaces for constructing workflows and can run on HPC and cloud systems. However, experienced bioinformaticians commonly work at a lower programming level and write their workflows as custom scripts in a scripting language such as Bash, Perl or Python. For this user group, a number of lightweight workflow systems have emerged to simplify scripting and parallelizing tasks, which is particular relevant for an efficient exploitation of HPC resources, including Luigi (<ext-link ext-link-type="uri" xlink:href="https://github.com/spotify/luigi">https://github.com/spotify/luigi</ext-link>), Bpipe [<xref ref-type="bibr" rid="CR9">9</xref>], Snakemake [<xref ref-type="bibr" rid="CR10">10</xref>] and BcBio (<ext-link ext-link-type="uri" xlink:href="https://github.com/chapmanb/bcbio-nextgen">https://github.com/chapmanb/bcbio-nextgen</ext-link>). General Linux tools such as Make [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>] are also widely used due to their simplicity.</p><p>HPC resources in academia traditionally consist of compute clusters with Linux operating system and batch (queueing) systems for scheduling jobs. Recently, cloud computing has emerged as an additional technology offering virtualized environments and the capability to run custom virtual machine images (VMI). For workflows this opens new possibilities such as packaging entire analyses or pipelines as VMIs, which has been acknowledged in bioinformatics [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]. There are also other technologies such as MapReduce [<xref ref-type="bibr" rid="CR15">15</xref>], Hadoop [<xref ref-type="bibr" rid="CR16">16</xref>] and Spark [<xref ref-type="bibr" rid="CR17">17</xref>] that show great promise in bioinformatics and that might change how bioinformatics analysis can be automated.</p><p>Within the COST Action BM1006: Next Generation Sequencing Data Analysis Network (&#x0201c;SeqAhead&#x0201d;, <ext-link ext-link-type="uri" xlink:href="http://www.seqahead.eu/">http://www.seqahead.eu/</ext-link>), a series of hackathons and workshops brought together a number of scientists from different organizations, all involved in data-intensive bioinformatics analysis. This manuscript summarizes the participants&#x02019; current e-infrastructure, their experiences with workflows, lists future challenges for automating data-intensive bioinformatics analysis, and defines the criteria to enable efficient yet simple bioinformatics workflow construction and execution.</p></sec><sec id="Sec2"><title>Workflow experiences</title><sec id="Sec3"><title>UPPMAX and Science for Life Laboratory, Uppsala University, Sweden</title><sec id="Sec4"><title>Overview</title><p>The Bioinformatics platform at UPPMAX and Science for Life Laboratory (SciLifeLab) provide high-performance computational resources for the national NGS community in Sweden, as well as the necessary tools and competences to enable Swedish bioinformaticians to work efficiently with HPC systems [<xref ref-type="bibr" rid="CR18">18</xref>]. Since 2010, UPPMAX has had over 500 projects and 300 users, and as of December 2014 has 3328 compute cores and almost 7 PB of storage. On UPPMAX HPC systems, users get access to installed software, reference data, and are able to carry out data-intensive bioinformatics analyses. Applications include whole genome-, <italic>de novo</italic>- and exome sequencing, targeted resequencing, single nucleotide polymorphisms (SNPs) discovery, gene expression and methylation analysis.</p></sec><sec id="Sec5"><title>Workflow experience</title><p>On our systems, most users use scripting in Bash, Perl, and Python to automate analysis. We have a security policy to not allow web servers, which has made it more difficult for us to use graphical platforms such as Galaxy. Recently, however, we have deployed a private cloud where we aim to provision images containing workflow systems like Galaxy, Chipster, and GPCR-ModSim [<xref ref-type="bibr" rid="CR19">19</xref>], which we believe will enable us to reach a larger scientific community. We are experimenting with the workflow system Luigi on our HPC system, and CloudGene [<xref ref-type="bibr" rid="CR20">20</xref>] on a previously established prototype Hadoop cluster in a private cloud. For automating workflow execution we use either cron jobs and an external Jenkins continuous integration instance.</p><p>Besides the workflow evaluations, considerable efforts were put on the quantitative comparison of the different approaches to solve usual bioinformatic tasks in DNA and RNA-seq experiments. In recent work we provide evidence for superior scalability for the task of mapping short reads followed by calling variants on the Hadoop-with-HDFS platform compared with the existing HPC cluster infrastructure [<xref ref-type="bibr" rid="CR21">21</xref>]. We also developed a versatile solution [<xref ref-type="bibr" rid="CR22">22</xref>] for the feature-counting and quality assessment tasks in RNA-seq analysis, extending the acknowledged HTSeq package [<xref ref-type="bibr" rid="CR23">23</xref>] into the e-Science domain with Hadoop and MapReduce. We are also evaluating the Spark platform for pipelining NGS data but our initial assessment did not reveal any performance gain compare to Hadoop due to the non-iterative nature of our problems. Spark has however in our opinion a more intuitive and appealing programming environment.</p></sec><sec id="Sec6"><title>Future challenges</title><p>It is important for UPPMAX as a national provider of HPC resources for NGS analysis to strive for efficient resource usage. With many biologists having little experience of automating bioinformatics analyses, it is important for us to provide workflow systems, examples, support, and training in order to maximize resource utilization and improve efficiency of analyses. We are noting that future pipelines will have problems running on our current HPC systems due to intensive use of shared file systems, and we will continue to evaluate and develop a future e-infrastructure where Hadoop and Spark are interesting options. There is however a challenge for traditional HPC centers like UPPMAX to adopt cloud computing and Hadoop clusters as they contrast a lot to current best practices and experiences of system administrators. The buildup of competence in these directions will be an important task.</p></sec></sec><sec id="Sec7"><title>Chair of Bioinformatics Research Group, Boku University Vienna, Austria</title><sec id="Sec8"><title>Overview</title><p>The Chair of Bioinformatics at Boku University Vienna is a method-centric research group at the interface of computational analysis and large-scale experimental assays. Recent work includes (i) an assessment of accuracy, reproducibility, and information content of gene transcript expression profiling platforms, including RNA-Seq, microarrays, and qPCR [<xref ref-type="bibr" rid="CR24">24</xref>]; (ii) a method benchmark in the comparison of normalization efficiency across multi-site RNA-Seq laboratories [<xref ref-type="bibr" rid="CR25">25</xref>]; (iii) signal level models of hybridization based assays for high-density microarrays [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. These analyses require high computational power largely provided by HPC facilities like the Vienna Scientific Cluster (VSC), with the VSC-2 consisting of 1,314 nodes with 16 cores and 32 GB RAM, and the VSC-3 consisting of 2,020 nodes with 16 cores and 64 GB RAM. Large memory tasks are run on individual fat nodes with 256 GB&#x02013;16 TB RAM.</p></sec><sec id="Sec9"><title>Workflow experience</title><p>In many instances, we simply use Make [<xref ref-type="bibr" rid="CR12">12</xref>] to run custom pipelines for both cluster and local jobs. It is a standalone tool with no setup/installation needed in most standard environments. In our experience, if a workflow system is less lightweight than Make [<xref ref-type="bibr" rid="CR12">12</xref>] and small scripts (Perl, Bash, etc.), people will not use it when they need to &#x02018;get something done&#x02019; even though many people know that in the long-term this is not efficient. Systems like Galaxy and Taverna provide useful platforms for the automation of routine data analysis steps as commonly found in industrial or facility settings, but are less effective for explorative and flexible analyses. In explorative work, one would like to run workflows for different configurations, and compare results. It would be helpful if there was transparent support for tagging or otherwise managing &#x02018;alternative&#x02019; workflow runs, and outputs. Moreover, most systems lack support for the enforcement of quality control on inputs/outputs, and support for cycle control (revisions of workflows, input data, tools).</p><p>We have initially tested several systems, including, Bpipe [<xref ref-type="bibr" rid="CR9">9</xref>], Moa [<ext-link ext-link-type="uri" xlink:href="https://github.com/mfiers/Moa">https://github.com/mfiers/Moa</ext-link>], Ruffus [<xref ref-type="bibr" rid="CR28">28</xref>], and Snakemake [<xref ref-type="bibr" rid="CR10">10</xref>]. We have since focused on exploring Snakemake due to, among other features, its make-like workflow definition, simple integration with Python, Bash code portability, ease of porting workflows to a cluster, intuitive parallelization, and ongoing active development. We are currently working on extending Snakemake with a lightweight modular system for development cycle control and policy-based specification of rules and requirements that supports an in-flow enforcement of consistency constraints. We have developed and validated a proof-of-concept prototype of the mechanism and automated the code generation of rules.</p><p>Specifically, we have used workflow systems to preprocess cancer-related data, like tumour/normal samples from the TCGA consortium [<xref ref-type="bibr" rid="CR29">29</xref>], and to fully automate some steps of data analysis. Furthermore, we apply workflow systems in the design of high-performance microarrays for <italic>Drosophila melanogaster</italic> and other complex eukaryotes or to automate specialized RNA-seq analyses in fast evolving domains like single-cell profiling in stem cell research.</p></sec><sec id="Sec10"><title>Future challenges</title><p>While Snakemake seems to be a promising tool, on its own, similar to currently available alternatives, it does not provide an exhaustive workflow system solution but instead requires external mechanisms to support critical features like revision control and management of multiple workflow instances run with varying parameter sets. We are now working to integrate Snakemake with external tools and our modular code generation system for in-flow enforcement of consistency constraints.</p></sec></sec><sec id="Sec11"><title>CSC, Espoo, Finland</title><sec id="Sec12"><title>Overview</title><p>CSC - IT Center for Science is a government-owned computing centre in Finland that provides IT support and resources for academia, research institutes and companies. CSC provides capacity through a traditional batch oriented HPC environment, but also with a cloud platform. Major HPC environments are Cray XC40 supercomputer with 40,608 cores and HP XL230a cluster with 12,960 cores. The OpenStack based infrastructure-as-a-service (IaaS) cloud runs on the HPC cluster hardware.</p><p>As a national bioinformatics facility CSC has a large number of users, the majority of which have bio/medical background and no experience in programming. We strive to enable users to work independently by providing training and user friendly interfaces. An example of the latter is the Chipster software, developed at CSC, that provides a graphical user interface to a large suite of analysis tools [<xref ref-type="bibr" rid="CR8">8</xref>].</p></sec><sec id="Sec13"><title>Workflow experience</title><p>Chipster enables users to create and share bioinformatics workflows. It tracks what the user does and allows him/her to save any series of analysis steps. These workflows can be exported, shared, and applied to a different dataset. Everything is tracked, including parameter settings and reference data. The result files are also automatically annotated with this information. An example of a Chipster workflow is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Visual representation of a user-made ChIP-seq data analysis workflow in the Chipster software. After detecting STAT1 binding regions in the genome, the user has filtered the resulting peaks for q-value, length and peak hight. S/he has then looked for common sequence motifs in the peaks and matched them against a transcription factor binding site database. S/he has also retrieved the closest genes to the peaks and performed pathway enrichment analysis for them. Finally, s/he has checked if the enriched pathways contain the STAT signaling pathway. All these downstream analysis steps can be saved as an automatic workflow, which can be shared and executed on another dataset. In addition to analysing data and building workflows, Chipster allows users to visualize data interactively. As an example, genome browser visualization is shown (bottom right panel)</p></caption><graphic xlink:href="13062_2015_71_Fig1_HTML" id="MO1"/></fig></p><p>One major challenge is where to stop when recording analysis execution. We include parameters, inputs and such, but also source code for the tools. However, maintaining full reproducibility over years is impossible because the underlying tools and databases change. Our philosophy has been to maintain reproducibility to the level that is needed for workflows to be a practical tool for users. For provenance and long term archival we store enough metadata on the workflow and, most importantly, all data with their relationships. That might not be enough for one-click rerun of the pipeline several years later, but it is still enough for manual reproduction of the analysis.</p><p>Chipster users represent a wide range of research fields, ranging from medicine to agriculture and biotechnology. Therefore also the workflow functionality has to be flexible enough to cater for very different types of analysis. The typical tasks include analysis of RNA-seq data (QC, preprocessing, alignment, quantitation, differential expression analysis, filtering and pathway analysis), ChIP-seq data (QC, preprocessing, alignment, peak calling, filtering, motif discovery and pathway analysis) and exome/genome-seq data (QC, preprocessing, alignment, variant calling and filtering).</p></sec><sec id="Sec14"><title>Future challenges</title><p>Potential future development at CSC is to provide a more technically oriented workflow engine on top of our cloud IaaS offering. We are looking into software packages that are used and developed in the cloud and big data communities as a base for our own development efforts. Workflow system would be presented with platform-as-a-service (PaaS) model. Technically capable users could program workflows that are run in the IaaS cloud, but they would not need to care about the IaaS aspects such as node provisioning and user management.</p><p>Important requirement for future workflow systems is the ability to distribute data processing workload with frameworks such as Hadoop and Spark. To this end, we have participated in development of tools that allow bioinformatics data to be efficiently processed in Hadoop: Hadoop-BAM and SeqPig [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR31">31</xref>]. This work is continued by integrating Hadoop and Spark into our IaaS environment and providing easy to use interfaces for data intensive computing.</p></sec></sec><sec id="Sec15"><title>Swedish National Genomics Infrastructure (NGI), SciLifeLab, Stockholm, Sweden</title><sec id="Sec16"><title>Overview</title><p>The Stockholm genomics core platform of the Swedish National Genomics Infrastructure (NGI) crunched over 45TBp (terabasepairs) in 2014. The current NGS instrumentation located in Stockholm includes 11 Illumina HiSeq 2500 sequencers, 3 MiSeq systems, and 3 HiSeq X sequencers, and with the coming addition of more HiSeq X instruments, the amount of data produced and processed at NGI is expected to increase dramatically in the year ahead.</p></sec><sec id="Sec17"><title>Workflow experience</title><p>NGI in Stockholm uses bcbio-nextgen (<ext-link ext-link-type="uri" xlink:href="https://github.com/chapmanb/bcbio-nextgen">https://github.com/chapmanb/bcbio-nextgen</ext-link>) and some customizations for assembling and running the analysis pipelines. For us, having support from a pipeline framework already established in other institutions has been a big plus. In our experience, home-grown bioinformatics pipeline frameworks not published or released early enough in the development process fail to gain wide adoption and momentum. As bioinformatics pipelines are inherently complex, we think it is better to share this complexity with the open source community and generalize as early as possible. Unfortunately we have not been able to keep up with fast developments upstream and periodically deploy validated instances of the pipeline.</p><p>We think that this shows the growing disconnect between traditional HPC architectures in academia and other sectors in industry: 
<list list-type="order"><list-item><p><italic>Non-community maintained software.</italic> Such as using the ancient, hard to mantain and update &#x0201c;module system&#x0201d; (<ext-link ext-link-type="uri" xlink:href="http://modules.sf.net">http://modules.sf.net</ext-link>) versus a more sustainable option such as the HomeBrew science (<ext-link ext-link-type="uri" xlink:href="https://github.com/chapmanb/homebrew-cbl">https://github.com/chapmanb/homebrew-cbl</ext-link>) system.</p></list-item><list-item><p><italic>Non-existent stable usage of cloud computing architectures</italic>. This could enable continuous integration and delivery. Having containerized execution units coupled with good software management would increase robustness and provenance tracking on pipelines. That is, globally trackable software releases as opposed to the home-grown local module system that we now use.</p></list-item><list-item><p>Lack of career paths for Research Software Engineers (RSE) personnel (<ext-link ext-link-type="uri" xlink:href="http://www.rse.ac.uk/who.html">http://www.rse.ac.uk/who.html</ext-link>) that could explore new avenues and maintain points 1 and 2. In other words, lack of a &#x0201c;research computing&#x0201d; unit able to keep up and be up to date with new ways of computing.</p></list-item></list></p><p>For instance, our current HPC system does not now (and is not predicted to anytime soon) support newer deployment strategies such as continuous deployment of lightweight Docker containers (<ext-link ext-link-type="uri" xlink:href="https://github.com/chapmanb/bcbio-nextgen-vm">https://github.com/chapmanb/bcbio-nextgen-vm</ext-link>). As a result, we are actively exploring workflow frameworks and methodologies that can survive the age of HPC systems. We are investigating Piper (<ext-link ext-link-type="uri" xlink:href="https://github.com/johandahlberg/piper">https://github.com/johandahlberg/piper</ext-link>), Snakemake, and Luigi, which seem to be more adaptable with regard to deployment strategies.</p><p>On the one hand, many pipelines incorporate a basic test suite to ensure that all moving parts work as expected. On the other hand, few of those include a benchmarking suite that can validate several bioinformatic tools and compare their performance and biological relevance. Bcbio-nextgen has put some good care in validating that the underlying biology remains sound across software versions by following up with the &#x0201c;Genome in a Bottle Consortium&#x0201d;, a gold standard for validation.</p><p>Having a continuously deployed and benchmarked pipeline allows researchers and RSEs to validate every single change in the source code, like industry does with continuous software delivery and deployment models. In this way, both source code and biology can be validated and errors spotted earlier [<xref ref-type="bibr" rid="CR32">32</xref>]. Likewise, performance of variant callers can be continuously, closely assessed and improved quantitatively in different versions of the whole system.</p></sec><sec id="Sec18"><title>Best practice pipeline</title><p>For a few years, bcbionextgen has been processing samples for the so called &#x0201c;best practice&#x0201d; pipeline at SciLifeLab. The typical outputs of the pipeline include: 
<list list-type="bullet"><list-item><p>Quality assessment via FastQC.</p></list-item><list-item><p>Contamination screening via fastqscreen.</p></list-item><list-item><p>Alignment against preconfigured reference genomes and its indexes (mainly hg19).</p></list-item><list-item><p>Variant analysis using the GATK toolkit and FreeBayes.</p></list-item><list-item><p>Functional anotation of variants using SNPeff.</p></list-item><list-item><p>Several RNAseq packages such as cufflinks and DEXSeq.</p></list-item></list></p><p>In practice, although the outputs are appreciated by service customers, there are many sample and project-specific details that have to be taken in consideration. This limits our ability to generalize the data that can be most useful to our scientists, but we found that at least the quality assessment and some alignment and coverage metrics are immediately useful to researchers.</p></sec><sec id="Sec19"><title>Future challenges</title><p>Modernizing the current computing environment to more modern ways to isolate and reproduce workflows (Docker) while collaboratively managing scientific software (Homebrew Science, <ext-link ext-link-type="uri" xlink:href="http://planemo.readthedocs.org/en/latest/">http://planemo.readthedocs.org/en/latest/</ext-link>) are big challenges that hinder reproducibility and portability. Currently, we think that systems like Piper and others are too tightly coupled with specific environments, compromising its generalization and portability.</p></sec></sec><sec id="Sec20"><title>CRS4, Pula, Italy</title><sec id="Sec21"><title>Overview</title><p>CRS4 is a government research center with a focus on applied computing and biology. It hosts a high-throughput genotyping and sequencing facility that is directly connected to the center&#x02019;s computational resources (3000 cores, 4.5 PB storage). With three Illumina HiSeq 2000 and two older Illumina Genome Analyzer IIx, it is the largest NGS platform in Italy. CRS4 directly participates in large-scale population-wide genetic studies &#x02013; for instance, pertaining to autoimmune diseases and longevity [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>] &#x02013; and provides sequencing services for external collaborators and clients. All the data produced by the sequencing laboratory undergoes some degree of processing in the computing center, spanning from quality control and packaging to reference mapping and variant calling. Over the past five years, the facility has processed more than 2000 whole-genome resequencing samples, 800 RNA-Seq samples and 200 exome sequencing samples.</p></sec><sec id="Sec22"><title>Workflow experience</title><p>At CRS4 we have worked to automate the standard preliminary analysis of sequencing data to achieve high sample throughput and consistency. The processing system is summarized by the schematic diagram in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. Our automation strategy is split in two layers. At the lower layer we are using the Galaxy platform to implement workflows for specific operations on data &#x02013; e.g., demultiplexing (Fig. <xref rid="Fig3" ref-type="fig">3</xref>), alignment and variant calling. At a higher level, a custom daemon launches and monitors the execution of these workflows according to its configuration. When a workflow completes its operations, the daemon registers the resulting datasets in our OMERO.biobank [<xref ref-type="bibr" rid="CR35">35</xref>] traceability framework, which allows us to keep track of which input datasets and sequence of operations were applied to produce the results (represented by serializing the galaxy history). The process effectively results in a dataset graph rooted at the original raw data.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Components in CRS4&#x02019;s automation system. The system has been created by linking together freely available components with some specialized software built in-house. In addition to running preliminary processing, it records operations within OMERO.biobank, thus ensuring reproducibility</p></caption><graphic xlink:href="13062_2015_71_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Example of a Galaxy Workflow. used at CRS4 to generates demultiplexed fastq files starting from an Illumina run directory. The BCL to qseq conversion and the demultiplexing operations are performed on a Hadoop cluster using the Seal toolkit</p></caption><graphic xlink:href="13062_2015_71_Fig3_HTML" id="MO3"/></fig></p><p>The automation daemon also connects multiple workflow operations in sequence, when necessary; for instance, after running the demultiplexing workflow it is configured to run a sample-specific workflow to process each sample dataset. The daemon implements an event-driven model, where events are emitted in the system when something specific happens (e.g., flowcell ready, workflow finished, etc.) and the system is programmed to react to each event type with a specific action. The action may perform some housekeeping task, such as moving files to a specific location, or execute some other workflow.</p><p>To help our operation sustain a high throughput level &#x02013; and to leverage CRS4&#x02019;s computing cluster &#x02013; we implemented some of the more time-consuming and data-intensive processing steps on the Hadoop platform [<xref ref-type="bibr" rid="CR36">36</xref>], and proceeded to integrate these tools with Galaxy [<xref ref-type="bibr" rid="CR37">37</xref>] to compose them with other conventional tools in our bioinformatics workflows.</p><p>In summary, our operation uses Galaxy to define complex operations (workflows) given its familiarity to biologists and bioinformaticians and its REST API, which allows is to supplement it with our own custom automation daemon. On the other hand, we have turned to Hadoop-based tools to improve our computational scalability. Finally, to ensure reproducibility we trace all our automated operations with OMERO.biobank. The entire operation is described in more detail in [<xref ref-type="bibr" rid="CR35">35</xref>].</p></sec><sec id="Sec23"><title>Future challenges</title><p>Future challenges vary in complexity and ambition. At a lower, perhaps simpler, level lies the need to have full reproducibility of these data analyses procedures. To a degree at CRS4 we have achieved this goal by tracing all automated operations with the combination of Galaxy and the OMERO.biobank. However, the system only works with operations that are run and monitored by our automation daemon; therefore, it cannot trace interactive, user-driven operations. In addition, our current solution introduces some complexity in managing of changes in workflows and tools versions. For these issues we currently rely on Galaxy, but its functionality in these terms is limited so alternative solutions will need to be devised or integrated.</p><p>A more ambitious challenge lies in the need to be able to efficiently deal with the steady stream of updates to model data (such as genomic references), bioinformatics tools and analysis procedures. To stay current, all acquired datasets need to be kept in line with the state-of-the-art. This is a very laborious, complex and computationally intensive task which, however, could be automated with the proper support for operating on both data and workflows computationally as first class citizens. With such functionality one could, for instance, update an alignment workflow to use the latest genome reference and automatically find all datasets that had been generated with the previous version and recompute them.</p></sec></sec><sec id="Sec24"><title>Division of Genetic Epidemiology, Medical University of Innsbruck, Austria</title><sec id="Sec25"><title>Overview</title><p>The Medical University of Innsbruck (MUI) is one of the leading centres of medicine in Austria. Within the MUI, the Division of Genetic Epidemiology is an internationally recognized expert on lipid-associated disorders, holds cooperations with several epidemiological studies and is involved in several genome-wide association studies (GWAS) and imputation projects. An intensive cooperation with the research group Databases and Information Systems (DBIS) at the University of Innsbruck exists, developing data-intensive bioinformatics software solutions such as Cloudgene [<xref ref-type="bibr" rid="CR20">20</xref>], HaploGrep [<xref ref-type="bibr" rid="CR38">38</xref>] or the mtDNA-Server (<ext-link ext-link-type="uri" xlink:href="http://mtdna-server.uibk.ac.at">http://mtdna-server.uibk.ac.at</ext-link>). Lately, the developed workflow system Cloudgene has been utilized as the underlying architecture for the Michigan Imputation Server, developed in cooperation with the Department of Biostatistics, University of Michigan. For in-house data analysis, a cloud approach based on a shared-nothing cluster architecture is used for data processing.</p></sec><sec id="Sec26"><title>Workflow experience</title><p>Our institute is especially experienced in providing biomedical workflows as a service to everyone (SaaS). For example, the Michigan Imputation Server (<ext-link ext-link-type="uri" xlink:href="https://imputationserver.sph.umich.edu">https://imputationserver.sph.umich.edu</ext-link>) provides an efficient, user-friendly and free service to impute large-scale population studies using the 1000 Genomes Panel (Phase 1 and 3) or the new HRC Panel. Furthermore, the mtDNA-Server (<ext-link ext-link-type="uri" xlink:href="http://mtdna-server.uibk.ac.at">http://mtdna-server.uibk.ac.at</ext-link>) enables a highly parallelized way to detect heteroplasmies and contamination within mtDNA samples.</p><p>For these time-intensive manipulation and analysis of huge datasets, we mainly focus on the application of Hadoop (hadoop.apache.org). Therefore we developed Cloudgene, a framework for the execution and tracking of Hadoop MapReduce workflows (<ext-link ext-link-type="uri" xlink:href="http://cloudgene.uibk.ac.at">http://cloudgene.uibk.ac.at</ext-link>). This graphical workflow system allows domain experts to run implemented MapReduce workflows directly from their web browsers. Cloudgene is able to combine existing MapReduce programs written in Java, approaches based on the high-level language Apache Pig, command line tools and R-based scripts to a sophisticated workflow. All used parameters and input/output data are tracked ensuring reproducibility and transparency. Final reports are created using R and RMarkdown. Within Cloudgene, workflow steps are defined in a YAML manifest file, the underlying workflow definition language (WDL) supports conditions and loops. Based on this workflow definition, Cloudgene creates user interfaces to submit MapReduce jobs graphically. Since Cloudgene supports the execution of command-line programs and bash scripts, it can also be seen as a generic workflow system. Furthermore, the architecture behind Cloudgene was developed in a way that it is compatible with existing cloud managers such as CloudMan [<xref ref-type="bibr" rid="CR39">39</xref>, <xref ref-type="bibr" rid="CR40">40</xref>]. Thus, the same workflow can be executed on a local infrastructure or on private and public clouds without any adaptions [<xref ref-type="bibr" rid="CR41">41</xref>]. This enables us to develop prototypes of new bioinformatics workflows fast and to provide them as services to other scientists.</p></sec><sec id="Sec27"><title>Future challenges</title><p>Reproducibility of data and software is from our perspective one of the most challenging task in near future. Many publications are presenting software solutions, which are often hard to integrate into a local workflow ore impossible to use due to specific requirements on software packages. We think that cloud-based SaaS approaches applying state-of-the-art pipelines could improve the quality of current data analyses. Of course, Apache Hadoop is not applicable to all kind of problems, but its scalable and open-source nature could result in a boost within Bioinformatics. One goal of Cloudgene is therefore to improve it to an even more generic big data platform by supporting the complete Hadoop YARN architecture. This opens the door to build and execute workflows based on different computational models such as Apache Spark (<ext-link ext-link-type="uri" xlink:href="http://spark.apache.org">http://spark.apache.org</ext-link>) or Apache Giraph (<ext-link ext-link-type="uri" xlink:href="http://giraph.apache.org">http://giraph.apache.org</ext-link>).</p></sec></sec><sec id="Sec28"><title>Faculty of Mathematics and Informatics and ABI/Joint Genomic Centre, Sofia University, Bulgaria</title><sec id="Sec29"><title>Overview</title><p>At the Faculty of Mathematics and Informatics and Joint Genomic Center, both part of Sofia University, we do research projects that require customized workflows. This has been necessary for tasks ranging from biodiversity estimation in metagenomics, to alternative transcript detection in the wheat, maize, sorghum and arabidopsis genomes, as well as SNP discovery in wheat. For this reason, graphical or web-based workflow software designed for easy creation and maintenance of workflows does not suffice for our requirements. We started with shell scripts and then moved to standard Makefiles and as an alternative, our own Python-based bioinformatics workflow system.</p></sec><sec id="Sec30"><title>Workflow experience</title><p>In our experience, more modern workflow systems do not always offer significant advantages. In our bioinformatics projects, the use of Makefiles alone is not enough. During tasks such as NGS assembly, alignment or variant calling, some custom data processing which cannot be implemented in shell pipelines is usually implemented in AWK, Bash or Python scripts. AWK allows compact presentation of simple data processing and is enough in surprisingly many cases. Biopython library has also proved to be very convenient for more complex handling of bioinformatics data files.</p><p>While easy to use and construct, Makefiles are often not flexible enough - their support for parallel jobs cannot take multi-threaded or multi-process jobs into account, and they do not provide any usable means to describe a recursive flow, such as progressive application of multiple alignment for large datasets. Some of the shortcomings can be overcome by using sub-Makefiles, however we thought it would be useful to develop a YAML-based workflow description system inspired by Makefiles. We apply them for the more convoluted problems, but we are hoping to make it generally applicable to simple problems as well [<xref ref-type="bibr" rid="CR42">42</xref>].</p></sec><sec id="Sec31"><title>Future challenges</title><p>Our aim is to build a workflow tool that is as simple as Makefiles, yet one that can make use of more complex functionality. The major challenge is making the system feature-complete and as expressive as Makefiles without sacrificing the simplicity that is inherent in the alternatives. Work is also ongoing to optimize the workflow schema and extension syntax to take maximum advantage of the YAML format. We are looking for expanding our expertise in workflow systems and improving our own tool.</p></sec></sec></sec><sec id="Sec32" sec-type="discussion"><title>Discussion</title><p>The approaches for automating bioinformatics analysis by the organizations at the SeqAhead hackathons and workshops roughly fall into the following categories: scripting (usually in languages such as Bash, Perl, or Python), Makefiles (Make, CMake, etc.), and other workflow systems (such as Snakemake, Luigi, Galax, Taverna, and BcBio). We summarize the main advantages and disadvantages from our point of view in Table <xref rid="Tab1" ref-type="table">1</xref>. One observation is that scientific workflow systems are used in two different ways. There are core workflows that are used for routine processing, are standardized, and rarely change. Then there are research workflows that a bioinformatician creates to run ad hoc analysis, explore the data and try to extract biologically relevant information. These are not and cannot be standardized and indeed the steps and parameters are chosen and modified often as the understanding of the data and the problem changes. We note that several of the organizations are not satisfied with the currently available tools and have resorted to developing in-house tools to better support their specific usage scenarios. We also observe that workflow tools developed and used in other domains, such as astronomy [<xref ref-type="bibr" rid="CR43">43</xref>] in many cases are not widely used in bioinformatics, which may partly be due to a lack of communication between scientists of different field, yet also reflect domain specific needs.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Advantages and disadvantages of different categories of automation strategies for bioinformatics</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Advantages</th><th align="left">Disadvantages</th></tr></thead><tbody><tr><td align="left">Scripting</td><td align="justify">&#x02219; Simple to construct</td><td align="justify">&#x02219; Hard to hand over, manual tools integration and difficult HPC interaction</td></tr><tr><td align="left">Makefile</td><td align="justify">&#x02219; Simple to construct once you are familiar with the programming languages and the bioinformatics command-line tools involved</td><td align="justify">&#x02219; Multithreaded programs and remote execution not handled well</td></tr><tr><td align="left"/><td align="justify">&#x02219; Describes data flow and takes care of dependency resolution, parallel execution and caching results from previous runs</td><td align="justify">&#x02219; Lack of recursion support</td></tr><tr><td align="left"/><td align="justify">&#x02219; Uses code fragments in familiar scripting languages for processing of data</td><td align="justify">&#x02219; Requires programming or shell experience</td></tr><tr><td align="left"/><td align="justify"/><td align="justify">&#x02219; Can&#x02019;t be automatically parsed and visualized</td></tr><tr><td align="left">Scientific Workflow Systems</td><td align="justify">&#x02219; More powerful features, easier to maintain and share</td><td align="justify">&#x02219; Requires more effort</td></tr></tbody></table></table-wrap></p><sec id="Sec33"><title>Scripting</title><p>Shell scripts are compact and tailored for running commands in a specific order. Standard Unix/Linux systems have simple yet powerful commands for text processing, and most bioinformatics tools are available as executables that can be launched from a Shell. Other popular scripting languages like Perl or Python can also launch executables implementing bioinformatics algorithms, and additional functionality is provided by libraries that are often maintained by the community. There are some significant disadvantages to this simple approach to automation. One is that it can be tricky to ensure reproducibility of analyses; or rather, the onus is completely on the individuals who are using the script to document in some way the datasets that have been produced. Moreover, desirable advanced features such as resilience to hardware problems, the ability to re-use intermediate datasets, integration with HPC cluster resources, etc. must all be written from scratch. It can be argued that by the time such features have been integrated into the script one has effectively written a new workflow system, and thus might have been better off adopting one from the start.</p><p>However, scripting also has advantages and hence many adopters. The most important convenience is probably its simplicity and flexibility, meaning that one can very quickly achieve some degree of process automation that works, though it may not be optimal or efficient. Another important advantage is that most bioinformaticians already have scripting experience and are familiar with some scripting languages. By automating through scripts that knowledge can easily be recycled. In the authors&#x02019; experience scripting is not sufficient to provide a fault-tolerant automation for production use.</p></sec><sec id="Sec34"><title>Makefiles</title><p>The standard Unix/Linux solution for automating compilation and other tasks that follow a dependency graph are Makefiles [<xref ref-type="bibr" rid="CR11">11</xref>]. These can serve as a simple yet effective tool to describe bioinformatics workflows, and are applicable to a wide variety of tasks. They describe dependencies between files and commands, and commands can be executed in parallel. Subsequent runs of the workflow use as much as possible of the computation files from previous runs, which serves as a basic form of caching. Drawbacks of Makefiles are their inability to describe dependencies between multiple output files per input file and a lack of support for multiple wildcards in I/O names. Moreover, the standard Unix / Linux Make tool shows limitations when encountering long running operations, and execution on heterogeneous failure-prone distributed resources. To address these issues both general purpose Make implementations, like SCons [<ext-link ext-link-type="uri" xlink:href="http://www.scons.org">http://www.scons.org</ext-link>], PGMake [<xref ref-type="bibr" rid="CR44">44</xref>] or GXP make [<xref ref-type="bibr" rid="CR45">45</xref>] were developed, and bioinformatrics-dedicated systems, like Makeflow [<xref ref-type="bibr" rid="CR46">46</xref>] and Snakemake [<xref ref-type="bibr" rid="CR10">10</xref>]. These tools try to move beyond Makefiles while retaining the simplicity of GNU Make [<xref ref-type="bibr" rid="CR12">12</xref>].</p><p>As Makefiles grow they tend to become very complex. In the authors&#x02019; experience, Makefiles are good for simpler use cases, but have shortcomings when it comes to more complex workflows with multiple steps and branches.</p></sec><sec id="Sec35"><title>Scientific workflows</title><p>Scientific workflow system s provide an environment to interconnect components and in most cases allow for execution on distributed resources. Authors&#x02019; experiences regarding their utility vary. While all acknowledge the power and importance of scientific workflow systems to enable reproducible data analysis and simplified integration with HPC systems, in practice it turns out that many projects have started using workflow tools and frameworks but later switched back to custom scripting and Makefiles (or similar) since they discovered limitations of the systems, especially with the pressure to deliver results faster in an internationally competitive environment.</p><p>An important remaining challenge is the standardization of data flow in workflow systems. There have been several attempts to address this issue, where some are based on describing common data types via a dedicated XML schema [<xref ref-type="bibr" rid="CR47">47</xref>, <xref ref-type="bibr" rid="CR48">48</xref>] or introducing ontology-based methods for managing data types [<xref ref-type="bibr" rid="CR49">49</xref>]. No particular approach, however, has yet emerged that could substantially impact the field or find widespread acceptance in the bioinformatics community. With no central authority to dictate standards for interoperability, the community can only develop standards through collaborative efforts like the EU COST action SeqAhead.</p></sec><sec id="Sec36"><title>Key insights</title><p><list list-type="bullet"><list-item><p>Automation on shared HPC clusters is difficult, and workflow tools can aid in achieving it.</p></list-item><list-item><p>Full analysis reproducibility is hard, sometimes impossible to achieve. This has two reasons: i) large scale analysis very often relies on external databases that commonly are not versioned, or even if they are versioned only &#x02019;milestone&#x02019; versions are available, ii) scientific software management is on one hand inefficient in HPC clusters while on the other hand usage of the Web Services might be risky due to instability and lacking versioning. Community efforts for standardized software packaging and versioning are also lacking.</p></list-item><list-item><p>The available log processing and provenance systems are not good enough. These would provide better reproducibility, monitoring, and analytics.</p></list-item><list-item><p>Bioinformatics analyses are currently to a large extent file-based and there is no standardized way of passing data between applications in a workflow. This would require a transparent conversion of data formats with the resulting technical as well as semantic challenges [<xref ref-type="bibr" rid="CR50">50</xref>]. In addition there is a necessity to check the consistency of the produced data. Although these tasks do not form a &#x02018;research&#x02019; part of the workflow they can still constitute the majority of the workload in a typical analysis [<xref ref-type="bibr" rid="CR51">51</xref>].</p></list-item><list-item><p>Biological validation of workflows is typically missing. In other words, integration with a reference biological dataset, such as genome in a bottle, and accompanying test suite that validates biology across changes to external data or tools, as well as workflow revisions is, unfortunately, not a common practice today.</p></list-item><list-item><p>Makefiles are a quick way to get the work done in a seemingly efficient manner, but the standard Make tool can become limiting when more advanced features are required. New efficient tools have been developed to address these issues (e.g. Snakemake, Bpipe).</p></list-item><list-item><p>Scripting is common for analysis development, but we see a move to Workflow tools for data production that has strict requirements to support audits, or similar.</p></list-item><list-item><p>Workflow systems on HPC resources have advantageous performance over cloud computing resources, but software installation is simplified on cloud systems, which can also more suitable for interactive use.</p></list-item></list></p></sec></sec><sec id="Sec37" sec-type="conclusion"><title>Conclusions</title><p>Many researchers have similar problems in data-intensive bioinformatics analyses. In the authors&#x02019; experience the trend is clear in bioinformatics &#x02014; workflow tools are getting increasingly more powerful, user-friendly, and hence more frequently used and appreciated for automation and creation of research pipelines. Nevertheless, the authors had to develop different ways of resolving remaining issues, which is clearly inefficient and leaves considerable room for future improvements in next-generation workflow systems.</p><p>Apart from using workflow systems the authors have developed new tools aiding workflow construction (Chipster, CloudGene, SeqPig), contributed to other workflow systems (Galaxy, Snakemake, Chipster) and analyzed frameworks such as Hadoop and Spark. Based on these experiences we have devised a set of recommendations for the next-generation automation systems for bioinformatics.</p><sec id="Sec38"><title>Recommendations for future workflow systems</title><p>Bioinformatics analysis are currently to a large extent file-based, and as long as this will be the case workflow tools will continue to be important for bioinformatics automation. Even though exciting new data analytics frameworks such as Hadoop and Spark provide alternatives, with high up-front costs and the so far low uptake in the bioinformatics community we do not see a shift in paradigm within the nearest years.</p><p>The harder it is for a scientist to use a system compared to an ad-hoc hack, script, or perhaps a suboptimal stand-alone tool, the lower the widespread acceptance of a workflow system is in the wider bioinformatics and computational biology community. In general, we therefore recommend further development of lightweight and layered systems, where at least the basic functionality is easily accessed. More specifically: 
<list list-type="bullet"><list-item><p>Maintain as much reproducibility as possible without sacrificing usability and simplicity of design and execution.</p></list-item><list-item><p>Keep things simple, lightweight, easy to install and integrate with Bash and scripting languages.</p></list-item><list-item><p>Workflows should be easily executed, with little or no change in local and distributed environments (HPC and cloud).</p></list-item><list-item><p>Encourage attempts for further data flow standardization and data versioning as well as standardized software management.</p></list-item><list-item><p>Put more effort into (biological) testing, validation, continuous delivery and deployment of the software. In other words, spend more effort on quality assurance.</p></list-item></list></p></sec></sec><sec id="Sec39"><title>Reviewer&#x02019;s report</title><sec id="Sec40"><title>Reviewer 1: Dr Andrew Clark</title><p>The authors represent a number of impressive bioinformatics operations and share some valuable insight and experiences in this paper. Research sites engaged in similar work will no doubt relate to many of these realistic lessons learned.</p><p>The authors are right to emphasize the need for further work in the bioinformatics community on: shared community-wide standards for data, more rigor/higher quality in bioinformatics software engineering, and reproducible research/workflow methods.</p><p>I do think that hearing more real world experiences from groups fully committed to using a scientific workflow management system (WMS) would improve the concluding discussion and comparison for/against each workflow option. This perspective seems underrepresented in the views expressed. Other research fields that require data-intensive computing workflows (e.g. astronomy, physics, neuro imaging) have contributed some robust APIs and tools which are equally viable for bioinformatics applications. But such WMS options are not given much real estate in favor of quicker solutions.</p><p>Author&#x02019;s response: <italic>We agree with the reviewer and have added a note and reference to the discussion on workflow tools which from other domains which could be useful in bioinformatics.</italic></p></sec></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>NGS</term><def><p>Next-generation sequencing</p></def></def-item><def-item><term>HPC</term><def><p>High-performance computing</p></def></def-item><def-item><term>VMI</term><def><p>Virtual machine image</p></def></def-item><def-item><term>PaaS</term><def><p>Platform-as-a-service</p></def></def-item><def-item><term>SaaS</term><def><p>Software-as-a-service</p></def></def-item><def-item><term>RSE</term><def><p>Research software engineer</p></def></def-item><def-item><term>GWAS</term><def><p>Genome-wide association study</p></def></def-item><def-item><term>WDL</term><def><p>Workflow definition language</p></def></def-item></def-list></glossary><fn-group><fn><p><bold>Competing interests</bold></p><p>The authors declare that they have no competing interests.</p></fn><fn><p><bold>Authors&#x02019; contributions</bold></p><p>OS, EBR, GCH, LF, MG, RVG, AK, EK, MMK, MK, DPK, OK, PPL, SL, LP, SS, AS and DV contributed with workflow experiences. OS coordinated the manuscript preparation. All authors read and approved the manuscript.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The hackathons and workshops leading to this compilation were supported by COST Action BM1006 Next Generation Sequencing Data Analysis Network SeqAhead. OS and AS were supported by the Swedish strategic research programme eSSENCE. LP&#x02019;s activity was performed within the context of the Ph.D. program in Biomedical Engineering at the University of Cagliari, Italy. MK and OK were supported by National Science Fund of Bulgaria within the &#x0201c;Methods for Data Analysis and Knowledge Discovery in Big Sequencing Dataset&#x0201d; project under contract DFNI02/7 of 12.12.2014.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marx</surname><given-names>V</given-names></name></person-group><article-title>Biology: The big challenges of big data</article-title><source>Nature</source><year>2013</year><volume>498</volume><issue>7453</issue><fpage>255</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1038/498255a</pub-id><?supplied-pmid 23765498?><pub-id pub-id-type="pmid">23765498</pub-id></element-citation></ref><ref id="CR2"><label>2</label><mixed-citation publication-type="other">Bux M, Leser U. Parallelization in Scientific Workflow Management Systems. ArXiv e-prints. 2013. 1303.7195.</mixed-citation></ref><ref id="CR3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oinn</surname><given-names>T</given-names></name><name><surname>Addis</surname><given-names>M</given-names></name><name><surname>Ferris</surname><given-names>J</given-names></name><name><surname>Marvin</surname><given-names>D</given-names></name><name><surname>Senger</surname><given-names>M</given-names></name><name><surname>Greenwood</surname><given-names>M</given-names></name><etal/></person-group><article-title>Taverna: a tool for the composition and enactment of bioinformatics workflows</article-title><source>Bioinformatics</source><year>2004</year><volume>20</volume><issue>17</issue><fpage>3045</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bth361</pub-id><?supplied-pmid 15201187?><pub-id pub-id-type="pmid">15201187</pub-id></element-citation></ref><ref id="CR4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blankenberg</surname><given-names>D</given-names></name><name><surname>Von Kuster</surname><given-names>G</given-names></name><name><surname>Coraor</surname><given-names>N</given-names></name><name><surname>Ananda</surname><given-names>G</given-names></name><name><surname>Lazarus</surname><given-names>R</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><etal/></person-group><article-title>Galaxy: a web-based genome analysis tool for experimentalists</article-title><source>Curr Protoc Mol Biol</source><year>2010</year><volume>Chapter 19</volume><fpage>19.10.1</fpage><lpage>21</lpage></element-citation></ref><ref id="CR5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giardine</surname><given-names>B</given-names></name><name><surname>Riemer</surname><given-names>C</given-names></name><name><surname>Hardison</surname><given-names>RC</given-names></name><name><surname>Burhans</surname><given-names>R</given-names></name><name><surname>Elnitski</surname><given-names>L</given-names></name><name><surname>Shah</surname><given-names>P</given-names></name><etal/></person-group><article-title>Galaxy: a platform for interactive large-scale genome analysis</article-title><source>Genome Res</source><year>2005</year><volume>15</volume><issue>10</issue><fpage>1451</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1101/gr.4086505</pub-id><?supplied-pmid 16169926?><pub-id pub-id-type="pmid">16169926</pub-id></element-citation></ref><ref id="CR6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goecks</surname><given-names>J</given-names></name><name><surname>Nekrutenko</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><collab>Galaxy Team</collab></person-group><article-title>Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences</article-title><source>Genome Biol</source><year>2010</year><volume>11</volume><issue>8</issue><fpage>86</fpage><pub-id pub-id-type="doi">10.1186/gb-2010-11-8-r86</pub-id></element-citation></ref><ref id="CR7"><label>7</label><mixed-citation publication-type="other">Altintas I, Berkley C, Jaeger E, Jones M, Ludascher B, Mock S. Kepler: an extensible system for design and execution of scientific workflows. In: Scientific and Statistical Database Management, 2004. Proceedings. 16th International Conference On: 2004. p. 423&#x02013;4. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/SSDM.2004.1311241">10.1109/SSDM.2004.1311241</ext-link>.</mixed-citation></ref><ref id="CR8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kallio</surname><given-names>MA</given-names></name><name><surname>Tuimala</surname><given-names>JT</given-names></name><name><surname>Hupponen</surname><given-names>T</given-names></name><name><surname>Klemel&#x000e4;</surname><given-names>P</given-names></name><name><surname>Gentile</surname><given-names>M</given-names></name><name><surname>Scheinin</surname><given-names>I</given-names></name><etal/></person-group><article-title>Chipster: user-friendly analysis software for microarray and other high-throughput data</article-title><source>BMC Genomics</source><year>2011</year><volume>12</volume><fpage>507</fpage><pub-id pub-id-type="doi">10.1186/1471-2164-12-507</pub-id><?supplied-pmid 21999641?><pub-id pub-id-type="pmid">21999641</pub-id></element-citation></ref><ref id="CR9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadedin</surname><given-names>SP</given-names></name><name><surname>Pope</surname><given-names>B</given-names></name><name><surname>Oshlack</surname><given-names>A</given-names></name></person-group><article-title>Bpipe: a tool for running and managing bioinformatics pipelines</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><issue>11</issue><fpage>1525</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts167</pub-id><?supplied-pmid 22500002?><pub-id pub-id-type="pmid">22500002</pub-id></element-citation></ref><ref id="CR10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>K&#x000f6;ster</surname><given-names>J</given-names></name><name><surname>Rahmann</surname><given-names>S</given-names></name></person-group><article-title>Snakemake&#x02013;a scalable bioinformatics workflow engine</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><issue>19</issue><fpage>2520</fpage><lpage>2</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts480</pub-id><?supplied-pmid 22908215?><pub-id pub-id-type="pmid">22908215</pub-id></element-citation></ref><ref id="CR11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>SI</given-names></name></person-group><article-title>Make - a program for maintaining computer programs a program for maintaining computer programs</article-title><source>Softw Pract Experience</source><year>1979</year><volume>9</volume><issue>4</issue><fpage>255</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1002/spe.4380090402</pub-id></element-citation></ref><ref id="CR12"><label>12</label><mixed-citation publication-type="other">Schwab M, Schroeder J. Reproducible research documents using gnumake. In: Stanford Exploration Project: 1995. p. 217&#x02013;26.</mixed-citation></ref><ref id="CR13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schatz</surname><given-names>M</given-names></name><name><surname>Langmead</surname><given-names>B</given-names></name><name><surname>Salzberg</surname><given-names>S</given-names></name></person-group><article-title>Cloud computing and the DNA data race</article-title><source>Nat Biotechnol</source><year>2010</year><volume>28</volume><fpage>691</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1038/nbt0710-691</pub-id><?supplied-pmid 20622843?><pub-id pub-id-type="pmid">20622843</pub-id></element-citation></ref><ref id="CR14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>L</given-names></name></person-group><article-title>The case for cloud computing in genome informatics</article-title><source>Genome Biol</source><year>2010</year><volume>11</volume><fpage>207</fpage><pub-id pub-id-type="doi">10.1186/gb-2010-11-5-207</pub-id><?supplied-pmid 20441614?><pub-id pub-id-type="pmid">20441614</pub-id></element-citation></ref><ref id="CR15"><label>15</label><mixed-citation publication-type="other">Dean J, Ghemawat S. MapReduce: Simplified data processing on large clusters. In: Sixth Symposium on Operating System Design and Implementation: 2004; San Francisco, CA. 2004.</mixed-citation></ref><ref id="CR16"><label>16</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>White</surname><given-names>T</given-names></name></person-group><source>Hadoop: The Definitive Guide</source><year>2009</year><publisher-loc>Sebastopol, CA</publisher-loc><publisher-name>O&#x02019;Reilly</publisher-name></element-citation></ref><ref id="CR17"><label>17</label><mixed-citation publication-type="other">Zaharia M, Chowdhury M, Franklin MJ, Shenker S, Stoica I. Spark: cluster computing with working sets. In: Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing: 2010. p. 10&#x02013;10.</mixed-citation></ref><ref id="CR18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lampa</surname><given-names>S</given-names></name><name><surname>Dahl&#x000f6;</surname><given-names>M</given-names></name><name><surname>Olason</surname><given-names>PI</given-names></name><name><surname>Hagberg</surname><given-names>J</given-names></name><name><surname>Spjuth</surname><given-names>O</given-names></name></person-group><article-title>Lessons learned from implementing a national infrastructure in sweden for storage and analysis of next-generation sequencing data</article-title><source>Gigascience</source><year>2013</year><volume>2</volume><issue>1</issue><fpage>9</fpage><pub-id pub-id-type="doi">10.1186/2047-217X-2-9</pub-id><?supplied-pmid 23800020?><pub-id pub-id-type="pmid">23800020</pub-id></element-citation></ref><ref id="CR19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodr&#x000ed;guez</surname><given-names>D</given-names></name><name><surname>Bello</surname><given-names>X</given-names></name><name><surname>Guti&#x000e9;rrez-de-Ter&#x000e1;n</surname><given-names>H</given-names></name></person-group><article-title>Molecular modelling of g protein-coupled receptors through the web</article-title><source>Mol Inf</source><year>2012</year><volume>31</volume><issue>5</issue><fpage>334</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1002/minf.201100162</pub-id></element-citation></ref><ref id="CR20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sch&#x000f6;nherr</surname><given-names>S</given-names></name><name><surname>Forer</surname><given-names>L</given-names></name><name><surname>Wei&#x000df;ensteiner</surname><given-names>H</given-names></name><name><surname>Kronenberg</surname><given-names>F</given-names></name><name><surname>Specht</surname><given-names>G</given-names></name><name><surname>Kloss-Brandst&#x000e4;tter</surname><given-names>A</given-names></name></person-group><article-title>Cloudgene: a graphical execution platform for mapreduce programs on private and public clouds</article-title><source>BMC Bioinformatics</source><year>2012</year><volume>13</volume><fpage>200</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-13-200</pub-id><?supplied-pmid 22888776?><pub-id pub-id-type="pmid">22888776</pub-id></element-citation></ref><ref id="CR21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siretskiy</surname><given-names>A</given-names></name><name><surname>Sundqvist</surname><given-names>T</given-names></name><name><surname>Voznesenskiy</surname><given-names>M</given-names></name><name><surname>Spjuth</surname><given-names>O</given-names></name></person-group><article-title>A quantitative assessment of the hadoop framework for analyzing massively parallel dna sequencing data</article-title><source>Gigascience</source><year>2015</year><volume>4</volume><fpage>26</fpage><pub-id pub-id-type="doi">10.1186/s13742-015-0058-5</pub-id><?supplied-pmid 26045962?><pub-id pub-id-type="pmid">26045962</pub-id></element-citation></ref><ref id="CR22"><label>22</label><mixed-citation publication-type="other">Siretskiy A, Spjuth O. Htseq-hadoop: Extending htseq for massively parallel sequencing data analysis using hadoop. In: eScience (eScience), 2014 IEEE 10th International Conference On: 2014.</mixed-citation></ref><ref id="CR23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anders</surname><given-names>S</given-names></name><name><surname>Pyl</surname><given-names>PT</given-names></name><name><surname>Huber</surname><given-names>W</given-names></name></person-group><article-title>Htseq-a python framework to work with high-throughput sequencing data</article-title><source>Bioinformatics</source><year>2015</year><volume>31</volume><issue>2</issue><fpage>166</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu638</pub-id><?supplied-pmid 25260700?><pub-id pub-id-type="pmid">25260700</pub-id></element-citation></ref><ref id="CR24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>SEQC/MAQC-III Consortium</collab></person-group><article-title>A comprehensive assessment of rna-seq accuracy, reproducibility and information content by the sequencing quality control consortium</article-title><source>Nat Biotechnol</source><year>2014</year><volume>32</volume><issue>9</issue><fpage>903</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/nbt.2957</pub-id><pub-id pub-id-type="pmid">25150838</pub-id></element-citation></ref><ref id="CR25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>&#x00141;abaj</surname><given-names>PP</given-names></name><name><surname>Zumbo</surname><given-names>P</given-names></name><name><surname>Sykacek</surname><given-names>P</given-names></name><name><surname>Shi</surname><given-names>W</given-names></name><name><surname>Shi</surname><given-names>L</given-names></name><etal/></person-group><article-title>Detecting and correcting systematic variation in large-scale rna sequencing data</article-title><source>Nat Biotechnol</source><year>2014</year><volume>32</volume><issue>9</issue><fpage>888</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1038/nbt.3000</pub-id><?supplied-pmid 25150837?><pub-id pub-id-type="pmid">25150837</pub-id></element-citation></ref><ref id="CR26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueckstein</surname><given-names>U</given-names></name><name><surname>Leparc</surname><given-names>GG</given-names></name><name><surname>Posekany</surname><given-names>A</given-names></name><name><surname>Hofacker</surname><given-names>I</given-names></name><name><surname>Kreil</surname><given-names>DP</given-names></name></person-group><article-title>Hybridization thermodynamics of nimblegen microarrays</article-title><source>BMC Bioinformatics</source><year>2010</year><volume>11</volume><fpage>35</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-11-35</pub-id><?supplied-pmid 20085625?><pub-id pub-id-type="pmid">20085625</pub-id></element-citation></ref><ref id="CR27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leparc</surname><given-names>GG</given-names></name><name><surname>T&#x000fc;chler</surname><given-names>T</given-names></name><name><surname>Striedner</surname><given-names>G</given-names></name><name><surname>Bayer</surname><given-names>K</given-names></name><name><surname>Sykacek</surname><given-names>P</given-names></name><name><surname>Hofacker</surname><given-names>IL</given-names></name><etal/></person-group><article-title>Model-based probe set optimization for high-performance microarrays</article-title><source>Nucleic Acids Res</source><year>2009</year><volume>37</volume><issue>3</issue><fpage>18</fpage><pub-id pub-id-type="doi">10.1093/nar/gkn1001</pub-id></element-citation></ref><ref id="CR28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodstadt</surname><given-names>L</given-names></name></person-group><article-title>Ruffus: a lightweight python library for computational pipelines</article-title><source>Bioinformatics</source><year>2010</year><volume>26</volume><issue>21</issue><fpage>2778</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btq524</pub-id><?supplied-pmid 20847218?><pub-id pub-id-type="pmid">20847218</pub-id></element-citation></ref><ref id="CR29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Cancer Genome Atlas Research Network</collab><name><surname>Weinstein</surname><given-names>JN</given-names></name><name><surname>Collisson</surname><given-names>EA</given-names></name><name><surname>Mills</surname><given-names>GB</given-names></name><name><surname>Shaw</surname><given-names>KRM</given-names></name><name><surname>Ozenberger</surname><given-names>BA</given-names></name><etal/></person-group><article-title>The cancer genome atlas pan-cancer analysis project</article-title><source>Nat Genet</source><year>2013</year><volume>45</volume><issue>10</issue><fpage>1113</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1038/ng.2764</pub-id><pub-id pub-id-type="pmid">24071849</pub-id></element-citation></ref><ref id="CR30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niemenmaa</surname><given-names>M</given-names></name><name><surname>Kallio</surname><given-names>A</given-names></name><name><surname>Schumacher</surname><given-names>A</given-names></name><name><surname>Klemel&#x000e4;</surname><given-names>P</given-names></name><name><surname>Korpelainen</surname><given-names>E</given-names></name><name><surname>Heljanko</surname><given-names>K</given-names></name></person-group><article-title>Hadoop-bam: directly manipulating next generation sequencing data in the cloud</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><issue>6</issue><fpage>876</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts054</pub-id><?supplied-pmid 22302568?><pub-id pub-id-type="pmid">22302568</pub-id></element-citation></ref><ref id="CR31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schumacher</surname><given-names>A</given-names></name><name><surname>Pireddu</surname><given-names>L</given-names></name><name><surname>Niemenmaa</surname><given-names>M</given-names></name><name><surname>Kallio</surname><given-names>A</given-names></name><name><surname>Korpelainen</surname><given-names>E</given-names></name><name><surname>Zanetti</surname><given-names>G</given-names></name><etal/></person-group><article-title>Seqpig: simple and scalable scripting for large sequencing data sets in hadoop</article-title><source>Bioinformatics</source><year>2014</year><volume>30</volume><issue>1</issue><fpage>119</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btt601</pub-id><?supplied-pmid 24149054?><pub-id pub-id-type="pmid">24149054</pub-id></element-citation></ref><ref id="CR32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merali</surname><given-names>Z</given-names></name></person-group><article-title>Computational science:...error</article-title><source>Nature</source><year>2010</year><volume>467</volume><issue>7317</issue><fpage>775</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/467775a</pub-id><?supplied-pmid 20944712?><pub-id pub-id-type="pmid">20944712</pub-id></element-citation></ref><ref id="CR33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orr&#x000f9;</surname><given-names>V</given-names></name><name><surname>Steri</surname><given-names>M</given-names></name><name><surname>Sole</surname><given-names>G</given-names></name><name><surname>Sidore</surname><given-names>C</given-names></name><name><surname>Virdis</surname><given-names>F</given-names></name><name><surname>Dei</surname><given-names>M</given-names></name><etal/></person-group><article-title>Genetic variants regulating immune cell levels in health and disease.</article-title><source>Cell</source><year>2013</year><volume>155</volume><issue>1</issue><fpage>242</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2013.08.041</pub-id><?supplied-pmid 24074872?><pub-id pub-id-type="pmid">24074872</pub-id></element-citation></ref><ref id="CR34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francalacci</surname><given-names>P</given-names></name><name><surname>Morelli</surname><given-names>L</given-names></name><name><surname>Angius</surname><given-names>A</given-names></name><name><surname>Berutti</surname><given-names>R</given-names></name><name><surname>Reinier</surname><given-names>F</given-names></name><name><surname>Atzeni</surname><given-names>R</given-names></name><etal/></person-group><article-title>Low-pass DNA sequencing of 1200 Sardinians reconstructs European Y-chromosome phylogeny.</article-title><source>Science (New York, N.Y.)</source><year>2013</year><volume>341</volume><issue>6145</issue><fpage>565</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1126/science.1237947</pub-id></element-citation></ref><ref id="CR35"><label>35</label><mixed-citation publication-type="other">Cuccuru G, Leo S, Lianas L, Muggiri M, Pinna A, Pireddu L, et al.An automated infrastructure to support high-throughput bioinformatics. In: High Performance Computing Simulation (HPCS), 2014 International Conference On: 2014. p. 600&#x02013;7. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/HPCSim.2014.6903742">10.1109/HPCSim.2014.6903742</ext-link>.</mixed-citation></ref><ref id="CR36"><label>36</label><mixed-citation publication-type="other">Pireddu L, Leo S, Zanetti G. Seal: a distributed short read mapping and duplicate removal tool. Bioinformatics. 2011. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bioinformatics/btr325">10.1093/bioinformatics/btr325</ext-link>. <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/content/early/2011/06/22/bioinformatics.btr325.full.pdfhtml">http://bioinformatics.oxfordjournals.org/content/early/2011/06/22/bioinformatics.btr325.full.pdfhtml</ext-link>.</mixed-citation></ref><ref id="CR37"><label>37</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pireddu</surname><given-names>L</given-names></name><name><surname>Leo</surname><given-names>S</given-names></name><name><surname>Soranzo</surname><given-names>N</given-names></name><name><surname>Zanetti</surname><given-names>G</given-names></name></person-group><article-title>A hadoop-galaxy adapter for user-friendly and scalable data-intensive bioinformatics in galaxy</article-title><source>Proceedings of the 5<sup><italic>th</italic></sup> ACM Conference on Bioinformatics, Computational Biology, and Health Informatics. BCB &#x02019;14</source><year>2014</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>ACM</publisher-name></element-citation></ref><ref id="CR38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kloss-Brandst&#x000e4;tter</surname><given-names>A</given-names></name><name><surname>Pacher</surname><given-names>D</given-names></name><name><surname>Sch&#x000f6;nherr</surname><given-names>S</given-names></name><name><surname>Weissensteiner</surname><given-names>H</given-names></name><name><surname>Binna</surname><given-names>R</given-names></name><name><surname>Specht</surname><given-names>G</given-names></name><etal/></person-group><article-title>Haplogrep: a fast and reliable algorithm for automatic classification of mitochondrial dna haplogroups</article-title><source>Hum Mutat</source><year>2011</year><volume>32</volume><issue>1</issue><fpage>25</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1002/humu.21382</pub-id><?supplied-pmid 20960467?><pub-id pub-id-type="pmid">20960467</pub-id></element-citation></ref><ref id="CR39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afgan</surname><given-names>E</given-names></name><name><surname>Baker</surname><given-names>D</given-names></name><name><surname>Coraor</surname><given-names>N</given-names></name><name><surname>Chapman</surname><given-names>B</given-names></name><name><surname>Nekrutenko</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name></person-group><article-title>Galaxy cloudman: delivering cloud compute clusters</article-title><source>BMC Bioinformatics</source><year>2010</year><volume>11</volume><issue>Suppl 12</issue><fpage>4</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-11-S12-S4</pub-id><pub-id pub-id-type="pmid">20047649</pub-id></element-citation></ref><ref id="CR40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afgan</surname><given-names>E</given-names></name><name><surname>Chapman</surname><given-names>B</given-names></name><name><surname>Jadan</surname><given-names>M</given-names></name><name><surname>Franke</surname><given-names>V</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name></person-group><article-title>Using cloud computing infrastructure with cloudbiolinux, cloudman, and galaxy</article-title><source>Curr Protoc Bioinformatics</source><year>2012</year><volume>Chapter 11</volume><fpage>11</fpage><lpage>9</lpage></element-citation></ref><ref id="CR41"><label>41</label><mixed-citation publication-type="other">Forer L, Lipic T, Schonherr S, Weisensteiner H, Davidovic D, Kronenberg F, et al.Delivering bioinformatics mapreduce applications in the cloud. In: Information and Communication Technology, Electronics and Microelectronics (MIPRO), 2014 37th International Convention On: 2014. p. 373&#x02013;7. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1109/MIPRO.2014.6859593">10.1109/MIPRO.2014.6859593</ext-link>.</mixed-citation></ref><ref id="CR42"><label>42</label><mixed-citation publication-type="other">Krachunov M. Hierarchy and expressions for automated workflows for ngs data processing. In: Proceedings of the 8th International Conference on Information Systems &#x00026; Grid Technologies (ISGT). Sofia, Bulgaria: 2015. p. 38&#x02013;48.</mixed-citation></ref><ref id="CR43"><label>43</label><mixed-citation publication-type="other">Schaaff A, Verdes-Montenegro L, Ruiz J, Vela JS. Scientific workflows in astronomy. In: Proceeding of Astronomical Data Analysis Software and Systems: 2012.</mixed-citation></ref><ref id="CR44"><label>44</label><mixed-citation publication-type="other">Lih A, Zadok E. Pgmake: A portable distributed make system. 1994. Technical report.</mixed-citation></ref><ref id="CR45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taura</surname><given-names>K</given-names></name><name><surname>Matsuzaki</surname><given-names>T</given-names></name><name><surname>Miwa</surname><given-names>M</given-names></name><name><surname>Kamoshida</surname><given-names>Y</given-names></name><name><surname>Yokoyama</surname><given-names>D</given-names></name><name><surname>Dun</surname><given-names>N</given-names></name><etal/></person-group><article-title>Design and implementation of gxp make - a workflow system based on make</article-title><source>Future Gener Comput Syst</source><year>2013</year><volume>29</volume><issue>2</issue><fpage>662</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.future.2011.05.026</pub-id></element-citation></ref><ref id="CR46"><label>46</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Albrecht</surname><given-names>M</given-names></name><name><surname>Donnelly</surname><given-names>P</given-names></name><name><surname>Bui</surname><given-names>P</given-names></name><name><surname>Thain</surname><given-names>D</given-names></name></person-group><article-title>Makeflow: A portable abstraction for data intensive computing on clusters, clouds, and grids</article-title><source>Proceedings of the 1st ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies. SWEET &#x02019;12</source><year>2012</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>ACM</publisher-name></element-citation></ref><ref id="CR47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seibel</surname><given-names>P</given-names></name><name><surname>Kruger</surname><given-names>J</given-names></name><name><surname>Hartmeier</surname><given-names>S</given-names></name><name><surname>Schwarzer</surname><given-names>K</given-names></name><name><surname>Lowenthal</surname><given-names>K</given-names></name><name><surname>Mersch</surname><given-names>H</given-names></name><etal/></person-group><article-title>XML schemas for common bioinformatic data types and their application in workflow systems</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>7</volume><issue>1</issue><fpage>490</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-7-490</pub-id><?supplied-pmid 17087823?><pub-id pub-id-type="pmid">17087823</pub-id></element-citation></ref><ref id="CR48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalas</surname><given-names>M</given-names></name><name><surname>Puntervoll</surname><given-names>P</given-names></name><name><surname>Joseph</surname><given-names>A</given-names></name><name><surname>Bartaseviciute</surname><given-names>E</given-names></name><name><surname>T&#x000f6;pfer</surname><given-names>A</given-names></name><name><surname>Venkataraman</surname><given-names>P</given-names></name><etal/></person-group><article-title>Bioxsd: the common data-exchange format for everyday bioinformatics web services</article-title><source>Bioinformatics</source><year>2010</year><volume>26</volume><issue>18</issue><fpage>540</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btq391</pub-id></element-citation></ref><ref id="CR49"><label>49</label><mixed-citation publication-type="other">Wilkinson M. Interoperability With Moby 1.0 - It&#x02019;s Better Than Sharing Your Toothbrush!. 2008. Available from Nature Precedings.</mixed-citation></ref><ref id="CR50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linke</surname><given-names>B</given-names></name><name><surname>Giegerich</surname><given-names>R</given-names></name><name><surname>Goesmann</surname><given-names>A</given-names></name></person-group><article-title>Conveyor: a workflow engine for bioinformatic analyses</article-title><source>Bioinformatics</source><year>2011</year><volume>27</volume><issue>7</issue><fpage>903</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btr040</pub-id><?supplied-pmid 21278189?><pub-id pub-id-type="pmid">21278189</pub-id></element-citation></ref><ref id="CR51"><label>51</label><mixed-citation publication-type="other">Wassink I, van der Vet PE, Wolstencroft K, Neerincx PBT, Roos M, Rauwerda H, et al.Analysing scientific workflows: Why workflows not only connect web services. In: Services - I, 2009 World Conference On: 2009. p. 314&#x02013;21.</mixed-citation></ref></ref-list></back></article>