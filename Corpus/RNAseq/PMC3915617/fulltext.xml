<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="product-review" xml:lang="en"><?DTDIdentifier.IdentifierValue http://www.biomedcentral.com/xml/article.dtd?><?DTDIdentifier.IdentifierType system?><?SourceDTD.DTDName article.dtd?><?SourceDTD.Version 1.0?><?ConverterInfo.XSLTName bmc2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id><journal-title-group><journal-title>BMC Bioinformatics</journal-title></journal-title-group><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">3915617</article-id><article-id pub-id-type="publisher-id">1471-2105-15-38</article-id><article-id pub-id-type="pmid">24495746</article-id><article-id pub-id-type="doi">10.1186/1471-2105-15-38</article-id><article-categories><subj-group subj-group-type="heading"><subject>Software</subject></subj-group></article-categories><title-group><article-title>DRUMS: Disk Repository with Update Management and Select option for high throughput sequencing data</article-title></title-group><contrib-group><contrib contrib-type="author" id="A1"><name><surname>Nettling</surname><given-names>Martin</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I2">2</xref><email>martin.nettling@unister.de</email></contrib><contrib contrib-type="author" id="A2"><name><surname>Thieme</surname><given-names>Nils</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>nils.thieme@unister.de</email></contrib><contrib contrib-type="author" corresp="yes" id="A3"><name><surname>Both</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="I2">2</xref><email>andreas.both@unister.de</email></contrib><contrib contrib-type="author" id="A4"><name><surname>Grosse</surname><given-names>Ivo</given-names></name><xref ref-type="aff" rid="I1">1</xref><xref ref-type="aff" rid="I3">3</xref><email>ivo.grosse@informatik.uni-halle.de</email></contrib></contrib-group><aff id="I1"><label>1</label>Institute of Computer Science, Martin Luther University, Halle (Saale), Germany</aff><aff id="I2"><label>2</label>R&#x00026;D, Unister GmbH, Leipzig, Germany</aff><aff id="I3"><label>3</label>German Centre for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Leipzig, Germany</aff><pub-date pub-type="collection"><year>2014</year></pub-date><pub-date pub-type="epub"><day>4</day><month>2</month><year>2014</year></pub-date><volume>15</volume><fpage>38</fpage><lpage>38</lpage><history><date date-type="received"><day>31</day><month>7</month><year>2013</year></date><date date-type="accepted"><day>17</day><month>1</month><year>2014</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2014 Nettling et al.; licensee BioMed Central Ltd.</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Nettling et al.; licensee BioMed Central Ltd.</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/2.0"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri xlink:href="http://www.biomedcentral.com/1471-2105/15/38"/><abstract><sec><title>Background</title><p>New technologies for analyzing biological samples, like next generation sequencing, are producing a growing amount of data together with quality scores. Moreover, software tools (e.g., for mapping sequence reads), calculating transcription factor binding probabilities, estimating epigenetic modification enriched regions or determining single nucleotide polymorphism increase this amount of position-specific DNA-related data even further. Hence, requesting data becomes challenging and expensive and is often implemented using specialised hardware. In addition, picking specific data as fast as possible becomes increasingly important in many fields of science. The general problem of handling big data sets was addressed by developing specialized databases like HBase, HyperTable or Cassandra. However, these database solutions require also specialized or distributed hardware leading to expensive investments. To the best of our knowledge, there is no database capable of (i) storing billions of position-specific DNA-related records, (ii) performing fast and resource saving requests, and (iii) running on a single standard computer hardware.</p></sec><sec><title>Results</title><p>Here, we present DRUMS (Disk Repository with Update Management and Select option), satisfying demands (i)-(iii). It tackles the weaknesses of traditional databases while handling position-specific DNA-related data in an efficient manner. DRUMS is capable of storing up to billions of records. Moreover, it focuses on optimizing relating single lookups as range request, which are needed permanently for computations in bioinformatics. To validate the power of DRUMS, we compare it to the widely used MySQL database. The test setting considers two biological data sets. We use standard desktop hardware as test environment.</p></sec><sec><title>Conclusions</title><p>DRUMS outperforms MySQL in writing and reading records by a factor of two up to a factor of 10000. Furthermore, it can work with significantly larger data sets. Our work focuses on mid-sized data sets up to several billion records without requiring cluster technology. Storing position-specific data is a general problem and the concept we present here is a generalized approach. Hence, it can be easily applied to other fields of bioinformatics.</p></sec></abstract><kwd-group><kwd>Database</kwd><kwd>HERV</kwd><kwd>SNP</kwd><kwd>DNA related data</kwd><kwd>High throughput data</kwd></kwd-group></article-meta></front><body><sec><title>Background</title><p>With the beginning of the information age in the 90s of the last century, a large set of processes are established to manipulate and analyze data. In particular in the field of bioinformatics, many different workflows produce a growing amount of data. One example are sequencing technologies, which are capable of sequencing an entire human genome in less than a day. Moreover, extensive software suites for analyzing biological data sets exist, e.g. <ext-link ext-link-type="uri" xlink:href="http://galaxy.psu.edu/">http://galaxy.psu.edu/</ext-link>[<xref ref-type="bibr" rid="B1">1</xref>-<xref ref-type="bibr" rid="B3">3</xref>]. In addition, it is possible that an analyzing process produces more output data than provided input. For example, the input size of the HERV data set used in this work is about 4 GB. The output of the mapping with BLAST is about 50 GB large. Hence, rapid processes for storing and querying data are needed as it has impact on the general performance of the analytic processes.</p><sec><title>Position-specific DNA related data (psDrd)</title><p>In the field of bioinformatics, data related to DNA sequences are of particular importance. Examples are single nucleotide polymorphisms (SNPs) [<xref ref-type="bibr" rid="B4">4</xref>], transcription factor binding affinities and probabilities [<xref ref-type="bibr" rid="B5">5</xref>,<xref ref-type="bibr" rid="B6">6</xref>], and RNAseq data [<xref ref-type="bibr" rid="B7">7</xref>,<xref ref-type="bibr" rid="B8">8</xref>]. We generalize these types of data by the term position-specific DNA-related data (<italic>psDrd</italic>). A <italic>psDrd</italic> record is an information related to a specific DNA position. <italic>psDrd</italic> records have three characteristics. First, a <italic>psDrd</italic> record <italic>R</italic> can be represented by a key-value pair <italic>R</italic>=(<italic>K</italic>,<italic>V</italic>). The key <italic>K</italic> is composed of the sequence identifier and the position of the associated value <italic>V</italic>. Hence, the key is unique, and records can be easily sorted. Second, <italic>psDrd</italic> records are usually requested by region (e.g., querying for all mutations in a specific gene or looking for transcription factors that are binding near a given position). We call this kind of access <italic>range select</italic>. Third, all <italic>psDrd</italic> of the same kind need the same space to be stored on device, i.e., two different records are represented by the same number of bytes. In contrast, textual annotations are generally of variable length. These three specific properties can be utilized for optimizing data handling of <italic>psDrd</italic>.</p></sec><sec><title>Time- and resource-intensive computations on psDrd</title><p>Many biological processes and bioinformatics algorithm have <italic>psDrd</italic> as input or output. This type of data is essential for understanding biological and biochemical processes. Furthermore, diagnostics in medicine for cancer prediction and genetic diseases are using <italic>psDrd</italic>[<xref ref-type="bibr" rid="B9">9</xref>-<xref ref-type="bibr" rid="B11">11</xref>].</p><p>Many activities in bioinformatics focus on analyzing <italic>psDrd</italic>. However, often a file and folder strategy or standard databases like MySQL [<xref ref-type="bibr" rid="B12">12</xref>] are used for data management. These approaches are straightforward but not optimized for the intended processing of <italic>psDrd</italic>. In addition, data types used in these tools are expensive and might lead to an exhaustive usage of valuable resources [<xref ref-type="bibr" rid="B13">13</xref>]. Both problems lead to resource-intensive requests of <italic>psDrd</italic>. For example, when performing range selects using MySQL, nearly each record in the range must be fetched by a costly random access to the storage. Because of the limits of standard desktop hardware, this might cause a bottleneck during data processing.</p></sec><sec><title>Requirements</title><p>The following requirements result from the above mentioned problems: The data management must be usable with standard desktop technology. It must be possible to store billions of data records. Platform independency was defined as an additional requirement (derived from the well-known segmentation of operation systems). Handling massive read requests during analytic processes has to be possible. While optimizing data handling of <italic>psDrd</italic>, the three specific properties from section &#x0201c;<italic>Position-specific DNA related data (psDrd)</italic>&#x0201d; have to be obeyed.</p></sec></sec><sec><title>Implementation</title><p>In this section, we first describe a concept called DRUM, on which DRUMS is based. Subsequently, we describe the architecture of DRUMS. Finally, we briefly sketch the implementation of DRUMS in Java considering the three main requirements of handling <italic>psDrd</italic> data sets efficiently.</p><sec><title>DRUM concept</title><p>The DRUM (Disk Repository with Update Management) concept [<xref ref-type="bibr" rid="B14">14</xref>] allows to store large collections of key-value pairs (KVs). DRUM allows fast bulk inserts without generating duplicate entries. To enable fast processing, incoming <italic>psDrd</italic> records (<italic>K</italic>,<italic>V</italic>) are allocated based on their key <italic>K</italic> to separate buffers <italic>B</italic> in the main memory: <inline-formula><mml:math id="M1" name="1471-2105-15-38-i1" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Those buffers are continuously written to their counterparts on disk (<italic>D</italic>), where they are called <italic>buckets</italic>. If a bucket on disk reaches a predefined size, a synchronisation process with the persistently saved data (on the hard disk) starts. The process is executed in the following way: A disk bucket is entirely read to a disk cache. There it is sorted. Thereafter, a synchronisation is performed by combining each bucket after the other with the corresponding cache. As the records of the disk cache are also sorted, using mergesort is efficient. The synchronisation process is blocking all other processes within DRUM.</p><p>The DRUM concept is very suitable for storing <italic>psDrd</italic>. However, requesting data efficiently was never a goal of this approach. Hence, neither single lookups nor range selects have been optimized. Furthermore, when synchronisation is performed, DRUM is not able to receive and cache new <italic>psDrd</italic> records. In the following, we propose an extension of DRUM that addresses these shortcomings.</p></sec><sec><title>Extensions by the DRUMS concept</title><p>We extend the DRUM concept by allowing the selection of records by key (<italic>single lookup</italic>) or by range (<italic>range selects</italic>). Within this concept we decoupled I/O-processes from memory processes to avoid blocking single components.</p><p>Following the three <italic>psDrd</italic> data properties, the following architecture decisions were made for DRUMS in addition to the DRUM concept: 1) All records are equally sized, so that jumping to the start position of an arbitrary record in the file is possible. Therefore, a sparse index [<xref ref-type="bibr" rid="B15">15</xref>] can be applied efficiently, making rapid single selects possible by the following two steps: The sparse index points to a block of records, where the <italic>psDrd</italic> of interest might be found. To finally find the requested record, a binary search is performed. The binary search massively benefits from equally sized records. 2) Records, which are close to each other on DNA are stored close on disk according to their keys. This enables efficient range selects. 3) Records are organized in buckets and chunks, which permits efficient prefiltering of regions of interest within a bucket.</p></sec><sec><title>Architecture of DRUMS</title><p>DRUMS is composed of the interacting components described in this section. Before each component is described in detail, we give a high-level overview of the insert and select process of DRUMS.</p><sec><title>Processes</title><sec><title>Insert process</title><p>The high-level overview of the insert process of DRUMS is shown in Figure <xref ref-type="fig" rid="F1">1</xref>. KV pairs are sent to DRUMS. As in DRUM, the incoming records are already distributed in memory between <italic>n</italic> buffers <italic>B</italic> (called memory buckets). Each bucket <italic>B</italic><sub><italic>i</italic>
</sub> in memory has a corresponding bucket <italic>D</italic><sub>
<italic>i</italic>
</sub> on disk. The sizes of the buckets are dynamic. If a bucket <italic>B</italic><sub>
<italic>i</italic>
</sub> exceeds a predefined size or memory limitations are reached, a synchronisation process, consisting of four phases, is started:</p><fig id="F1" position="float"><label>Figure 1</label><caption><p><bold>High level overview of insert process.</bold> Key-value pairs are sent to DRUMS. The incoming records are distributed between <italic>k</italic> buffers (memory buckets), based on their key. If a bucket <italic>B</italic><sub><italic>i</italic></sub> exceeds a predefined size or memory limitations are reached, a synchronisation process is instantiated.</p></caption><graphic xlink:href="1471-2105-15-38-1"/></fig><p>1) The bucket <italic>B</italic><sub>
<italic>i</italic>
</sub> is taken and replaced by an empty one. Hence, incoming data can still be buffered. 2) The KV pairs of <italic>B</italic><sub>
<italic>i</italic>
</sub> are sorted by their keys. 3) <italic>B</italic><sub>
<italic>i</italic>
</sub> and <italic>D</italic><sub>
<italic>i</italic>
</sub> are synchronised using mergesort. Already existing records can be updated using state-dependent operations. 4) The merged data is continuously written back to bucket <italic>D</italic><sub>
<italic>i</italic>
</sub>. Hence, input data is now saved persistently on the disk.</p><p>Note: Step 3 and 4 of the synchronization process are performed chunk-wise, so that optimal read and write performance can be achieved. The optimal chunk-size depends on the used hardware, the size of a single record, the expected data volume, and several parameters in DRUMS. Therefore, it has to be determined empirically.</p></sec><sec><title>Range select process</title><p>Figure <xref ref-type="fig" rid="F2">2</xref> shows the high-level overview of the select process. When a request is sent to DRUMS, four steps are performed to read the requested records given by the keys <italic>K</italic><sub>
<italic>S</italic>
</sub> and <italic>K</italic><sub>
<italic>E</italic>
</sub> (start and end of the range). 1) The requested bucket <italic>D</italic><sub>
<italic>i</italic>
</sub> is identified by <inline-formula><mml:math id="M2" name="1471-2105-15-38-i2" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. 2) The index of <italic>D</italic><sub>
<italic>i</italic>
</sub> is used for determining the correct chunk <italic>C</italic><sub>
<italic>k</italic>
</sub> of the first requested record <italic>R</italic><sub>
<italic>S</italic>
</sub>=(<italic>K</italic><sub>
<italic>S</italic>
</sub>,<italic>V</italic><sub>
<italic>S</italic>
</sub>). 3) Within <italic>C</italic><sub>
<italic>k</italic>
</sub> a binary search is performed for finding <italic>R</italic><sub>
<italic>S</italic>
</sub>. The binary search massively benefits from equally sized records. 4) A sequential read is performed until <italic>K</italic><sub>
<italic>E</italic>
</sub> was found and consequently <italic>R</italic><sub>
<italic>E</italic>
</sub> returned. It might be needed to perform the sequential read over chunk and bucket boundaries.</p><fig id="F2" position="float"><label>Figure 2</label><caption><p><bold>High level overview of select process.</bold> When a request is sent to DRUMS, four steps are done to read the requested records. 1) The bucket of interest is determined. 2) The correct chunk of the first requested record is identified, using a sparse index. 3) The position of the requested key-value pair is determined. 4) A sequential read is performed until the requested range is completely processed.</p></caption><graphic xlink:href="1471-2105-15-38-2"/></fig></sec><sec><title>Single select process</title><p>A request of a single row (single select) is considered as special case of the range select process where <italic>K</italic><sub>
<italic>S</italic>
</sub>=<italic>K</italic><sub>
<italic>E</italic>
</sub>. Therefore, it is covered by step 1 to 3.</p></sec></sec></sec><sec><title>Components of DRUMS</title><sec><title>BucketContainer and its buckets</title><p>The BucketContainer is a buffer that is organized in buckets <italic>B</italic> (memory buckets). It manages the distribution of incoming records to the buckets in RAM. As in DRUM, the distribution of the incoming records <italic>R</italic>=(<italic>K</italic>,<italic>V</italic>) to the Buckets <italic>B</italic> is based on a predefined mapping function <inline-formula><mml:math id="M3" name="1471-2105-15-38-i3" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi><mml:mo>(</mml:mo><mml:mi>K</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02192;</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>The BucketContainer is decoupled from any I/O-operation, so that preparing the data for writing can be done in parallel to the I/O-processes. The larger the size of the BucketContainer, the larger are the parts of the data that can be processed sequentially. This increases the performance significantly as sequential I/O-operations are the most efficient on HDDs and SSDs.</p></sec><sec><title>SyncManager, SyncProcess, and Synchronizer</title><p>The SyncManager manages all SyncProcesses. It observes the BucketContainer and verifies the preconditions for the synchronisation of buckets <italic>B</italic> with their counterparts on disk <italic>D</italic>. If these preconditions are fulfilled, the SyncManager instantiates new SyncProcesses. Several SyncProcesses can be run in parallel. In our implementation, a bucket in memory must reach a predefined fill level or age to be synchronized.</p><p>A new SyncProcess is always instantiated with the largest bucket in the BucketContainer fulfilling the above mentioned condition. When a new SyncProcess is started, the affected bucket in the BucketContainer is replaced by an empty one. In this way the synchronization process is not blocking further insert operations for this bucket.</p><p>The SyncProcess instantiates new Synchronizers. A Synchronizer is in charge of writing data from the bucket <italic>B</italic><sub>
<italic>i</italic>
</sub> in memory to the bucket <italic>D</italic><sub>
<italic>i</italic>
</sub> on disk. All records are sorted in <italic>B</italic><sub>
<italic>i</italic>
</sub> and in <italic>D</italic><sub>
<italic>i</italic>
</sub>. Hence, the Synchronizer is capable of using mergesort for synchronizing the records in memory with those on disk.</p></sec><sec><title>Representation and structure of the data</title><p>Each persistent bucket is represented by a file on a hard disk. The file is structured into two parts (see Figure <xref ref-type="fig" rid="F3">3</xref>): (i) the header with meta information and the index structure referencing chunks of a predefined size and (ii) the rest of the file used for the records to store, which are organized in chunks. A sparse index [<xref ref-type="bibr" rid="B15">15</xref>] is applied as it is memory efficient and takes advantage of the order of <italic>psDrds</italic>.</p><fig id="F3" position="float"><label>Figure 3</label><caption><p><bold>Structure of a file on storage device.</bold> The file is structured into (i.) a header, (ii.) an index structure and (iii.) the content, containing the records.</p></caption><graphic xlink:href="1471-2105-15-38-3"/></fig><p>Whenever a bucket <italic>D</italic> is opened for reading or writing, the header and the index are read into memory. In this way, a rapid access to the required chunks is possible.</p><p>The internal representation of a record in a chunk is a sequence of bytes. This sequence is composed of a key-part and a value-part. Each part may consist of several subparts, each of its own data-type (e.g., integer, long, char or even high level data structures like objects). Because of the fact that each record is of equal size, data structures and memory can be easily reused by application of the adaptor and the prototype pattern [<xref ref-type="bibr" rid="B16">16</xref>].</p></sec></sec><sec><title>Implementation of DRUMS</title><p>DRUMS is build upon Oracle Java 1.6. Therefore, it is platform independent. We developed DRUMS in an atomic thread-based way. All components work asynchronously and are exchangeable. This allows fast adaptations on single subprocesses or exchanging whole components like the Synchronizer.</p></sec></sec><sec><title>Results and discussion</title><p>In this section we first give a short introduction into two different <italic>psDrd</italic> sets used for evaluation. Second, we present the results and the evaulation approach considering (i) inserts, (ii) random lookups, and (iii) random range selects.</p><p>To prove the superiority of DRUMS in comparison with standard solutions within a desktop environment, we compare it to MySQL which is used widely in the bioinformatics community.</p><p>Two different <italic>psDrd</italic> sets are evaluated. The data sets are described below. DRUMS as well as MySQL were tested comparatively using the three measures: (i) - (iii). For all tests a standard desktop computer was used. MySQL as well as DRUMS are limited to use only 2 GB of the available memory. Details can be obtained from Table <xref ref-type="table" rid="T1">1</xref>.</p><table-wrap position="float" id="T1"><label>Table 1</label><caption><p>Test system</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/></colgroup><thead valign="top"><tr><th align="left" valign="bottom"><bold>Processor</bold><hr/></th><th align="left" valign="bottom"><bold>Intel Xeon E31225</bold><hr/></th></tr><tr><th align="left">&#x000a0;</th><th align="left"><bold>(4 native cores, no hyperthreading)</bold></th></tr></thead><tbody valign="top"><tr><td align="left" valign="bottom">Memory<hr/></td><td align="left" valign="bottom">8 GB<hr/></td></tr><tr><td align="left" valign="bottom">Operation system<hr/></td><td align="left" valign="bottom">Debian 6.0 (Squeeze)<hr/></td></tr><tr><td align="left">Hard drive</td><td align="left">Western digital WD10EALX-759, 32 MB cache</td></tr></tbody></table><table-wrap-foot><p>The desktop system which was used for the tests. MySQL as well as DRUMS are limited to use only 2 GB of the available memory.</p></table-wrap-foot></table-wrap><sec><title>Data sets</title><sec><title>SNP-Data from the 1001 genomes project</title><p>The 1001 Genomes Project [<xref ref-type="bibr" rid="B17">17</xref>,<xref ref-type="bibr" rid="B18">18</xref>] has the goal to understand the resulting of small mutations in different accessions of the reference plant Arabidopsis thaliana. Each accession mainly consists of five attributes: accession identifier, sequence identifier, position on sequence, source base, and target base. We downloaded filtered quality data of the strains sequenced by the Gregor Mendel Institute and the Salk institute on 2012-01-15, containing 251 data sets, with 137,369,902 SNPs. From all files, we extracted the data of the following five columns: accession name, chromosome, position on chromosome reference nucleotide, and mutated nucleotide. For the definitions of the used data types and their configuration (e.g., index properties) used in MySQL and DRUMS see Table <xref ref-type="table" rid="T2">2</xref>.</p><table-wrap position="float" id="T2"><label>Table 2</label><caption><p>Data types used for SNP data</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/></colgroup><thead valign="top"><tr><th align="left"><bold>Column</bold></th><th align="left"><bold>MySQL properties</bold></th><th align="left"><bold>DRUMS properties</bold></th></tr></thead><tbody valign="top"><tr><td align="left" valign="bottom">Accession name<hr/></td><td align="left" valign="bottom">TINY INT, primary key<hr/></td><td align="left" valign="bottom">1 byte, key part 1<hr/></td></tr><tr><td align="left" valign="bottom">Chromosome<hr/></td><td align="left" valign="bottom">SMALL INT, primary key<hr/></td><td align="left" valign="bottom">2 byte, key part 2<hr/></td></tr><tr><td align="left" valign="bottom">Position on chromosome<hr/></td><td align="left" valign="bottom">INT, primary key<hr/></td><td align="left" valign="bottom">4 byte, key part 3<hr/></td></tr><tr><td align="left" valign="bottom">Reference nucleotide<hr/></td><td align="left" valign="bottom">VARCHAR<hr/></td><td align="left" valign="bottom">1 byte, value part 1<hr/></td></tr><tr><td align="left">Mutated nucleotide</td><td align="left">VARCHAR</td><td align="left">1 byte, value part 2</td></tr></tbody></table><table-wrap-foot><p>Used data types in MySQL and DRUMS for SNP data. All columns being part of the primary key are indexed.</p></table-wrap-foot></table-wrap><p>All data are public available at <ext-link ext-link-type="uri" xlink:href="http://1001genomes.org/datacenter/">http://1001genomes.org/datacenter/</ext-link>.</p></sec><sec><title>HERV data</title><p>Human endogenous retroviruses (HERVs) have integrated themselves in the human genome millions of years ago. Because of the high number of existing HERV fragments, they are thought to have a regulatory role. To investigate a possible influence of HERVs, it is needed to locate HERV fragments. Therefore, over 7000 known HERV fragments were blasted against the human genome to find new putative HERV-like regions. In the work of Konstantin Kruse [<xref ref-type="bibr" rid="B19">19</xref>] all regions with an E-value less than 1<italic>e</italic>-20 were accepted as putative HERV-like region. This lead to 802,710,938 single records, stored in 20 files with tab-separated data field, with a total size of 50 GB. From these files we used the following seven columns: query id, subject id, query start, query end, subject start, subject end, and E-value. For the definitions of the used data types and their configuration (e.g., index properties) used in MySQL and DRUMS see Table <xref ref-type="table" rid="T3">3</xref>.</p><table-wrap position="float" id="T3"><label>Table 3</label><caption><p>Data types used for HERV data</p></caption><table frame="hsides" rules="groups" border="1"><colgroup><col align="left"/><col align="center"/><col align="center"/></colgroup><thead valign="top"><tr><th align="left"><bold>Column</bold></th><th align="left"><bold>MySQL properties</bold></th><th align="left"><bold>DRUMS properties</bold></th></tr></thead><tbody valign="top"><tr><td align="left" valign="bottom">Chromosome<hr/></td><td align="left" valign="bottom">TINY INT, primary key<hr/></td><td align="left" valign="bottom">1 byte, key part 1<hr/></td></tr><tr><td align="left" valign="bottom">Start-position on chromosome<hr/></td><td align="left" valign="bottom">INT, primary key<hr/></td><td align="left" valign="bottom">4 byte, key part 2<hr/></td></tr><tr><td align="left" valign="bottom">End-position on chromosome<hr/></td><td align="left" valign="bottom">INT, primary key<hr/></td><td align="left" valign="bottom">4 byte, key part 3<hr/></td></tr><tr><td align="left" valign="bottom">Start-position on HERV<hr/></td><td align="left" valign="bottom">SMALL INT, primary key<hr/></td><td align="left" valign="bottom">2 byte, key part 4<hr/></td></tr><tr><td align="left" valign="bottom">End-position on HERV<hr/></td><td align="left" valign="bottom">SMALL INT, primary key<hr/></td><td align="left" valign="bottom">2 byte, key part 5<hr/></td></tr><tr><td align="left" valign="bottom">Id of referenced HERV<hr/></td><td align="left" valign="bottom">SMALL INT, primary key<hr/></td><td align="left" valign="bottom">2 byte, key part 6<hr/></td></tr><tr><td align="left" valign="bottom">Strand on chromosome<hr/></td><td align="left" valign="bottom">TINY INT, primary key<hr/></td><td align="left" valign="bottom">1 byte, key part 7<hr/></td></tr><tr><td align="left">E-value</td><td align="left">DOUBLE</td><td align="left">4 byte, value part 1</td></tr></tbody></table><table-wrap-foot><p>Used data types in MySQL and DRUMS for HERV data. All columns being part of the primary key are indexed.</p></table-wrap-foot></table-wrap></sec></sec><sec><title>Insert performance</title><p>DRUMS must be able to store hundreds of millions of records. Because of this, it is needed to evaluate the insert performance.</p><p>To estimate the insert performance, we measure the time for inserting 10<sup>6</sup> records. We obtain 140 time measurements points in case of SNP-Data and 800 for HERV data. Figures <xref ref-type="fig" rid="F4">4</xref>a and <xref ref-type="fig" rid="F4">4</xref>b show the insert performance of DRUMS (blue) and MySQL (green). Despite using bulk-requests for inserting the data, it was impossible to insert all 800 million HERV records into the MySQL instance. MySQL inserts about 200 million records in the first week, but Figure <xref ref-type="fig" rid="F4">4</xref>b shows that the insert performance has dropped to 300 records per second after one week. The insert performance of DRUMS also decreases, but it was able to insert the whole data set within 4.53 hours. At the end of the test, DRUMS was still able to perform more than 20000 inserts per second.</p><fig id="F4" position="float"><label>Figure 4</label><caption><p><bold>Insert performance.</bold> The blue line represents DRUMS, the green line represents MySQL. <bold>(a)</bold> Insert performance on SNP-data <bold>(b)</bold> Insert performance on HERV data. Concerning MySQL, it was impossible to insert all 800 million HERV records. DRUMS inserted the complete data set within 4.53 hours.</p></caption><graphic xlink:href="1471-2105-15-38-4"/></fig><p>Figure <xref ref-type="fig" rid="F4">4</xref>a and <xref ref-type="fig" rid="F4">4</xref>b show that DRUMS has a better insert performance than MySQL on both test datasets. The insert performance of MySQL and of DRUMS decreases with the number of records already inserted. Regarding MySQL one possible explanation is the continuous reorganistation and rewriting of the index.</p><p>The insert performance of DRUMS decreases slowly in comparison to MySQL. The reason for this is the decreasing ratio of read- to write-accesses with each round of synchronisation. With other words, DRUMS must read more and more records per new record to write with the growing amount of data already stored on disk. However, DRUMS still inserts more than 20000 records per second at the end of the insert test for HERV data, corresponding to approximately 400 kB per second.</p></sec><sec><title>Performance on random lookups</title><p>From the view of bioinformatics, single lookups make no sense in both experiments. However, the performance of single-lookups is a significant indicator for the overall performance and the suitability of the implementation of a tool for handling data sets. Moreover, the test may show how close the measured performance to the theoretical hardware limits of the used standard desktop hardware is. Considering the test environment, it is assumed that a random access would take approximately 20 ms. Hence, if no other disk accesses are done, it would be theoretically possible to read 50 records per second.</p><p>Figures <xref ref-type="fig" rid="F5">5</xref>a and <xref ref-type="fig" rid="F5">5</xref>b show the performance of MySQL and DRUMS, when performing random lookups. Again, DRUMS performs better than MySQL in case of handling our two data sets. Figure <xref ref-type="fig" rid="F5">5</xref>a implies that DRUMS is able to do 160 times more random lookups than theoretically possible, when accessing SNP data. In comparison, only 20 random lookups per second are performed when accessing HERV data. The reason for this difference are cache structures provided by the operating system and the underlying hardware.</p><fig id="F5" position="float"><label>Figure 5</label><caption><p><bold>Random lookups performance.</bold> The blue line represents DRUMS, the green line represents MySQL. <bold>(a)</bold> Random lookup performance on SNP-data. <bold>(b)</bold> Random lookup performance on HERV data.</p></caption><graphic xlink:href="1471-2105-15-38-5"/></fig><p>In case of accessing SNP data, the complete data set might be cached by the operating system after approximately 650,000 lookups. Hence, organizing the SNP data as DRUMS structure results in a file size small enough that it could be loaded into memory. Therefore, nearly each request could be answered from the operating systems cache after a warm up. In contrast, the HERV data set is too large to fit into memory, so only a few random lookups could be answered from cache. The increasing performance of MySQL and DRUMS in Figure <xref ref-type="fig" rid="F5">5</xref>b is also an indication for the use of caches. Figure <xref ref-type="fig" rid="F5">5</xref>b shows that DRUMS can perform 20 random lookups of theoretically possible 50.</p><p>While considering the experimental results of MySQL, the impression is conveyed that the defined index was not used correctly. However, a closer look validates the results as the explicit MySQL index for the SNP table has the size of 2380 MB, which will not fit into the allowed 2 GB of main memory. Hence, even index-based searches in MySQL need several accesses to the hard disk resulting in worse performance. In contrast, the sparse index of each bucket of DRUMS requires just 0.5 MB, which sums up to only 123 MB for all buckets. To find a single record in a chunk, DRUMS performs a binary search. The binary search can be done very efficiently for the reason that all records are of equal size. Because of the reduced demands on the hardware, DRUMS provides a good performance even on very large data sets like HERV.</p></sec><sec><title>Performance on random range selects</title><p>As described in the section Background, <italic>psDrd</italic>-records are mostly requested by range. Therefore, the need to benchmark the performance of range requests is obvious.</p><p>The request for the SNP-data is as follows: Select all SNPs on chromosome c between position x and y for all ecotypes in the database. To perform the read test for SNP-data, we first randomly generated 10<sup>6</sup> ranges of length 10<sup>3</sup> to 10<sup>4</sup>. Second, we request records within those ranges randomly distributed over the whole genome of Arabidopsis thaliana.</p><p>Analogously, we generate 10<sup>6</sup> test requests for the HERV data set with lengths from 10<sup>5</sup> to 10<sup>6</sup>. Again, we randomly distributed range-requests over the whole human genome. It might be a common task to filter the requested data by value. MySQL provides this functionality by defining the filter condition in the WHERE-clause. To accomplish this in DRUMS, the returned records must be checked iteratively. In this test, we filter the requested HERV records by an E-value less than 10<sup>-20</sup>, 10<sup>-25</sup>, 10<sup>-30</sup>, 10<sup>-35</sup>, 10<sup>-40</sup>, 10<sup>-45</sup> or 10<sup>-50</sup>, randomly chosen.</p><p>Figures <xref ref-type="fig" rid="F6">6</xref>a and <xref ref-type="fig" rid="F6">6</xref>b show the results of the range select test. Once more, both databases perform much better on the smaller SNP-data set. Besides caching, this time another explanation for this observation is that a range request on the SNP-data contains in average 3 times fewer records than a range request on the HERV data. The performance increases with the number of read records. The performance of DRUMS increases by a factor of 10 and of MySQL by a factor of 26. However, DRUMS performs in average on the SNP-data 24 times faster than MySQL.</p><fig id="F6" position="float"><label>Figure 6</label><caption><p><bold>Range select performance.</bold> The blue line represents DRUMS, the green line represents MySQL. <bold>(a)</bold> Range select performance on SNP-data. <bold>(b)</bold> Range select performance on HERV data. Concerning MySQL, we stopped the test after 26.35 hours. DRUMS read 64 billion records in 9.61 hours.</p></caption><graphic xlink:href="1471-2105-15-38-6"/></fig><p>Regarding the larger HERV data set, DRUMS is able to perform 30 range-selects per second in average. This is over 15000 times faster than MySQL.</p><p>Within the whole test, 64 billion records were read in 9.61 hours. That corresponds to an overall read performance of 35.7 MB per second, filtering included. In contrast, MySQL read 6.6 million records in 26.35 hours, which corresponds to only 1.3 kB per second.</p></sec></sec><sec sec-type="conclusions"><title>Conclusions</title><p>We defined <italic>psDrd (position-specific DNA related data)</italic> and showed three important properties of this kind of data. The flaws of DRUM were shown, which is already suitable for storing <italic>psDrd</italic>, but not for requesting it efficiently. The article introduces DRUMS, a data management concept optimized to tackle the challenges of dealing with mid-size data sets in form of <italic>psDrd</italic> using standard desktop technology instead of expensive cluster hardware.</p><p>An implementation of the DRUMS concept was compared to the widely spread standard database management solution MySQL considering two data sets of the bioinformatics context. On the larger HERV data set, the evaluated DRUMS implementation was 23 times faster inserting all records, two times faster performing random lookups, and 15456 faster performing range requests. Hence, the experiments show that dealing with <italic>psDrd</italic> benefits significantly from the characteristics of the DRUMS concept. Therefore, our main contribution is suggesting this data management concept for increasing the performance during data intensive processes while keeping the hardware investments low.</p></sec><sec><title>Availability and requirements</title><p><bold>Project name:</bold> DRUMS </p><p><bold>Project home page:</bold><ext-link ext-link-type="uri" xlink:href="http://mgledi.github.io/DRUMS">http://mgledi.github.io/DRUMS</ext-link></p><p><bold>Project home page of examples:</bold><ext-link ext-link-type="uri" xlink:href="http://github.com/mgledi/BioDRUMS">http://github.com/mgledi/BioDRUMS</ext-link></p><p><bold>Operating system:</bold> Platform independent </p><p><bold>Programming language:</bold> Java </p><p><bold>Other requirements:</bold> none </p><p><bold>License:</bold> GNU GPL v2 </p><p><bold>Any restrictions to use by non-academics:</bold> No specific restrictions.</p></sec><sec><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Authors&#x02019; contributions</title><p>MN and NT developed and tested the Java code. All of the authors contributed to the design of the software architecture. All of the authors read and approved the final version of the manuscript.</p></sec></body><back><sec><title>Acknowledgements</title><p>We are grateful to Dr. Christiane Lemke and Anika Gross for revising the manuscript. We thank Michael Roeder for testing the installation and usage instructions. Furthermore, we thank <italic>Unister GmbH</italic> for the opportunity to develop and publish the software as open source project.</p></sec><ref-list><ref id="B1"><mixed-citation publication-type="journal"><name><surname>Goecks</surname><given-names>J</given-names></name><name><surname>Nekrutenko</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><article-title>Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences</article-title><source>Genome Biol</source><year>2010</year><volume>11</volume><issue>8</issue><fpage>R86+</fpage><pub-id pub-id-type="pmid">20738864</pub-id></mixed-citation></ref><ref id="B2"><mixed-citation publication-type="other"><name><surname>Blankenberg</surname><given-names>D</given-names></name><name><surname>Von Kuster</surname><given-names>G</given-names></name><name><surname>Coraor</surname><given-names>N</given-names></name><name><surname>Ananda</surname><given-names>G</given-names></name><name><surname>Lazarus</surname><given-names>R</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><name><surname>Nekrutenko</surname><given-names>A</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><article-title>Galaxy: a web-based genome analysis tool for experimentalists</article-title><source>Current protocols in molecular biology/edited by Frederick M. Ausubel... [et al.]</source><year>2010</year><fpage>Chapter 19</fpage></mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><name><surname>Giardine</surname><given-names>B</given-names></name><name><surname>Riemer</surname><given-names>C</given-names></name><name><surname>Hardison</surname><given-names>RC</given-names></name><name><surname>Burhans</surname><given-names>R</given-names></name><name><surname>Elnitski</surname><given-names>L</given-names></name><name><surname>Shah</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Blankenberg</surname><given-names>D</given-names></name><name><surname>Albert</surname><given-names>I</given-names></name><name><surname>Taylor</surname><given-names>J</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Kent</surname><given-names>WJ</given-names></name><name><surname>Nekrutenko</surname><given-names>A</given-names></name><article-title>Galaxy: a platform for interactive large-scale genome analysis</article-title><source>Genome Res</source><year>2005</year><volume>15</volume><issue>10</issue><fpage>1451</fpage><lpage>1455</lpage><pub-id pub-id-type="pmid">16169926</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="other"><article-title>Single nucleotide polymorphism</article-title><year>2012</year><comment>[<ext-link ext-link-type="uri" xlink:href="http://en.wikipedia.org/wiki/Single_Nucleotide_Polymorphism">http://en.wikipedia.org/wiki/Single_Nucleotide_Polymorphism</ext-link>]</comment></mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><name><surname>Bulyk</surname><given-names>M</given-names></name><article-title>Computational prediction of transcription-factor binding site locations</article-title><source>Genome Biol</source><year>2003</year><volume>5</volume><fpage>201+</fpage><pub-id pub-id-type="pmid">14709165</pub-id></mixed-citation></ref><ref id="B6"><mixed-citation publication-type="journal"><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Androulakis</surname><given-names>I</given-names></name><article-title>Recent advances in the computational discovery of transcription factor binding sites</article-title><source>Algorithms</source><year>2009</year><volume>2</volume><fpage>582</fpage><lpage>605</lpage></mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><name><surname>Malone</surname><given-names>J</given-names></name><name><surname>Oliver</surname><given-names>B</given-names></name><article-title>Microarrays, deep sequencing and the true measure of the transcriptome</article-title><source>BMC Biol</source><year>2011</year><volume>9</volume><fpage>34+</fpage><pub-id pub-id-type="pmid">21627854</pub-id></mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Gerstein</surname><given-names>M</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><article-title>RNA-Seq: a revolutionary tool for transcriptomics</article-title><source>Nat Rev Genet</source><year>2009</year><volume>10</volume><fpage>57</fpage><lpage>63</lpage><pub-id pub-id-type="pmid">19015660</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><name><surname>de Leeuw</surname><given-names>N</given-names></name><name><surname>Hehir-Kwa</surname><given-names>JY</given-names></name><name><surname>Simons</surname><given-names>A</given-names></name><name><surname>Geurts van Kessel</surname><given-names>A</given-names></name><name><surname>Smeets</surname><given-names>DF</given-names></name><name><surname>Faas</surname><given-names>BH</given-names></name><name><surname>Pfundt</surname><given-names>R</given-names></name><article-title>SNP array analysis in constitutional and cancer genome diagnostics&#x02013;copy number variants, genotyping and quality control</article-title><source>Cytogenet Genome Res</source><year>2011</year><volume>135</volume><fpage>212</fpage><lpage>221</lpage><pub-id pub-id-type="pmid">21934286</pub-id></mixed-citation></ref><ref id="B10"><mixed-citation publication-type="journal"><name><surname>Kihara</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>YDD</given-names></name><name><surname>Hawkins</surname><given-names>T</given-names></name><article-title>Bioinformatics resources for cancer research with an emphasis on gene function and structure prediction tools</article-title><source>Cancer Inform</source><year>2006</year><volume>2</volume><fpage>25</fpage><lpage>35</lpage><pub-id pub-id-type="pmid">19458756</pub-id></mixed-citation></ref><ref id="B11"><mixed-citation publication-type="book"><name><surname>Roukos</surname><given-names>DH</given-names></name><source>Next-Generation Sequencing &#x00026; Molecular Diagnostics</source><year>2013</year><publisher-name>London: Future Medicine Ltd</publisher-name></mixed-citation></ref><ref id="B12"><mixed-citation publication-type="other"><article-title>MySQL classic edition</article-title><year>2012</year><comment>[<ext-link ext-link-type="uri" xlink:href="http://www.mysql.com/products/classic/">http://www.mysql.com/products/classic/</ext-link>]</comment></mixed-citation></ref><ref id="B13"><mixed-citation publication-type="other"><article-title>Common wrong data types</article-title><year>2012</year><comment>[<ext-link ext-link-type="uri" xlink:href="http://code.openark.org/blog/mysql/common-data-types-errors-compilation">http://code.openark.org/blog/mysql/common-data-types-errors-compilation</ext-link>]</comment></mixed-citation></ref><ref id="B14"><mixed-citation publication-type="book"><name><surname>Lee</surname><given-names>HT</given-names></name><name><surname>Leonard</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Loguinov</surname><given-names>D</given-names></name><article-title>IRLbot: scaling to 6 billion pages and beyond</article-title><source>Proceedings of the 17th international conference on World Wide Web, WWW &#x02019;08. New York, NY,</source><year>2008</year><publisher-name>USA: ACM</publisher-name><fpage>427</fpage><lpage>436</lpage></mixed-citation></ref><ref id="B15"><mixed-citation publication-type="other"><article-title>Database index - sparse index</article-title><year>2012</year><comment>[<ext-link ext-link-type="uri" xlink:href="http://en.wikipedia.org/wiki/Database_index#Sparse_index">http://en.wikipedia.org/wiki/Database_index#Sparse_index</ext-link>]</comment></mixed-citation></ref><ref id="B16"><mixed-citation publication-type="book"><name><surname>Gamma</surname><given-names>E</given-names></name><name><surname>Helm</surname><given-names>R</given-names></name><name><surname>Johnson</surname><given-names>R</given-names></name><name><surname>Vlissides</surname><given-names>J</given-names></name><source>Design patterns: elements of reusable object-oriented software</source><year>1995</year><publisher-name>Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc.</publisher-name></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><name><surname>Schneeberger</surname><given-names>K</given-names></name><name><surname>Ossowski</surname><given-names>S</given-names></name><name><surname>Ott</surname><given-names>F</given-names></name><name><surname>Klein</surname><given-names>JD</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Lanz</surname><given-names>C</given-names></name><name><surname>Smith</surname><given-names>LM</given-names></name><name><surname>Cao</surname><given-names>J</given-names></name><name><surname>Fitz</surname><given-names>J</given-names></name><name><surname>Warthmann</surname><given-names>N</given-names></name><name><surname>Henz</surname><given-names>SR</given-names></name><name><surname>Huson</surname><given-names>DH</given-names></name><name><surname>Weigel</surname><given-names>D</given-names></name><article-title>Reference-guided assembly of four diverse Arabidopsis thaliana genomes</article-title><source>Proc Nat Acad Sci USA</source><year>2011</year><volume>108</volume><issue>25</issue><fpage>10249</fpage><lpage>10254</lpage><pub-id pub-id-type="pmid">21646520</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="journal"><name><surname>Cao</surname><given-names>J</given-names></name><name><surname>Schneeberger</surname><given-names>K</given-names></name><name><surname>Ossowski</surname><given-names>S</given-names></name><name><surname>G&#x000fc;nther</surname><given-names>T</given-names></name><name><surname>Bender</surname><given-names>S</given-names></name><name><surname>Fitz</surname><given-names>J</given-names></name><name><surname>Koenig</surname><given-names>D</given-names></name><name><surname>Lanz</surname><given-names>C</given-names></name><name><surname>Stegle</surname><given-names>O</given-names></name><name><surname>Lippert</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Ott</surname><given-names>F</given-names></name><name><surname>M&#x000fc;ller</surname><given-names>J</given-names></name><name><surname>Alonso-Blanco</surname><given-names>C</given-names></name><name><surname>Borgwardt</surname><given-names>K</given-names></name><name><surname>Schmid</surname><given-names>KJ</given-names></name><name><surname>Weigel</surname><given-names>D</given-names></name><article-title>Whole-genome sequencing of multiple Arabidopsis thaliana populations</article-title><source>Nat Genet</source><year>2011</year><volume>43</volume><issue>10</issue><fpage>956</fpage><lpage>963</lpage><pub-id pub-id-type="pmid">21874002</pub-id></mixed-citation></ref><ref id="B19"><mixed-citation publication-type="other"><name><surname>Kruse</surname><given-names>K</given-names></name><article-title>Analysis of gene expression in correlation to endogenous retroviruses</article-title><comment>Martin Luther University, Halle (Saale) Germany 2011. [Bachelor Thesis]</comment></mixed-citation></ref></ref-list></back></article>