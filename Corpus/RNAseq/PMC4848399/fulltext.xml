<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d1 20130915//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bioinformatics</journal-id><journal-id journal-id-type="hwp">bioinfo</journal-id><journal-title-group><journal-title>Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1367-4803</issn><issn pub-type="epub">1367-4811</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4848399</article-id><article-id pub-id-type="doi">10.1093/bioinformatics/btv764</article-id><article-id pub-id-type="publisher-id">btv764</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Papers</subject><subj-group subj-group-type="heading"><subject>Structural Bioinformatics</subject></subj-group></subj-group></article-categories><title-group><article-title>Bayesian variable selection for binary outcomes in high-dimensional genomic studies using non-local priors</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Nikooienejad</surname><given-names>Amir</given-names></name><xref ref-type="aff" rid="btv764-aff1"><sup>1</sup></xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Wenyi</given-names></name><xref ref-type="aff" rid="btv764-aff2"><sup>2</sup></xref><xref ref-type="corresp" rid="btv764-cor1">*</xref></contrib><contrib contrib-type="author"><name><surname>Johnson</surname><given-names>Valen E.</given-names></name><xref ref-type="aff" rid="btv764-aff1"><sup>1</sup></xref><xref ref-type="corresp" rid="btv764-cor1">*</xref></contrib><aff id="btv764-aff1"><sup>1</sup>Department of Statistics, Texas A&#x00026;M University, College Station, TX 77843, USA and </aff><aff id="btv764-aff2"><sup>2</sup>Department of Bioinformatics and Computational Biology, M. D. Anderson Cancer Center, Houston, TX 77030, USA</aff></contrib-group><author-notes><corresp id="btv764-cor1">*To whom correspondence should be addressed.</corresp><fn id="btv764-FN1"><p>Associate Editor: John Hancock</p></fn></author-notes><pub-date pub-type="ppub"><day>01</day><month>5</month><year>2016</year></pub-date><pub-date pub-type="epub"><day>06</day><month>1</month><year>2016</year></pub-date><pub-date pub-type="pmc-release"><day>06</day><month>1</month><year>2016</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the
							<pub-date pub-type="epub"/>. --><volume>32</volume><issue>9</issue><fpage>1338</fpage><lpage>1345</lpage><history><date date-type="received"><day>12</day><month>8</month><year>2015</year></date><date date-type="rev-recd"><day>27</day><month>12</month><year>2015</year></date><date date-type="accepted"><day>28</day><month>12</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; The Author 2016. Published by Oxford University Press.</copyright-statement><copyright-year>2016</copyright-year><license xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" license-type="creative-commons"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p></license></permissions><abstract><p><bold>Motivation</bold>: The advent of new genomic technologies has resulted in the production of massive data sets. Analyses of these data require new statistical and computational methods. In this article, we propose one such method that is useful in selecting explanatory variables for prediction of a binary response. Although this problem has recently been addressed using penalized likelihood methods, we adopt a Bayesian approach that utilizes a mixture of non-local prior densities and point masses on the binary regression coefficient vectors.</p><p><bold>Results</bold>: The resulting method, which we call iMOMLogit, provides improved performance in identifying true models and reducing estimation and prediction error in a number of simulation studies. More importantly, its application to several genomic datasets produces predictions that have high accuracy using far fewer explanatory variables than competing methods. We also describe a novel approach for setting prior hyperparameters by examining the total variation distance between the prior distributions on the regression parameters and the distribution of the maximum likelihood estimator under the null distribution. Finally, we describe a computational algorithm that can be used to implement iMOMLogit in ultrahigh-dimensional settings (<inline-formula id="IE1"><mml:math id="IEQ1"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mo>&#x0003e;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>) and provide diagnostics to assess the probability that this algorithm has identified the highest posterior probability model.</p><p><bold>Availability and implementation:</bold> Software to implement this method can be downloaded at: <ext-link ext-link-type="uri" xlink:href="http://www.stat.tamu.edu/~amir/code.html">http://www.stat.tamu.edu/&#x0223c;amir/code.html</ext-link>.</p><p><bold>Contact:</bold>
<email>wwang7@mdanderson.org</email> or <email>vjohnson@stat.tamu.edu</email></p><p><bold>Supplementary information:</bold>
<ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/lookup/suppl/doi:10.1093/bioinformatics/btv764/-/DC1">Supplementary data</ext-link> are available at <italic>Bioinformatics</italic> online.</p></abstract><counts><page-count count="8"/></counts></article-meta></front><body><sec><title>1 Introduction</title><p>Recent developments in bioinformatics and cancer genomics have made it possible to measure thousands of genomic variables that might be associated with the manifestation of cancer. The availability of such data has resulted in a pressing need for the development of statistical methods to use these data to identify variables that are associated with binary outcomes (e.g. cancer or control, survival or death). The topic of this article is a statistical model for identifying, from a large number <italic>p</italic> of potential feature vectors, a sparse subset that are useful in predicting a binary outcome vector. Throughout this article, we assume that the binary vector of interest is denoted by <bold>y</bold>, and that the matrix of potential explanatory variables is denoted by <bold>X</bold>. Letting <inline-formula id="IE2"><mml:math id="IEQ2"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula> denote the submatrix of <bold>X</bold> containing the &#x02018;true&#x02019; predictors, we assume that
<disp-formula id="E1"><label>(1)</label><mml:math id="EQ1"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003c0;</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>F</italic> denotes a known binary link function (assumed to be the logistic distribution in what follows), and <inline-formula id="IE3"><mml:math id="IEQ3"><mml:mi>&#x003c0;</mml:mi></mml:math></inline-formula> is the <italic>n</italic> vector of success probabilities for <bold>y</bold>. The regression coefficient <inline-formula id="IE4"><mml:math id="IEQ4"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> represents the non-zero regression effect for each column of <inline-formula id="IE5"><mml:math id="IEQ5"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula> in predicting <inline-formula id="IE6"><mml:math id="IEQ6"><mml:mi>&#x003c0;</mml:mi></mml:math></inline-formula>. The primary statistical challenge addressed in this article is the selection of the submatrix <inline-formula id="IE7"><mml:math id="IEQ7"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula> to be used for the prediction of <inline-formula id="IE8"><mml:math id="IEQ8"><mml:mi>&#x003c0;</mml:mi></mml:math></inline-formula>.</p><p>A number of related methods have been proposed to address this problem. These include the LASSO (<xref rid="btv764-B25" ref-type="bibr">Tibshirani, 1996</xref>), which is a penalized likelihood method that maximizes a product of the binary likelihood function implied by (1) and a constraint on the sum of the absolute value of components of the regression coefficient <inline-formula id="IE9"><mml:math id="IEQ9"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. A closely related method called Smoothly Clipped Absolute Deviation (SCAD) (<xref rid="btv764-B7" ref-type="bibr">Fan and Li, 2001</xref>) uses a non-convex penalty function and has been demonstrated to have certain oracle properties in idealized asymptotic settings. Other penalized likelihood functions include the adaptive LASSO (<xref rid="btv764-B30" ref-type="bibr">Zou, 2006</xref>) and the Dantzig selector (<xref rid="btv764-B5" ref-type="bibr">Candes and Tao, 2007</xref>); these methods share asymptotic properties similar to SCAD.</p><p>In ultrahigh-dimensions (<inline-formula id="IE10"><mml:math id="IEQ10"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mo>&#x0003e;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>), an effective computational technique for implementing the techniques described above is the Iterative Sure Independence Screening (ISIS) procedure (<xref rid="btv764-B8" ref-type="bibr">Fan and Lv, 2008</xref>), which iteratively performs a correlation screening step to reduce the number of explanatory variables so that penalized likelihood methods can be applied. ISIS has been used in conjunction with several penalized likelihood methods&#x02014;including adaptive LASSO (<xref rid="btv764-B30" ref-type="bibr">Zou, 2006</xref>), the Dantzig Selector (<xref rid="btv764-B5" ref-type="bibr">Candes and Tao, 2007</xref>), and SCAD (<xref rid="btv764-B7" ref-type="bibr">Fan and Li, 2001</xref>)&#x02014;to perform model selection.</p><p>A number of Bayesian methods have also been proposed for variable selection. Notable among these are the approaches proposed by <xref rid="btv764-B10" ref-type="bibr">George and McCulloch (1997)</xref>, which used a mixture-of-normals approximation to spike-and-slab priors on the regression coefficients. <xref rid="btv764-B17" ref-type="bibr">Lee <italic>et al.</italic> (2003)</xref> proposed a hierarchical probit model along with MCMC based stochastic search to perform gene selection in high-dimensional settings using a latent response variable and Gaussian priors on model coefficients. <xref rid="btv764-B26" ref-type="bibr">West <italic>et al.</italic> (2000)</xref> provided a Bayesian approach to this problem employing singular value regression and classes of informative prior distributions to estimate coefficients in high-dimensional settings. <xref rid="btv764-B18" ref-type="bibr">Liang <italic>et al.</italic> (2008)</xref> studied mixtures of <italic>g</italic> priors for Bayesian variable selection as an alternative to default <italic>g</italic> priors to overcome several consistency issues associated with the default <italic>g</italic> prior densities. Along more similar lines, <xref rid="btv764-B21" ref-type="bibr">Rossell <italic>et al.</italic> (2013)</xref> studied the utilization of non-local priors in Bayesian classifiers where they also address the problem of identifying variables with high predictive power.</p><p>Except for <xref rid="btv764-B21" ref-type="bibr">Rossell <italic>et al.</italic> (2013)</xref>, each of the Bayesian methods described above impose local prior densities on regression coefficients in the true model. That is, the prior density on the regression coefficients has a positive prior density function at 0 (and in most cases has its mode at 0), which from a Bayesian perspective makes it more difficult to distinguish between models that include regression coefficients that are close to 0 and those that do not. <xref rid="btv764-B16" ref-type="bibr">Johnson and Rossell (2012)</xref> proposed two new classes of non-local prior densities to ameliorate this problem. In the model selection context, non-local prior densities are 0 when a regression coefficient in the model is 0. This makes it easier to distinguish between coefficients that do not have an impact on the prediction of <bold>y</bold> from those that do. <xref rid="btv764-B16" ref-type="bibr">Johnson and Rossell (2012)</xref> used a Markov Chain Monte Carlo (MCMC) algorithm to sample from the posterior distribution on the model space; the convergence properties of this algorithm were studied in <xref rid="btv764-B15" ref-type="bibr">Johnson (2013)</xref>.</p><p>The primary goal of this article is to extend the methodology proposed in <xref rid="btv764-B16" ref-type="bibr">Johnson and Rossell (2012)</xref> for application to binary outcomes and to compare the performance of this algorithm to leading penalized likelihood methods. In addition, we describe a default procedure for setting the hyperparameters (i.e. tuning parameters) in the non-local priors, and we examine a numerical strategy for identifying the highest posterior probability model (HPPM).</p></sec><sec><title>2 Methods</title><p>Let <inline-formula id="IE11"><mml:math id="IEQ11"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> denote a vector of independent binary observations, <inline-formula id="IE12"><mml:math id="IEQ12"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> an <italic>n</italic> &#x000d7; <italic>p</italic> matrix of real numbers, <inline-formula id="IE13"><mml:math id="IEQ13"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi></mml:math></inline-formula> a <inline-formula id="IE14"><mml:math id="IEQ14"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> regression vector, and <inline-formula id="IE15"><mml:math id="IEQ15"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> the <italic>i<sup>th</sup></italic> row of <inline-formula id="IE16"><mml:math id="IEQ16"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We denote a model by <inline-formula id="IE17"><mml:math id="IEQ17"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE18"><mml:math id="IEQ18"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02264;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0003c;</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:mo>&#x0003c;</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02264;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and it is assumed that <inline-formula id="IE19"><mml:math id="IEQ19"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02260;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>&#x02260;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and all other elements of <inline-formula id="IE20"><mml:math id="IEQ20"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi></mml:math></inline-formula> are 0. The design matrix corresponding to model <inline-formula id="IE21"><mml:math id="IEQ21"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> is denoted by <inline-formula id="IE22"><mml:math id="IEQ22"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula> and is defined to have cardinality <italic>k</italic>. We assume that the columns of <bold>X</bold> have been standardized. The <italic>i<sup>th</sup></italic> row of <inline-formula id="IE23"><mml:math id="IEQ23"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula> is denoted by <inline-formula id="IE24"><mml:math id="IEQ24"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Assuming the logistic link function for <italic>F</italic> in (1), the goal of the model selection procedure proposed in this article is to identify sparse regression models that have high predictive probability. We propose to do this by identifying the highest posterior probability model <inline-formula id="IE25"><mml:math id="IEQ25"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> for data <inline-formula id="IE26"><mml:math id="IEQ26"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>, distributed according to 
<disp-formula id="E2"><label>(2)</label><mml:math id="EQ2"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>&#x02009;</mml:mtext><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mtext>&#x02009;</mml:mtext><mml:mo>&#x0223c;</mml:mo><mml:mtext>&#x02009;Bernoulli&#x02009;</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>i</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>i</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
under prior constraints on the model space and the assumption of non-local prior density constraints on the regression parameter <inline-formula id="IE27"><mml:math id="IEQ27"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Our primary focus is on the case <inline-formula id="IE28"><mml:math id="IEQ28"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mo>&#x0003e;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>Bayesian model selection is based on the calculation of posterior model probabilities. From Bayes theorem, the posterior probability of model <inline-formula id="IE29"><mml:math id="IEQ29"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script" class="calligraphy">J</mml:mi></mml:mrow></mml:math></inline-formula> is specified as
<disp-formula id="E3"><label>(3)</label><mml:math id="EQ3"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi mathvariant="script" class="calligraphy">J</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where
<disp-formula id="E4"><label>(4)</label><mml:math id="EQ4"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>&#x0222b;</mml:mo><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p>The art in implementing a Bayesian model selection procedure thus focuses on specifying the prior densities <inline-formula id="IE30"><mml:math id="IEQ30"><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for <inline-formula id="IE31"><mml:math id="IEQ31"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> under each model, as well as the prior model probabilities <inline-formula id="IE32"><mml:math id="IEQ32"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for the models themselves. Except for the intercept, we assume non-local priors on the components of the regression vector in each model. These non-local priors are described in the next section. Discussion of the prior on the model space is described after that.</p><sec><title>2.1 Non-local priors</title><p>The form of the non-local prior densities imposed on the (non-zero) regression coefficients <inline-formula id="IE33"><mml:math id="IEQ33"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in this article take the form of a product of independent iMOM priors, or piMOM densities, expressible as
<disp-formula id="E5"><label>(5)</label><mml:math id="EQ5"><mml:mrow><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>k</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>&#x00393;</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x003a0;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mo stretchy="false">|</mml:mo></mml:mstyle><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p>Here <inline-formula id="IE34"><mml:math id="IEQ34"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a vector of coefficients of length <italic>k</italic>, and <inline-formula id="IE35"><mml:math id="IEQ35"><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The hyperparameter <italic>&#x003c4;</italic> represents a scale parameter that determines the dispersion of the prior around <inline-formula id="IE36"><mml:math id="IEQ36"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>, while <italic>r</italic> is similar to the shape parameter in the Inverse Gamma distribution and determines the tail behavior of the density. An example of an iMOM density is illustrated in <xref ref-type="fig" rid="btv764-F1">Figure 1</xref> for the particular case of <italic>r</italic> = 1 and <italic>&#x003c4;</italic> = 3.
<fig id="btv764-F1" orientation="portrait" position="float"><label>Fig. 1.</label><caption><p>iMOM prior for <italic>r</italic> = 1 and &#x003c4;&#x02009;=&#x02009;3</p></caption><graphic xlink:href="btv764f1p"/></fig>
</p><p>An important feature of this non-local prior, as highlighted in <xref rid="btv764-B16" ref-type="bibr">Johnson and Rossell (2012)</xref>, is that these priors do not necessarily impose significant penalties on non-sparse models, provided that the estimated coefficients in the non-sparse models are not too small. That is, large values of regression coefficients are not penalized since the value of the exponential kernel in (5) tends to 1 as <italic>&#x003b2;<sub>i</sub></italic> becomes large. This fact lies in stark contrast to most penalized likelihood methods.</p></sec><sec><title>2.2 Prior on model space</title><p>To define the prior on the model space, we adopt a subjective version of the prior proposed by <xref rid="btv764-B23" ref-type="bibr">Scott <italic>et al.</italic> (2010)</xref>. In the fully Bayesian version of the beta-binomial prior, this formulation specifies that the prior probability for model <inline-formula id="IE37"><mml:math id="IEQ37"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> is
<disp-formula id="E6"><label>(6)</label><mml:math id="EQ6"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>B</italic>(<italic>a</italic>, <italic>b</italic>) denotes the beta function and <italic>a</italic> and <italic>b</italic> are prior parameters that describe an underlying beta distribution on the marginal probability that a selected feature is associated with a non-zero regression coefficient in (2). This type of prior on the model size is also recommended in <xref rid="btv764-B6" ref-type="bibr">Castillo <italic>et al.</italic> (2015)</xref>, where it is suggested that an exponential decrease in prior probabilities with model size provides optimal results when the prior density on regression parameters has the form of a double exponential.</p><p>To incorporate our belief that the optimal predictive models are sparse, we arbitrarily set <inline-formula id="IE38"><mml:math id="IEQ38"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mi>min</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mo>&#x0230a;</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>&#x0230b;</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE39"><mml:math id="IEQ39"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula>. For large <italic>n</italic>, this implies that we expect, on average, <italic>a</italic> feature vectors to be included in the model. Here, <inline-formula id="IE40"><mml:math id="IEQ40"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mi>p</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x0003c;</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. This choice of <inline-formula id="IE41"><mml:math id="IEQ41"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for the prior hyperparameter reflects the belief that the number of models that can be constructed from available covariates should be smaller than the number of possible binary responses. Similarly, by restricting <italic>a</italic> to be less than <inline-formula id="IE42"><mml:math id="IEQ42"><mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, comparatively small prior probabilities are assigned to models that contain more than <inline-formula id="IE43"><mml:math id="IEQ43"><mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> covariates. Finally, we impose a deterministic constraint on model size and define <inline-formula id="IE44"><mml:math id="IEQ44"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> if <inline-formula id="IE45"><mml:math id="IEQ45"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&#x0003e;</mml:mo><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>A sensitivity analysis for <italic>a</italic> and <italic>b</italic> in (6) is provided in Section 4.1.1.</p><sec><title>2.2.1 Choosing hyperparameters</title><p>A critical aspect of implementing our model is the choice of the hyperparameters <italic>r</italic> and <italic>&#x003c4;</italic>. The value of <italic>r</italic> determines the tail behavior of the piMOM prior, while <italic>&#x003c4;</italic> plays a role similar to the tuning parameter in penalized likelihood methods, with its value largely determining the minimum value of a component of <inline-formula id="IE46"><mml:math id="IEQ46"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> that will be selected into a high posterior probability model.</p><p>To pick an appropriate, application-specific value for <italic>&#x003c4;</italic>, we adopt a strategy in which we compare the null distribution of the maximum likelihood estimator for <inline-formula id="IE47"><mml:math id="IEQ47"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (i.e., when all components of <inline-formula id="IE48"><mml:math id="IEQ48"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are 0), obtained from a randomly selected design matrix <inline-formula id="IE49"><mml:math id="IEQ49"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></inline-formula>, to the prior density on <inline-formula id="IE50"><mml:math id="IEQ50"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> under the alternative assumption that the components are non-zero. By choosing <italic>&#x003c4;</italic> to be just large enough so that the intersection of these two densities falls below a specified threshold, we are able to approximately bound the probability of false positives in the model, while at the same time maintaining sensitivity to regression coefficients that fall outside of the distribution of MLEs that estimate 0. In principle, we could employ this strategy to obtain a distinct value of <italic>&#x003c4;</italic> for each model <bold>k</bold>, but were unable to do so in this article because of the computational expense this procedure would impose. Instead, we mixed over models to obtain a single value of <italic>&#x003c4;</italic>.</p><p>Numerically, our strategy is implemented as follows. We begin by sampling a model from the prior on the model space. That is, we randomly sample <italic>k</italic> columns of <bold>X</bold> where <italic>k</italic> is determined by a draw from the prior on the model space. A Bernoulli vector of length <italic>n</italic> with success probability <inline-formula id="IE51"><mml:math id="IEQ51"><mml:mover accent="true"><mml:mi>&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math></inline-formula> is generated, where <inline-formula id="IE52"><mml:math id="IEQ52"><mml:mover accent="true"><mml:mi>&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math></inline-formula> is the proportion of successes in the observed data. Then the MLE for the model is estimated using standard logistic regression software with an intercept included in the model. This process is repeated <italic>N</italic> times to obtain a normal density approximation to the marginal density of maximum likelihood estimates under the condition that all true regression coefficients (except for the intercept) are 0. Typically, <inline-formula id="IE53"><mml:math id="IEQ53"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>Next, piMOM priors corresponding to different values of <italic>&#x003c4;</italic> are compared to the null distribution of the MLE. Based on these comparisons, we numerically determine the value of <italic>&#x003c4;</italic> so that the overlap of these densities falls below a threshold of <inline-formula id="IE54"><mml:math id="IEQ54"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. This overlap value is chosen heuristically in a way that suggests the number of false positives will decrease to 0 as <italic>p</italic> and <italic>n</italic> become large. Other thresholds of the form <inline-formula id="IE55"><mml:math id="IEQ55"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> might also be considered, but we have found that <inline-formula id="IE56"><mml:math id="IEQ56"><mml:mrow><mml:mi>&#x003b1;</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> provides good performance in a wide range of simulation studies and in real data examples. Further justification for this threshold is provided in the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/lookup/suppl/doi:10.1093/bioinformatics/btv764/-/DC1">supplementary data</ext-link>.</p><p>Notice that for a fixed <italic>p</italic>, the dispersion of the null distribution of the MLE around 0 decreases as the sample size <italic>n</italic> increases, although the rate of decrease is also affected by the structure of the design matrix <bold>X</bold>. This effect is illustrated in <xref ref-type="table" rid="btv764-T1">Table 1</xref>.
<table-wrap id="btv764-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Selected <italic>&#x003c4;</italic> parameter of piMOM prior for different simulation settings</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"><italic>n</italic> = 50</th><th rowspan="1" colspan="1"><italic>n</italic> = 100</th><th rowspan="1" colspan="1"><italic>n</italic> = 200</th><th rowspan="1" colspan="1"><italic>n</italic> = 400</th><th rowspan="1" colspan="1"><italic>n</italic> = 600</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1"><italic>P</italic> = 1000</td><td align="char" char="." rowspan="1" colspan="1">5.50</td><td align="char" char="." rowspan="1" colspan="1">1.66</td><td align="char" char="." rowspan="1" colspan="1">0.68</td><td align="char" char="." rowspan="1" colspan="1">0.30</td><td align="char" char="." rowspan="1" colspan="1">0.20</td></tr><tr><td rowspan="1" colspan="1"><italic>P</italic> = 10 000</td><td align="char" char="." rowspan="1" colspan="1">4.28</td><td align="char" char="." rowspan="1" colspan="1">1.85</td><td align="char" char="." rowspan="1" colspan="1">0.76</td><td align="char" char="." rowspan="1" colspan="1">0.34</td><td align="char" char="." rowspan="1" colspan="1">0.21</td></tr></tbody></table></table-wrap></p><p>We also note that a similar procedure for setting the scale parameter for local priors on the regression coefficients could potentially be implemented. Unfortunately, the application of this procedure to local priors can require extremely large values of tuning parameters in order to &#x02018;squash&#x02019; the prior near 0 and achieve small overlap with the null distribution. As a consequence of this fact, the tuning parameters selected by this procedure will not reflect any reasonable prior belief on the values of the regression parameters in a logistic model with a standardized design matrix.</p><p>To find an appropriate value of <italic>r</italic> for the piMOM prior (5), we impose a constraint that the prior mass assigned to the interval (&#x02212;10,10) equals 0.95. This constraint is imposed because coefficients larger than 10 in magnitude are not expected when the columns of the design matrix have been standardized.</p><p>Together, these constraints identify a unique combination of <italic>r</italic> and <italic>&#x003c4;</italic> for the piMOM prior.</p><p>A numerical strategy for finding this hyperparameter vector is outlined in Algorithm 1.</p><p><boxed-text id="btv764-BOX1" position="float" orientation="portrait"><caption><p><bold>Algorithm 1</bold> Choosing Appropriate <italic>r</italic> and <italic>&#x003c4;</italic> for piMOM</p></caption><p>1: <bold>Procedure</bold>
<sc>rtauselect</sc> (<inline-formula id="IE57"><mml:math id="IEQ57"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>)</p><p>2: &#x02002;&#x02002;&#x02002;<inline-formula id="IE58"><mml:math id="IEQ58"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo>&#x02190;</mml:mo><mml:mtext>Sample&#x02009;from&#x02009;Binomial</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>3: &#x02002;&#x02002;&#x02002;<bold>for</bold> (i in 1:N) <bold>do</bold></p><p>4: &#x02002;&#x02002;&#x02002;&#x02002;&#x02002;&#x02002;<inline-formula id="IE59"><mml:math id="IEQ59"><mml:mrow><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mtext>&#x02009;Sample&#x02009;from&#x02009;prior&#x02009;on&#x02009;model&#x02009;space&#x02009;in&#x02009;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>5: &#x02002;&#x02002;&#x02002;&#x02002;&#x02002; <inline-formula id="IE60"><mml:math id="IEQ60"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo>&#x02190;</mml:mo><mml:mtext>&#x02009;Randomly&#x02009;choose&#x02009;</mml:mtext><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mtext>&#x02009;columns&#x02009;from&#x02009;</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow></mml:math></inline-formula></p><p>6: &#x02002;&#x02002;&#x02002;&#x02002;&#x02002; <inline-formula id="IE61"><mml:math id="IEQ61"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02190;</mml:mo><mml:mtext>&#x02009;MLE&#x02009;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>7: &#x02002;&#x02002;&#x02002;&#x02002;&#x02002; <inline-formula id="IE62"><mml:math id="IEQ62"><mml:mrow><mml:mi>&#x003b2;</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mtext>,</mml:mtext><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></p><p>8: &#x02002; &#x02002; <inline-formula id="IE63"><mml:math id="IEQ63"><mml:mrow><mml:mi>f</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mtext>&#x02009;Normal&#x02009;density&#x02009;approximation&#x02009;to&#x02009;density&#x02009;of&#x02009;</mml:mtext><mml:mi>&#x003b2;</mml:mi></mml:mrow></mml:math></inline-formula></p><p>9: &#x02002;&#x02002; <inline-formula id="IE64"><mml:math id="IEQ64"><mml:mrow><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mtext>Overlap&#x02009;area&#x02009;between&#x02009;</mml:mtext><mml:mi>f</mml:mi><mml:mtext>&#x02009;and&#x02009;iMOM</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>10: &#x02002; <inline-formula id="IE65"><mml:math id="IEQ65"><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02190;</mml:mo><mml:mtext>&#x02009;Area&#x02009;under&#x02009;iMOM</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>&#x02009;outside&#x02009;the&#x02009;interval&#x02009;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>11: &#x02002; <inline-formula id="IE66"><mml:math id="IEQ66"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>&#x02190;</mml:mo><mml:munder><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003c4;</mml:mi></mml:mrow></mml:munder><mml:mtext>&#x02009;</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mi>p</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>t</mml:mi><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>0.05</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p><p>12: &#x02002; <bold>return</bold>
<inline-formula id="IE67"><mml:math id="IEQ67"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></p></boxed-text>
</p><p>Notice that this procedure for choosing the hyperparameters depends on the prior on the model space. This implies that <italic>&#x003c4;</italic> will tend to be larger in larger models, because it is more likely that the sampled columns <bold>X</bold> will exhibit high collinearity in large models. Ideally, we would adjust <italic>&#x003c4;</italic> for each individual model, but as mentioned earlier it was not computationally feasible to do so for the applications and simulations reported in this article.</p></sec></sec></sec><sec><title>3 Numerical aspects of implementation</title><p>The model described in Section 2 leads to a joint density for the data, model <inline-formula id="IE68"><mml:math id="IEQ68"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> and its parameters. As a result, the posterior distribution of model <inline-formula id="IE69"><mml:math id="IEQ69"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> and its coefficients can be expressed as
<disp-formula id="E7"><label>(7)</label><mml:math id="EQ7"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x0221d;</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>k</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>&#x00393;</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>k</mml:mi></mml:munderover><mml:mo>|</mml:mo></mml:mstyle><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mo>|</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mi>&#x003c4;</mml:mi><mml:mrow><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x000d7;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x0220f;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mtext>j</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Because of the high dimension of the parameter space and the complexity of the posterior density function in (7), it is not feasible to maximize this function analytically to obtain the HPPM. To search for the HPPM, we therefore utilized a Markov chain Monte Carlo algorithm. To reduce the dimension of the parameter space, we used a Laplace approximation to marginalize over the regression coefficient <inline-formula id="IE70"><mml:math id="IEQ70"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> associated with each model. The resulting approximation to the marginal posterior density of the data <bold>y</bold> under model <inline-formula id="IE71"><mml:math id="IEQ71"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula> can be expressed as
<disp-formula id="E8"><label>(8)</label><mml:math id="EQ8"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>&#x0222b;</mml:mo><mml:mi>&#x003c0;</mml:mi></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>&#x02009;</mml:mtext><mml:mi>d</mml:mi><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub><mml:mo>&#x02248;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mi>&#x003a3;</mml:mi><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p>Here <inline-formula id="IE72"><mml:math id="IEQ72"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the MAP estimate of <inline-formula id="IE73"><mml:math id="IEQ73"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE74"><mml:math id="IEQ74"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>&#x003a3;</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> is the determinant of the Hessian of the function <inline-formula id="IE75"><mml:math id="IEQ75"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, computed at <inline-formula id="IE76"><mml:math id="IEQ76"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo stretchy="true">&#x002dc;</mml:mo></mml:mover></mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The elements of the Hessian matrix can be expressed as
<disp-formula id="E9"><label>(9)</label><mml:math id="EQ9"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>;</mml:mo><mml:mtext>&#x02003;</mml:mtext><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>6</mml:mn><mml:mi>&#x003c4;</mml:mi><mml:msubsup><mml:mi>&#x003b2;</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>s</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>s</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>j</mml:mi><mml:mo>;</mml:mo><mml:mtext>&#x02003;</mml:mtext><mml:mstyle displaystyle="true"><mml:munder><mml:mo>&#x02211;</mml:mo><mml:mi>s</mml:mi></mml:munder><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>s</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mtext>s</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle></mml:mrow><mml:mo>&#x02032;</mml:mo></mml:msubsup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
</p><p>A simple birth-death scheme was used to sample from the posterior distribution. At each iteration of MCMC algorithm, each of the <italic>p</italic> covariates was visited in random order. The update at position <italic>i</italic> was performed by proposing a candidate model by flipping the inclusion state of that variable in the model. The candidate model was accepted using a Metropolis algorithm where the probability of accepting the candidate model, <inline-formula id="IE77"><mml:math id="IEQ77"><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mrow><mml:mtext>cand</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, was
<disp-formula id="E10"><label>(10)</label><mml:math id="EQ10"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mrow><mml:mtext>cand</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mrow><mml:mtext>cand</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mrow><mml:mtext>curr</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>n</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>k</mml:mi></mml:mstyle><mml:mrow><mml:mtext>curr</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p>The MAP estimate for <inline-formula id="IE78"><mml:math id="IEQ78"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mi mathvariant="bold-italic">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> was obtained using the <monospace>nlminb()</monospace> function in R. We assumed that an intercept was present in all models.</p><sec><title>3.1 Convergence diagnostics</title><p>Convergence diagnostics of MCMC can be used to assess whether an adequate number of iterations have been performed. Because of the high dimension of the parameter space for even moderately large <italic>p</italic>, we implemented a modified coupling diagnostic (<xref rid="btv764-B13" ref-type="bibr">Johnson, 1996</xref>, <xref rid="btv764-B14" ref-type="bibr">1998</xref>) to assess the probability that our MCMC algorithm had identified the true model. In the standard implementation of this method, one randomly initializes two MCMC chains by independently including each variable in the model according to a fixed probability. The components of the model in each chain are then updated synchronously, using the same uniform random deviate to perform acceptance/rejection of the candidate models. The chains are said to couple when the models from each chain are identical. Note that once the chains become coupled, they never uncouple. In theory, the distribution of the number of updates of the chains required to obtain coupling can be used to establish a bound on the total variation distance (TVD) between iterates in the chain and the target distribution.</p><p>In our implementation of the coupling diagnostic, we started 100 pairs of model chains. Each pair was updated until either they had coupled or all <italic>p</italic> components in each of the chains had been updated <italic>N</italic> times where <italic>N</italic> = 250. The (local) HPPM identified by each chain was recorded, and then the HPPM&#x02019;s for the 100 chains were compared. We then identified the global HPPM among the 100 models in the paired chains, and also examined the proportion of chains that had both coupled and identified the &#x02018;global&#x02019; HPPM.</p></sec></sec><sec><title>4 Results</title><p>To investigate the performance of the proposed model selection procedure, we applied our procedure to both simulated data sets and real data. We compared the performance of our algorithm to ISIS-SCAD (<xref rid="btv764-B8" ref-type="bibr">Fan and Lv, 2008</xref>) in both real and simulated data because ISIS-SCAD has proven to be among the most successful model selection procedures used in practice. For the real data analyses, we also compared our method to another Bayesian procedure based on the product moment prior (<xref rid="btv764-B21" ref-type="bibr">Rossell <italic>et al.</italic>, 2013)</xref>.</p><sec><title>4.1 Simulation studies</title><p>In all simulation studies, we assumed that the response vector represents a sequence of Bernoulli samples whose component probabilities of success are given by
<disp-formula id="E11"><label>(11)</label><mml:math id="EQ11"><mml:mrow><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>&#x000a0;</mml:mtext><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:msup><mml:mrow/><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mo mathvariant="bold-italic">&#x003b2;</mml:mo><mml:mo mathvariant="bold-italic">k</mml:mo></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
for a true model <bold>k</bold>.</p><p>Elements of the design matrix <bold>X</bold> were sampled from a multivariate normal distribution with mean 0 and covariance matrix &#x003a3;, where the diagonal elements of &#x003a3; were 1 and off diagonal elements were 0.5. That is,
<disp-formula id="E12"><label>(12)</label><mml:math id="EQ12"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mi>&#x003a3;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x000d7;</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>&#x003a3;</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>j</mml:mi></mml:mstyle></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup><mml:mtext>&#x02009;row&#x02009;of&#x02009;design&#x02009;matrix&#x02009;</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:mtext>Bernoulli</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
</p><p>Different combinations of <italic>n</italic> and <italic>p</italic> were investigated. Moreover, different ranges of regression coefficients were tested. In our simulations, the true model contained three variables. The following combinations of <italic>n</italic>, <italic>p</italic> and <inline-formula id="IE79"><mml:math id="IEQ79"><mml:mi>&#x003b2;</mml:mi></mml:math></inline-formula> were used to perform the simulation studies.
<list list-type="bullet"><list-item><p><inline-formula id="IE80"><mml:math id="IEQ80"><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn>400</mml:mn><mml:mo>,</mml:mo><mml:mn>600</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><inline-formula id="IE81"><mml:math id="IEQ81"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mn>10000</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula></p></list-item><list-item><p><inline-formula id="IE82"><mml:math id="IEQ82"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mtext>1</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mtext>2</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mtext>3</mml:mtext></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, where the non-zero coefficients of the <inline-formula id="IE83"><mml:math id="IEQ83"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mtext>i</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> vector were the <italic>i<sup>th</sup></italic> row of the matrix <inline-formula id="IE84"><mml:math id="IEQ84"><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>3</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>3</mml:mn></mml:mtd><mml:mtd><mml:mn>4</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>4</mml:mn></mml:mtd><mml:mtd><mml:mn>5</mml:mn></mml:mtd><mml:mtd><mml:mn>6</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p>The hyperparameters <italic>&#x003c4;</italic> and <italic>r</italic> for the piMOM prior were selected by the procedure explained in Section 2.2.1 for each of the 10 combinations of <italic>n</italic> and <italic>p</italic>. Values of <italic>&#x003c4;</italic> and <italic>r</italic> selected by this procedure are summarized in <xref ref-type="table" rid="btv764-T1">Tables 1</xref> and <xref ref-type="table" rid="btv764-T2">2</xref>, respectively.
<table-wrap id="btv764-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Selected <italic>r</italic> parameter of piMOM prior for different simulation settings</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"><italic>n</italic> = 50</th><th rowspan="1" colspan="1"><italic>n</italic> = 100</th><th rowspan="1" colspan="1"><italic>n</italic> = 200</th><th rowspan="1" colspan="1"><italic>n</italic> = 400</th><th rowspan="1" colspan="1"><italic>n</italic> = 600</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1"><italic>P</italic> = 1000</td><td align="char" char="." rowspan="1" colspan="1">2.04</td><td align="char" char="." rowspan="1" colspan="1">1.50</td><td align="char" char="." rowspan="1" colspan="1">1.24</td><td align="char" char="." rowspan="1" colspan="1">1.07</td><td align="char" char="." rowspan="1" colspan="1">1.00</td></tr><tr><td rowspan="1" colspan="1"><italic>P</italic> = 10 000</td><td align="char" char="." rowspan="1" colspan="1">1.90</td><td align="char" char="." rowspan="1" colspan="1">1.54</td><td align="char" char="." rowspan="1" colspan="1">1.27</td><td align="char" char="." rowspan="1" colspan="1">1.09</td><td align="char" char="." rowspan="1" colspan="1">1.01</td></tr></tbody></table></table-wrap></p><p>To run ISIS-SCAD, we used the R package &#x02018;SIS&#x02019; (<xref rid="btv764-B9" ref-type="bibr">Fan <italic>et al.</italic>, 2015)</xref> available from CRAN.</p><p>The variable selection procedure in both algorithms was run 50 times for each of the 30 combinations of <italic>n</italic>, <italic>p</italic> and <inline-formula id="IE85"><mml:math id="IEQ85"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi></mml:math></inline-formula>. In each trial, true and false positive values for iMOMLogit and ISIS-SCAD were counted by comparing the selected model with the true one. TP and FP rates were defined as the average true and false positive values over 50 trials. A true positive, TP, was defined to be the number of variables that were correctly selected, while false positives, FP, were the number of variables that were mistakenly selected.</p><p><xref ref-type="fig" rid="btv764-F2">Figures 2</xref> and <xref ref-type="fig" rid="btv764-F3">3</xref> show average TP and FP counts of both methods for all combinations of <italic>n</italic> and <italic>p</italic> and <inline-formula id="IE86"><mml:math id="IEQ86"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The figures for <inline-formula id="IE87"><mml:math id="IEQ87"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE88"><mml:math id="IEQ88"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> are provided in the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/lookup/suppl/doi:10.1093/bioinformatics/btv764/-/DC1">supplementary materials</ext-link> and demonstrate similar trends. In all cases, the average FP count for iMOMLogit was less than ISIS-SCAD, while its average TP count was higher. The only case where both iMOMLogit and ISIS-SCAD had the same average TP count was when they both found the true model in all 50 simulation trials.
<fig id="btv764-F2" orientation="portrait" position="float"><label>Fig. 2.</label><caption><p>Average true positive count for <inline-formula id="IE96"><mml:math id="IEQ96"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula></p></caption><graphic xlink:href="btv764f2p"/></fig>
<fig id="btv764-F3" orientation="portrait" position="float"><label>Fig. 3.</label><caption><p>Average false positive count for <inline-formula id="IE97"><mml:math id="IEQ97"><mml:mrow><mml:msub><mml:mi>&#x003b2;</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula></p></caption><graphic xlink:href="btv764f3p"/></fig>
</p><p>We next compared the performance of both methods in estimating the regression coefficients. For each simulation setting, we compared the mean squared error in estimating the probability of success for each binary observation by performing 10-fold cross validation. The point estimate <inline-formula id="IE89"><mml:math id="IEQ89"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo stretchy="true">&#x002c6;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> was estimated as the posterior mode under the HPPM. The predicted value of <inline-formula id="IE90"><mml:math id="IEQ90"><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover></mml:math></inline-formula> was then computed according to (1). Note that the prediction of the response vector involves both coefficient estimation and variable selection. The mean squared error of prediction (MSE) was defined as follows:
<disp-formula id="E13"><label>(13)</label><mml:math id="EQ13"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo>&#x02212;</mml:mo><mml:mi mathvariant="bold-italic">&#x003c0;</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003c0;</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p>The comparison between cross validated MSEs of both methods is shown in <xref ref-type="fig" rid="btv764-F4">Figures 4</xref> and <xref ref-type="fig" rid="btv764-F5">5</xref>. As in the comparisons of TP and FP rates, these figures suggest that iMOMLogit is preferred to ISIS-SCAD in estimating the success probabilities of binary observations.
<fig id="btv764-F4" orientation="portrait" position="float"><label>Fig. 4.</label><caption><p>10-fold cross validation MSE<inline-formula id="IE98"><mml:math id="IEQ98"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="true">&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of iMOMLogit vs. ISIS-SCAD, <italic>P</italic> = 1000</p></caption><graphic xlink:href="btv764f4p"/></fig>
<fig id="btv764-F5" orientation="portrait" position="float"><label>Fig. 5.</label><caption><p>10-fold cross validation MSE<inline-formula id="IE99"><mml:math id="IEQ99"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>&#x003c0;</mml:mi><mml:mo stretchy="true">&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> of iMOMLogit vs. ISIS-SCAD, <italic>P</italic>= 10 000</p></caption><graphic xlink:href="btv764f5p"/></fig>
</p><sec><title>4.1.1 Sensitivity analysis for prior parameters on model space</title><p>To assess the sensitivity of our results to the prior hyperparameters on the model space (6), we conducted a brief sensitivity analysis under the simulation settings for which <italic>n</italic> = 200, <italic>p</italic> = 1000 and <inline-formula id="IE91"><mml:math id="IEQ91"><mml:mrow><mml:mi mathvariant="bold-italic">&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. We also fixed <inline-formula id="IE92"><mml:math id="IEQ92"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> as before. This insured that the prior mean of the number of variables selected would be <italic>a</italic>. Based on the default procedure for defining <italic>a</italic> described in Section 2.2, the default value for <italic>a</italic> in this setting was 6. We examined sensitivity to this choice of <italic>a</italic> by varying <italic>a</italic> around this default value within the interval (3, 9). To quantitatively assess the sensitivity of the selection procedure to values of <italic>a</italic> in this range, we examined the consequent changes to <inline-formula id="IE93"><mml:math id="IEQ93"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> described in (13). This measure incorporates errors in both variable selection and coefficient estimation.</p><p>The figure provided in the <ext-link ext-link-type="uri" xlink:href="http://bioinformatics.oxfordjournals.org/lookup/suppl/doi:10.1093/bioinformatics/btv764/-/DC1">supplementary material</ext-link> depicts <inline-formula id="IE94"><mml:math id="IEQ94"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi mathvariant="bold-italic">&#x003c0;</mml:mi><mml:mo>&#x002c6;</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> for different values of <italic>a</italic> in the described simulation setting. As shown in that figure, model output does not change dramatically with changes in <italic>a</italic>, varying by at most <inline-formula id="IE95"><mml:math id="IEQ95"><mml:mrow><mml:mn>4.8</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> from the default choice of <italic>a</italic>.</p></sec></sec><sec><title>4.2 Real data analysis</title><p>We applied iMOMLogit to two data sets, one with a small sample size and one with a large sample size. These two data sets are publicly available and have good clinical annotations. The first data set was the Golub leukemia data (<xref rid="btv764-B11" ref-type="bibr">Golub <italic>et al.</italic>, 1999)</xref>. The goal of our analysis for these data was to discriminate between two types of acute leukemia, myeloid (AML) and lymphoblastic (ALL). The design matrix consisted of gene expression levels produced by cDNA microarrays from bone marrow samples, and was pre-processed by RMA (<xref rid="btv764-B12" ref-type="bibr">Irizarry <italic>et al.</italic>, 2003)</xref>. There are 72 samples and 7,129 genes in the data set. The second data set was the clear cell Renal Cell Carcinoma (ccRCC) RNAseq data, available from the Cancer Genome Atlas projects (<xref rid="btv764-B4" ref-type="bibr">Cancer Genome Atlas Research Network, 2013</xref>) (TCGA). There were 467 tumor samples and more than 20 000 genes in this data set.</p><p>As mentioned earlier, we also compared our selection procedure results to a related Bayesian method proposed in <xref rid="btv764-B21" ref-type="bibr">Rossell <italic>et al.</italic> (2013)</xref>, called pmomPM. This method uses a probit link function with a moment prior, (pMOM), another type of non-local prior. The pMOM prior has Gaussian tails and decreases quadratically near the origin. We implemented this method with the default hyperparameter suggested in <xref rid="btv764-B21" ref-type="bibr">Rossell <italic>et al.</italic> (2013)</xref> for sparse models. To run pmomPM method, we used the R package &#x02019;mombf&#x02019; (<xref rid="btv764-B22" ref-type="bibr">Rossell <italic>et al.</italic>, 2015)</xref> available from CRAN.</p><p>In contrast to iMOMLogit and ISIS-SCAD, the mombf package focuses on prediction using Bayesian model averaging, rather than on the identification of biologically important genes using the HPPM. Because of the behavior of the pMOM prior near the origin, the pMOM model selects many more genes in the models over which it averages. Though model averaging can improve prediction accuracy (<xref rid="btv764-B20" ref-type="bibr">Raftery <italic>et al.</italic>, 1997)</xref>, the current version of the mombf package does not provide estimates of the HPPM, which complicates comparisons with the other methods considered here. These attributes of the pmomPM method are illustrated in the examples that follow.</p><sec><title>4.2.1 Leukemia data</title><p>Following <xref rid="btv764-B11" ref-type="bibr">Golub <italic>et al.</italic> (1999)</xref>, we split the data into training and test sets. The training set contained 38 samples, with 27 ALL and 11 AML. The testing set contained 34 samples, with 20 ALL and 14 AML.</p><p><xref ref-type="table" rid="btv764-T3">Table 3</xref> summarizes the results of applying iMOMLogit, ISIS-SCAD and pmomPM to these data. The error rate for predicting the test data observations was 5.88% for iMOMLogit, which misclassified 2 out of 34 observations, samples 17 and 31. Both ISIS-SCAD and the method described in <xref rid="btv764-B11" ref-type="bibr">Golub <italic>et al.</italic> (1999)</xref> resulted in an error rate of 14.7%. ISIS-SCAD achieved this error rate by finding two significant genes, &#x02018;Zyxin&#x02019; and &#x02018;FAH&#x02019;, whereas <xref rid="btv764-B11" ref-type="bibr">Golub <italic>et al.</italic> (1999)</xref> selected 50 genes. The pmomPM method achieved an error rate of 23.53% with an average model size of 11.08. None of the genes were assigned marginal posterior probability of 0.5 by the pmomPM method; the highest marginal posterior probability of any gene was 0.052, acheived by CD33.
<table-wrap id="btv764-T3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Comparison between iMOMLogit and other methods for leukemia data set</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Method</th><th rowspan="1" colspan="1">Error rate</th><th rowspan="1" colspan="1">Reported genes</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">iMOMLogit</td><td rowspan="1" colspan="1">5.88%</td><td rowspan="1" colspan="1">Zyxin</td></tr><tr><td rowspan="1" colspan="1">ISIS-SCAD</td><td rowspan="1" colspan="1">14.70%</td><td rowspan="1" colspan="1">Zyxin - FAH</td></tr><tr><td rowspan="1" colspan="1">pmomPM</td><td rowspan="1" colspan="1">23.53%</td><td rowspan="1" colspan="1">No genes had marginal posterior probability greater than 0.5</td></tr></tbody></table></table-wrap></p><p>iMOMLogit selected a model containing only one gene named &#x02018;Zyxin&#x02019;, which perfectly predicted the classifications in the training data. This gene was also listed in the top 50 genes reported by <xref rid="btv764-B11" ref-type="bibr">Golub <italic>et al.</italic> (1999)</xref>, and was found to be advantageous for classifying the two types of leukemia in four published data sets (<xref rid="btv764-B3" ref-type="bibr">Baker and Kramer, 2006</xref>). The gene &#x02018;FAH&#x02019; found only by ISIS-SCAD is involved in certain metabolic pathways that are not known to be associated with leukemia (Kegg.org).</p><p>Following the methodology discussed in Section 3.1, 74% of pairs of chains that were updated using the coupling algorithm found the same highest posterior probability model (HPPM). Among all pairs, 95% coupled.</p></sec><sec><title>4.2.2 Renal cell carcinoma data</title><p>The second data set was generated by the <xref rid="btv764-B4" ref-type="bibr">Cancer Genome Atlas Research Network (2013)</xref> and contained Illumina HiSeq data on mRNA expression for 467 patient samples. The survival outcomes of these patients were available. A hierarchical clustering of the gene expression data [preprocessed using DeMix (<xref rid="btv764-B1" ref-type="bibr">Ahn <italic>et al.</italic>, 2013)</xref> to remove stromal contamination] were performed on the data. That led to the identification of four clusters of patients based on survival times. To apply iMOMLogit, we considered two of those clusters, presenting the best and worst survival outcomes and labeled them as 0 (worst) and 1 (best). The resulting number of samples included in our analysis was 193, with 14150 features in the design matrix.</p><p>The results using iMOMLogit, ISIS-SCAD and pmomPM are summarized in <xref ref-type="table" rid="btv764-T4">Table 4</xref>. To compare methods, we performed a 10-fold cross-validation. The error rate of iMOMLogit was 9.79%, ISIS-SCAD&#x02019;s error rate was 12.97%, and pmomPM was 9.84%. In the model selected by iMOMLogit, there were 3 significant genes named &#x02018;C7orf43&#x02019;, &#x02018;NUMBL&#x02019; and &#x02018;SAV1&#x02019;, with the latter two being uniquely identified by our model. &#x02018;NUMBL&#x02019; participates in the Notch signaling pathway and is believed to contribute to nervous system tumors (glioma) (<xref rid="btv764-B24" ref-type="bibr">Tao <italic>et al.</italic>, 2012)</xref> as well as lung cancer (<xref rid="btv764-B29" ref-type="bibr">Yingjie <italic>et al.</italic>, 2013)</xref>. The Notch signaling pathway is highly conserved, manages communication between adjacent cells and maintenance of adult stem cells, and is linked to the development of various cancers (<xref rid="btv764-B2" ref-type="bibr">Alketbi and Attoub, 2015</xref>). Not surprisingly, we identified NUMBL as differentiating two groups of kidney patients. &#x02018;SAV1&#x02019; has been reported to play a role in kidney cancer (<xref rid="btv764-B19" ref-type="bibr">Matsuura <italic>et al.</italic>, 2011)</xref>, and is located in a Hippo signaling pathway (Kegg.org). The Hippo signaling pathway is highly conserved and controls epithelial tissue growth. Recently, its relation to other signaling pathways has been studied to identify new therapeutic interventions for cancer (<xref rid="btv764-B28" ref-type="bibr">Yimlamai <italic>et al.</italic>, 2015)</xref>.
<table-wrap id="btv764-T4" orientation="portrait" position="float"><label>Table 4.</label><caption><p>Comparison between iMOMLogit and other methods for renal cell carcinoma data set</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Method</th><th rowspan="1" colspan="1">Error rate</th><th rowspan="1" colspan="1">Reported genes</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">iMOMLogit</td><td rowspan="1" colspan="1">9.79%</td><td rowspan="1" colspan="1">C7orf43 - NUMBL - SAV1</td></tr><tr><td rowspan="1" colspan="1">ISIS-SCAD</td><td rowspan="1" colspan="1">12.97%</td><td rowspan="1" colspan="1">C7orf43 - C19orf66 - ATXN7L2 - MICAL1</td></tr><tr><td rowspan="1" colspan="1">pmomPM</td><td rowspan="1" colspan="1">9.84%</td><td rowspan="1" colspan="1">No genes had marginal posterior probability greater than 0.5</td></tr></tbody></table></table-wrap></p><p>Among all pairs of chains with different random starts, 32% of them reported the same global HPPM and 6% of paired chains were coupled. This suggests that convergence in this data set was more problematic, and that our multiple coupled chain approach (or other modifications of the standard, single chain MCMC algorithm) is required to identify the HPPM model.</p><p>The genes uniquely selected by ISIS-SCAD were &#x02018;C19orf66&#x02019;, &#x02018;ATXN7L2&#x02019; and &#x02018;MIICAL1&#x02019;. &#x02018;ATXN7L2&#x02019; was previously reported to be associated with non-small cell lung cancer (<xref rid="btv764-B27" ref-type="bibr">Wu <italic>et al.</italic>, 2013</xref>), whereas &#x02018;MICAL1&#x02019; was previously reported to control survival in melanoma cell lines.</p><p>As for the leukemia data, the pmomPM selected substantially more genes in each of its sampled models, and the genes selected in each model were highly variable. The average model size of the pmomPM method for this data set was 13.84. As before, none of the genes were assigned marginal probability of 0.5; the highest marginal posterior probability assigned to any gene was 0.33, for API5.</p><p>The genes identified by iMOMLogit seem to be more biologically meaningful and better annotated in the literature for ccRCC than those selected by ISIS-SCAD.</p></sec></sec></sec><sec><title>5 Discussion</title><p>In this article, we introduced a Bayesian method, iMOMLogit, for variable selection in binary response regression problems in high and ultrahigh-dimensional settings. There are many applications associated with these type of data. Such data are of great interest to bioinformaticians and biologists, who routinely collect gene expression data to find prognostic features to classify cancer types.</p><p>For two real datasets, iMOMLogit identified sparse models with low prediction error rates. In both cases, biological considerations suggest that the genes reported by iMOMLogit appear to be valid predictors of biological outcomes.</p><p>The primary disadvantage of the iMOMLogit procedure is that it is computationally much more intensive than ISIS-SCAD and related penalized likelihood methods. We are currently investigating methods for reducing the computational burden of our algorithm by implementing various screening procedures that are similar to those used in ISIS-SCAD.</p></sec>
<sec sec-type="supplementary-material">
<title>Supplementary Material</title>
<supplementary-material id="PMC_1" content-type="local-data">
<caption>
<title>Supplementary Data</title>
</caption>
<media mimetype="text" mime-subtype="html" xlink:href="supp_32_9_1338__index.html"/>
<media xlink:role="associated-file" mimetype="application" mime-subtype="pdf"
xlink:href="supp_btv764_Supplementary_Feb_18th.pdf"/>
</supplementary-material>
</sec>
</body><back><ack><title>Acknowledgements</title><p>The authors would like to thank John Hancock and three anonymous referees for their valuable comments that improved the presentation of the materials in this article. The authors would also like to thank Jaeil Ahn for providing the deconvolved RNAseq data for ccRCC samples as well as the two clusters of patients with most distinct survival outcomes.</p></ack><sec><title>Funding</title><p>National Institute of Health (R01CA158113 to A.N. and V.E.J.); National Cancer Institute (1R01CA174206-01 to A.N. and W.W., 1R01CA183793-01 and P30 CA016672 to W.W.); Cancer Prevention Research Institute of Texas (RP130090 to W.W.).</p><p><italic>Conflict of Interest</italic>: none declared.</p></sec><ref-list><title>References</title><ref id="btv764-B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahn</surname><given-names>J.</given-names></name></person-group>
<etal/> (<year>2013</year>) <article-title>Demix: deconvolution for mixed cancer transcriptomes using raw measured data</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>1865</fpage>&#x02013;<lpage>1871</lpage>.<pub-id pub-id-type="pmid">23712657</pub-id></mixed-citation></ref><ref id="btv764-B2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alketbi</surname><given-names>A.</given-names></name><name><surname>Attoub</surname><given-names>S.</given-names></name></person-group> (<year>2015</year>) <article-title>Notch signaling in cancer: Rationale and strategies for targeting</article-title>. <source>Curr. Cancer Drug Targets</source>, <volume>15</volume>, <fpage>364</fpage>&#x02013;<lpage>374</lpage>.<pub-id pub-id-type="pmid">26239151</pub-id></mixed-citation></ref><ref id="btv764-B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>S.G.</given-names></name><name><surname>Kramer</surname><given-names>B.S.</given-names></name></person-group> (<year>2006</year>) <article-title>Identifying genes that contribute most to good classification in microarrays</article-title>. <source>BMC Bioinformatics</source>, <volume>7</volume>, <fpage>407</fpage><pub-id pub-id-type="pmid">16959042</pub-id></mixed-citation></ref><ref id="btv764-B4"><mixed-citation publication-type="journal"><collab>Cancer Genome Atlas Research Network</collab> (<year>2013</year>) <article-title>Comprehensive molecular characterization of clear cell renal cell carcinoma</article-title>. <source>Nature</source>, <volume>499</volume>, <fpage>43</fpage>&#x02013;<lpage>49</lpage>.<pub-id pub-id-type="pmid">23792563</pub-id></mixed-citation></ref><ref id="btv764-B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Candes</surname><given-names>E.</given-names></name><name><surname>Tao</surname><given-names>T.</given-names></name></person-group> (<year>2007</year>) <article-title>The dantzig selector: Statistical estimation when p is much larger than n</article-title>. <source>Ann. Stat</source>., <fpage>2313</fpage>&#x02013;<lpage>2351</lpage>.</mixed-citation></ref><ref id="btv764-B6"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castillo</surname><given-names>I.</given-names></name></person-group>
<etal/> (<year>2015</year>) <article-title>Bayesian linear regression with sparse priors</article-title>. <source>Ann. Statist</source>., <volume>43</volume>, <fpage>1986</fpage>&#x02013;<lpage>2018</lpage>.</mixed-citation></ref><ref id="btv764-B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>R.</given-names></name></person-group> (<year>2001</year>) <article-title>Variable selection via nonconcave penalized likelihood and its oracle properties</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>96</volume>, <fpage>1348</fpage>&#x02013;<lpage>1360</lpage>.</mixed-citation></ref><ref id="btv764-B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>J.</given-names></name><name><surname>Lv</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>) <article-title>Sure independence screening for ultrahigh dimensional feature space</article-title>. <source>J. Roy. Stat. Soc. B</source>, <volume>70</volume>, <fpage>849</fpage>&#x02013;<lpage>911</lpage>.</mixed-citation></ref><ref id="btv764-B9"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>J.</given-names></name></person-group>
<etal/> (<year>2015</year>) <italic>SIS: Sure Independence Screening</italic>. R package version 0.7-6.</mixed-citation></ref><ref id="btv764-B10"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>George</surname><given-names>E.I.</given-names></name><name><surname>McCulloch</surname><given-names>R.E.</given-names></name></person-group> (<year>1997</year>) <article-title>Approaches for bayesian variable selection</article-title>. <source>Statistica Sinica</source>, <volume>7</volume>, <fpage>339</fpage>&#x02013;<lpage>373</lpage>.</mixed-citation></ref><ref id="btv764-B11"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golub</surname><given-names>T.R.</given-names></name></person-group>
<etal/> (<year>1999</year>) <article-title>Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</article-title>. <source>Science</source>, <volume>286</volume>, <fpage>531</fpage>&#x02013;<lpage>537</lpage>.<pub-id pub-id-type="pmid">10521349</pub-id></mixed-citation></ref><ref id="btv764-B12"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Irizarry</surname><given-names>R.A.</given-names></name></person-group>
<etal/> (<year>2003</year>) <article-title>Summaries of affymetrix genechip probe level data</article-title>. <source>Nucleic Acids Res</source>., <volume>31</volume>, <fpage>e15</fpage>&#x02013;<lpage>e15</lpage>.<pub-id pub-id-type="pmid">12582260</pub-id></mixed-citation></ref><ref id="btv764-B13"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>V.E.</given-names></name></person-group> (<year>1996</year>) <article-title>Studying convergence of markov chain monte carlo algorithms using coupled sample paths</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>91</volume>, <fpage>154</fpage>&#x02013;<lpage>166</lpage>.</mixed-citation></ref><ref id="btv764-B14"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>V.E.</given-names></name></person-group> (<year>1998</year>) <article-title>A coupling-regeneration scheme for diagnosing convergence in markov chain monte carlo algorithms</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>93</volume>, <fpage>238</fpage>&#x02013;<lpage>248</lpage>.</mixed-citation></ref><ref id="btv764-B15"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>V.E.</given-names></name></person-group> (<year>2013</year>) <article-title>On numerical aspects of bayesian model selection in high and ultrahigh-dimensional settings</article-title>. <source>Bayesian Anal</source>., <volume>8</volume>, <fpage>741</fpage>&#x02013;<lpage>758</lpage>.<pub-id pub-id-type="pmid">24683431</pub-id></mixed-citation></ref><ref id="btv764-B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>V.E.</given-names></name><name><surname>Rossell</surname><given-names>D.</given-names></name></person-group> (<year>2012</year>) <article-title>Bayesian model selection in high-dimensional settings</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>107</volume>, <fpage>649</fpage>&#x02013;<lpage>660</lpage>.</mixed-citation></ref><ref id="btv764-B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>K.E.</given-names></name></person-group>
<etal/> (<year>2003</year>) <article-title>Gene selection: a bayesian variable selection approach</article-title>. <source>Bioinformatics</source>, <volume>19</volume>, <fpage>90</fpage>&#x02013;<lpage>97</lpage>.<pub-id pub-id-type="pmid">12499298</pub-id></mixed-citation></ref><ref id="btv764-B18"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>F.</given-names></name></person-group>
<etal/> (<year>2008</year>) <article-title>Mixtures of g priors for bayesian variable selection</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>103</volume>.</mixed-citation></ref><ref id="btv764-B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsuura</surname><given-names>K.</given-names></name></person-group>
<etal/> (<year>2011</year>) <article-title>Downregulation of SAV1 plays a role in pathogenesis of high-grade clear cell renal cell carcinoma</article-title>. <source>BMC Cancer</source>, <volume>11</volume>, <fpage>523</fpage>.<pub-id pub-id-type="pmid">22185343</pub-id></mixed-citation></ref><ref id="btv764-B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raftery</surname><given-names>A.E.</given-names></name></person-group>
<etal/> (<year>1997</year>) <article-title>Bayesian model averaging for linear regression models</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>92</volume>, <fpage>179</fpage>&#x02013;<lpage>191</lpage>.</mixed-citation></ref><ref id="btv764-B21"><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rossell</surname><given-names>D.</given-names></name></person-group>
<etal/> (<year>2013</year>). <chapter-title>High-dimensional bayesian classifiers using non-local priors</chapter-title> In: <source>Statistical Models for Data Analysis</source>, pp. <fpage>305</fpage>&#x02013;<lpage>313</lpage>. <publisher-name>Springer</publisher-name>.</mixed-citation></ref><ref id="btv764-B22"><mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Rossell</surname><given-names>D.</given-names></name></person-group>
<etal/> (<year>2015</year>) <italic>mombf: Moment and Inverse Moment Bayes Factors</italic>. R package version 1.6.1.</mixed-citation></ref><ref id="btv764-B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>J.G.</given-names></name><name><surname>Berger</surname><given-names>J.O</given-names></name></person-group>
<etal/> (<year>2010</year>) <article-title>Bayes and empirical-bayes multiplicity adjustment in the variable-selection problem</article-title>. <source>Ann. Stat</source>., <volume>38</volume>, <fpage>2587</fpage>&#x02013;<lpage>2619</lpage>.</mixed-citation></ref><ref id="btv764-B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tao</surname><given-names>T.</given-names></name></person-group>
<etal/> (<year>2012</year>) <article-title>Numbl inhibits glioma cell migration and invasion by suppressing TRAF5-mediated NF-B activation</article-title>. <source>Mol. Biol. Cell</source>, <volume>23</volume>, <fpage>2635</fpage>&#x02013;<lpage>2644</lpage>.<pub-id pub-id-type="pmid">22593207</pub-id></mixed-citation></ref><ref id="btv764-B25"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>) <article-title>Regression shrinkage and selection via the lasso</article-title>. <source>J. Roy. Stat. Soc. B (Methodological)</source>, <fpage>267</fpage>&#x02013;<lpage>288</lpage>.</mixed-citation></ref><ref id="btv764-B26"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>West</surname><given-names>M.</given-names></name></person-group>
<etal/> (<year>2000</year>) <article-title>Dna microarray data analysis and regression modeling for genetic expression profiling</article-title>. <source>ISDS Discussion</source>.</mixed-citation></ref><ref id="btv764-B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>X.</given-names></name></person-group>
<etal/> (<year>2013</year>) <article-title>Genome-wide association study of genetic predictors of overall survival for non-small cell lung cancer in never smokers</article-title>. <source>Cancer Res</source>., <volume>73</volume>, <fpage>4028</fpage>&#x02013;<lpage>4038</lpage>.<pub-id pub-id-type="pmid">23704207</pub-id></mixed-citation></ref><ref id="btv764-B28"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yimlamai</surname><given-names>D.</given-names></name></person-group>
<etal/> (<year>2015</year>) <article-title>Emerging evidence on the role of the hippo/yap pathway in liver physiology and cancer</article-title>. <source>J. Hepatol</source>., <volume>63</volume>, <fpage>1491</fpage>&#x02013;<lpage>1501</lpage>.<pub-id pub-id-type="pmid">26226451</pub-id></mixed-citation></ref><ref id="btv764-B29"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yingjie</surname><given-names>L.</given-names></name></person-group>
<etal/> (<year>2013</year>) <article-title>Numblike regulates proliferation, apoptosis, and invasion of lung cancer cell</article-title>. <source>Tumour Biol</source>., <volume>34</volume>, <fpage>2773</fpage>&#x02013;<lpage>2780</lpage>.<pub-id pub-id-type="pmid">23681800</pub-id></mixed-citation></ref><ref id="btv764-B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>H.</given-names></name></person-group> (<year>2006</year>) <article-title>The adaptive lasso and its oracle properties</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>101</volume>, <fpage>1418</fpage>&#x02013;<lpage>1429</lpage>.</mixed-citation></ref></ref-list></back></article>