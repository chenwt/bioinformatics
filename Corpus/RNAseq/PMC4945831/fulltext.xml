<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d1 20130915//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName JATS-journalpublishing1.dtd?><?SourceDTD.Version 39.96?><?ConverterInfo.XSLTName jp2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id><journal-id journal-id-type="iso-abbrev">Brief. Bioinformatics</journal-id><journal-id journal-id-type="publisher-id">bib</journal-id><journal-id journal-id-type="hwp">bib</journal-id><journal-title-group><journal-title>Briefings in Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1467-5463</issn><issn pub-type="epub">1477-4054</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4945831</article-id><article-id pub-id-type="doi">10.1093/bib/bbv108</article-id><article-id pub-id-type="publisher-id">bbv108</article-id><article-categories><subj-group subj-group-type="heading"><subject>Special Issue continued: Computational Systems Biomedicine Papers</subject></subj-group></article-categories><title-group><article-title>Dimension reduction techniques for the integrative analysis of multi-omics data</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Meng</surname><given-names>Chen</given-names></name><xref ref-type="author-notes" rid="bbv108-NT1">*</xref><xref ref-type="bio" rid="d36e39">*</xref></contrib><contrib contrib-type="author"><name><surname>Zeleznik</surname><given-names>Oana A.</given-names></name><xref ref-type="author-notes" rid="bbv108-NT1">*</xref><xref ref-type="bio" rid="d36e52">*</xref></contrib><contrib contrib-type="author"><name><surname>Thallinger</surname><given-names>Gerhard G.</given-names></name><xref ref-type="bio" rid="d36e63">*</xref></contrib><contrib contrib-type="author"><name><surname>Kuster</surname><given-names>Bernhard</given-names></name><xref ref-type="bio" rid="d36e74">*</xref></contrib><contrib contrib-type="author"><name><surname>Gholami</surname><given-names>Amin M.</given-names></name><xref ref-type="bio" rid="d36e85">*</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Culhane</surname><given-names>Aed&#x000ed;n C.</given-names></name><xref ref-type="bio" rid="d36e96">*</xref></contrib></contrib-group><author-notes><corresp id="bbv108-COR2">Corresponding author: Aed&#x000ed;n C. Culhane, Dana-Farber Cancer Institute, 450 Brookline Ave, Boston, MA 02115, USA. Tel.: <phone>+1 (617) 632 2468</phone>; Fax: <fax>+1 (617) 582 7760</fax>; E-mail: <email>aedin@jimmy.harvard.edu</email></corresp><fn id="bbv108-NT1"><p>*These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="ppub"><month>7</month><year>2016</year></pub-date><pub-date pub-type="epub"><day>11</day><month>3</month><year>2016</year></pub-date><pub-date pub-type="pmc-release"><day>11</day><month>3</month><year>2016</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. --><volume>17</volume><issue>4</issue><issue-title>Special Issue continued: Computational Systems Biomedicine</issue-title><fpage>628</fpage><lpage>641</lpage><history><date date-type="received"><day>8</day><month>6</month><year>2015</year></date><date date-type="rev-recd"><day>26</day><month>10</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; The Author 2016. Published by Oxford University Press.</copyright-statement><copyright-year>2016</copyright-year><license xlink:href="http://creativecommons.org/licenses/by/4.0/" license-type="creative-commons"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract><p>State-of-the-art next-generation sequencing, transcriptomics, proteomics and other high-throughput &#x02018;omics' technologies enable the efficient generation of large experimental data sets. These data may yield unprecedented knowledge about molecular pathways in cells and their role in disease. Dimension reduction approaches have been widely used in exploratory analysis of single omics data sets. This review will focus on dimension reduction approaches for simultaneous exploratory analyses of multiple data sets. These methods extract the linear relationships that best explain the correlated structure across data sets, the variability both within and between variables (or observations) and may highlight data issues such as batch effects or outliers. We explore dimension reduction techniques as one of the emerging approaches for data integration, and how these can be applied to increase our understanding of biological systems in normal physiological function and disease.</p></abstract><kwd-group><kwd>multivariate analysis</kwd><kwd>multi-omics data integration</kwd><kwd>dimension reduction</kwd><kwd>integrative genomics</kwd><kwd>exploratory data analysis</kwd><kwd>multi-assay</kwd></kwd-group><counts><page-count count="14"/></counts></article-meta></front><body><sec sec-type="intro"><title>Introduction</title><p>Technological advances and lower costs have resulted in studies using multiple comprehensive molecular profiling or omics assays on each biological sample. Large national and international consortia including The Cancer Genome Atlas (TCGA) and The International Cancer Genome Consortium have profiled thousands of biological samples, assaying multiple different molecular profiles per sample, including mRNA, microRNA, methylation, DNA sequencing and proteomics. These data have the potential to reveal great insights into the mechanism of disease and to discover novel biomarkers; however, statistical methods for integrative analysis of multi-omics (or multi-assay) data are only emerging.</p><p>Exploratory data analysis (EDA) is an important early step in omics data analysis [<xref rid="bbv108-B1" ref-type="bibr">1</xref>]. It summarizes the main characteristics of data and may identify potential issues such as batch effects [<xref rid="bbv108-B2" ref-type="bibr">2</xref>] and outliers. Techniques for EDA include cluster analysis and dimension reduction. Both have been widely applied to transcriptomics data analysis [<xref rid="bbv108-B1" ref-type="bibr">1</xref>], but there are advantages to dimension reduction approaches when integrating multi-assay data. While cluster analysis generally investigates pairwise distances between objects looking for fine relationships, dimension reduction or latent variable methods consider the global variance of the data set, highlighting general gradients or patterns in the data [<xref rid="bbv108-B3" ref-type="bibr">3</xref>].</p><p>Biological data frequently have complex phenotypes and depending on the subset of variables analyzed, multiple valid clustering classifications may co-exist. Dimension reduction approaches decompose the data into a few new variables (called components) that explain most of the differences in observations. For example, a recent dimension reduction analysis of bladder cancers identified components associated with batch effects, GC content in the RNA sequencing data, in addition to seven components that were specific to tumor cells and three components associated with tumor stroma [<xref rid="bbv108-B4" ref-type="bibr">4</xref>]. By contrast, most clustering approaches are optimized for discovery of discrete clusters, where each observation or variable is assigned to only one cluster. Limitations of clustering were observed when the method, cluster-of-cluster assignments, was applied to TCGA pan-cancer multi-omics data of 3527 specimens from 12 cancer type sources [<xref rid="bbv108-B5" ref-type="bibr">5</xref>]. Tumors were assigned to one cluster, and these clusters grouped largely by anatomical origin and failed to identify clusters associated with known cancer pathways [<xref rid="bbv108-B5" ref-type="bibr">5</xref>]. However, a dimension reduction analysis across 10 different cancers, identified novel and known cancer-specific pathways, in addition to pathways such as cell cycle, mitochondria, gender, interferon response and immune response that were common among different cancers [<xref rid="bbv108-B4" ref-type="bibr">4</xref>].</p><p>Overlapping clusters have been identified in many tumors including glioblastoma and serous ovarian cancer [<xref rid="bbv108-B6" ref-type="bibr">6</xref>, <xref rid="bbv108-B7" ref-type="bibr">7</xref>]. Gusenleitner and colleagues [<xref rid="bbv108-B8" ref-type="bibr">8</xref>] found that k-means or hierarchical clustering failed to identify the correct cluster structure in simulated data with multiple overlapping clusters. Clustering methods may also falsely discover clusters in unimodal data. For example, Senbabao&#x0011f;lu <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B7" ref-type="bibr">7</xref>] applied consensus clustering to randomly generated unimodal data and found it divided the data into apparently stable clusters for a range of K, where K is a predefined number of clusters. However, principal component analysis (PCA) did not identify these clusters.</p><p>In this article, we first introduce linear dimension reduction of a single data set, describing the fundamental concepts and terminology that are needed to understand its extensions to multiple matrices. Then we review multivariate dimension reduction approaches, which can be applied to the integrative exploratory analysis of multi-omics data. To demonstrate the application of these methods, we apply multiple co-inertia analysis (MCIA) to EDA of mRNA, miRNA and proteomics data of a subset of 60 cell lines studied at the National Cancer Institute (NCI-60).</p></sec><sec><title>Introduction to dimension reduction</title><p>Dimension reduction methods arose in the early 20th century [<xref rid="bbv108-B9" ref-type="bibr">9</xref>, <xref rid="bbv108-B10" ref-type="bibr">10</xref>] and have continued to evolve, often independently in multiple fields, giving rise to a myriad of associated terminology. Wikipedia lists over 10 different names for PCA, the most widely used dimension reduction approach. Therefore, we provide a glossary (<xref ref-type="table" rid="bbv108-T1">Table 1</xref>) and tables of methods (<xref ref-type="table" rid="bbv108-T2 bbv108-T3 bbv108-T4">Tables 2&#x02013;4</xref>) to assist beginners to the field. Each of these are dimension reduction techniques, whether they are applied to one (<xref ref-type="table" rid="bbv108-T2">Table 2</xref>) or multiple (<xref ref-type="table" rid="bbv108-T3">Tables 3</xref> and <xref ref-type="table" rid="bbv108-T4">4</xref>) data sets. We start by introducing the central concepts of dimension reduction.
<table-wrap id="bbv108-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Glossary</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Term</th><th rowspan="1" colspan="1">Definition</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">Variance</td><td rowspan="1" colspan="1">The variance of a random variable measures the spread (variability) of its realizations (values of the random variable). The variance is always a positive number. If the variance is small, the values of the random variable are close to the mean of the random variable (the spread of the data is low). A high variance is equivalent to widely spread values of the random variable. See [<xref rid="bbv108-B11" ref-type="bibr">11</xref>].</td></tr><tr><td rowspan="1" colspan="1">Standard deviation</td><td rowspan="1" colspan="1">The standard deviation of a random variable measures the spread (variability) of its realizations (values of the random variable). It is defined as the square root of the variance. The standard deviation will have the same units as the random variable, in contrast to the variance. See [<xref rid="bbv108-B11" ref-type="bibr">11</xref>].</td></tr><tr><td rowspan="1" colspan="1">Covariance</td><td rowspan="1" colspan="1">The covariance is an unstandardized measure about the tendency of two random variables to vary together. See [<xref rid="bbv108-B12" ref-type="bibr">12</xref>].</td></tr><tr><td rowspan="1" colspan="1">Correlation</td><td rowspan="1" colspan="1">The correlation of two random variables is defined by the covariance of the two random variables normalized by the product between their standard deviations. It measures the linear relationship between the two random variables. The correlation coefficient ranges between &#x02212;1 and +1. See [<xref rid="bbv108-B12" ref-type="bibr">12</xref>].</td></tr><tr><td rowspan="1" colspan="1">Inertia</td><td rowspan="1" colspan="1">Inertia is a measure for the variability of the data. The inertia of a set of points relative to one point P is defined by the weighted sum of the squared distances between each considered point and the point P. Correspondingly, the inertia of a centered matrix (mean is equal to zero) is simply the sum of the squared matrix elements. The inertia of the matrix <bold>X</bold> defined by the metrics <bold>L</bold> and <bold>D</bold> is the weighted sum of its squared values. The inertia is equal the total variance of <bold>X</bold> when <bold>X</bold> is centered, <bold>L</bold> is the Euclidean metric and <bold>D</bold> is a diagonal matrix with diagonal elements equal to 1/<italic>n</italic>. See [<xref rid="bbv108-B13" ref-type="bibr">13</xref>].</td></tr><tr><td rowspan="1" colspan="1">Co-inertia</td><td rowspan="1" colspan="1">The co-inertia is a global measure for the co-variability of two data sets (for example, two high-dimensional random variables). If the data sets are centered, the co-inertia is the sum of squared covariances. When coupling a pair of data sets, the co-inertia between two matrices, <bold>X</bold> and <bold>Y</bold>, is calculated as <italic>trace</italic> (<bold>XLX</bold><sup>T</sup><bold>DYRY</bold><sup>T</sup><bold>D</bold>). See [<xref rid="bbv108-B13" ref-type="bibr">13</xref>].</td></tr><tr><td rowspan="1" colspan="1">Orthogonal</td><td rowspan="1" colspan="1">Two vectors are called orthogonal if they form an angle that measures 90 degrees. Generally, two vectors are orthogonal if their inner product is equal to zero. Two orthogonal vectors are always linearly independent. See [<xref rid="bbv108-B12" ref-type="bibr">12</xref>].</td></tr><tr><td rowspan="1" colspan="1">Independent</td><td rowspan="1" colspan="1">In linear algebra, two vectors are called linearly independent if their liner combination is equal to zero only when all constants of the linear combination are equal to zero. See [<xref rid="bbv108-B14" ref-type="bibr">14</xref>]. In statistics, two random variables are called statistically independent if the distribution of one of them does not affect the distribution of the other. If two independent random variables are added, then the mean of the sum is the sum of the two mean values. This is also true for the variance. The covariance of two independent variables is equal to zero. See [<xref rid="bbv108-B11" ref-type="bibr">11</xref>].</td></tr><tr><td rowspan="1" colspan="1">Eigenvector, eigenvalue</td><td rowspan="1" colspan="1">An eigenvector of a matrix is a vector that does not change its direction after a linear transformation. The vector <inline-formula><mml:math id="MM1"><mml:mi>v</mml:mi></mml:math></inline-formula> is an eigenvector of the matrix <bold>A</bold> if: <inline-formula><mml:math id="MM2"><mml:mrow><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="MM3"><mml:mi>&#x003bb;</mml:mi></mml:math></inline-formula> is the eigenvalue associated with the eigenvector <inline-formula><mml:math id="MM4"><mml:mi>v</mml:mi></mml:math></inline-formula> and it reflects the stretch of the eigenvector following the linear transformation. The most popular way to compute eigenvectors and eigenvalues is the SVD. See [<xref rid="bbv108-B14" ref-type="bibr">14</xref>].</td></tr><tr><td rowspan="1" colspan="1">Linear combination</td><td rowspan="1" colspan="1">Mathematical expression calculated through the multiplication of variables with constants and adding the individual multiplication results. A linear combination of the variables <italic>x</italic> and <italic>y</italic> is <inline-formula><mml:math id="MM5"><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi>b</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> where <italic>a</italic> and <italic>b</italic> are the constants. See [<xref rid="bbv108-B15" ref-type="bibr">15</xref>].</td></tr><tr><td rowspan="1" colspan="1">Omics</td><td rowspan="1" colspan="1">The study of biological molecules in a comprehensive fashion. Examples of omics data types include genomics, transcriptomics, proteomics, metabolomics and epigenomics [<xref rid="bbv108-B16" ref-type="bibr">16</xref>].</td></tr><tr><td rowspan="1" colspan="1">Dimension reduction</td><td rowspan="1" colspan="1">Dimension reduction is the mapping of data to a lower dimensional space such that redundant variance in the data is reduced or discarded, enabling a lower-dimensional representation without significant loss of information. See [<xref rid="bbv108-B17" ref-type="bibr">17</xref>].</td></tr><tr><td rowspan="1" colspan="1">Exploratory data analysis</td><td rowspan="1" colspan="1">EDA is the application of statistical techniques that summarize the main characteristics of data, often with visual methods. In contrast to statistical hypothesis testing (confirmatory data analysis), EDA can help to generate hypotheses. See [<xref rid="bbv108-B18" ref-type="bibr">18</xref>].</td></tr><tr><td rowspan="1" colspan="1">Sparse vector</td><td rowspan="1" colspan="1">A sparse vector is a vector in which most elements are zero. A sparse loadings matrix in PCA or related methods reduce the number of features contributing to a PC. The variables with nonzero entries (features) are the &#x02018;selected features'. See [<xref rid="bbv108-B19" ref-type="bibr">19</xref>].</td></tr></tbody></table></table-wrap>
<table-wrap id="bbv108-T2" orientation="portrait" position="float"><label>Table 2.</label><caption><p>Dimension reduction methods for one data set</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Method</th><th rowspan="1" colspan="1">Description</th><th rowspan="1" colspan="1">Name of R function {R package}</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">PCA</td><td rowspan="1" colspan="1">Principal component analysis</td><td rowspan="1" colspan="1">prcomp{stats}, princomp{stats}, dudi.pca{ade4}, pca{vegan}, PCA{FactoMineR}, principal{psych}</td></tr><tr><td rowspan="1" colspan="1">CA, COA</td><td rowspan="1" colspan="1">Correspondence analysis</td><td rowspan="1" colspan="1">ca{ca}, CA{FactoMineR}, dudi.coa{ade4}</td></tr><tr><td rowspan="1" colspan="1">NSC</td><td rowspan="1" colspan="1">Nonsymmetric correspondence analysis</td><td rowspan="1" colspan="1">dudi.nsc{ade4}</td></tr><tr><td rowspan="1" colspan="1">PCoA, MDS</td><td rowspan="1" colspan="1">Principal co-ordinate analysis/multiple dimensional scaling</td><td rowspan="1" colspan="1">cmdscale{stats} dudi.pco{ade4} pcoa{ape}</td></tr><tr><td rowspan="1" colspan="1">NMF</td><td rowspan="1" colspan="1">Nonnegative matrix factorization</td><td rowspan="1" colspan="1">nmf{nmf}</td></tr><tr><td rowspan="1" colspan="1">nmMDS</td><td rowspan="1" colspan="1">Nonmetric multidimensional scaling</td><td rowspan="1" colspan="1">metaMDS{vegan}</td></tr><tr><td rowspan="1" colspan="1">sPCA, nsPCA, pPCA</td><td rowspan="1" colspan="1">Sparse PCA, nonnegative sparse PCA, penalized PCA. (PCA with feature selection)</td><td rowspan="1" colspan="1">SPC{PMA}, spca{mixOmics}, nsprcomp{nsprcomp}, PMD{PMA}</td></tr><tr><td rowspan="1" colspan="1">NIPALS PCA</td><td rowspan="1" colspan="1">Nonlinear iterative partial least squares analysis (PCA on data with missing values)</td><td rowspan="1" colspan="1">nipals{ade4} pca{pcaMethods}<xref ref-type="table-fn" rid="bbv108-TF1"><sup>a</sup></xref> nipals{mixOmics}</td></tr><tr><td rowspan="1" colspan="1">pPCA, bPCA</td><td rowspan="1" colspan="1">Probabilistic PCA, Bayesian PCA</td><td rowspan="1" colspan="1">pca{pcaMethods}<xref ref-type="table-fn" rid="bbv108-TF1"><sup>a</sup></xref></td></tr><tr><td rowspan="1" colspan="1">MCA</td><td rowspan="1" colspan="1">Multiple correspondence analysis</td><td rowspan="1" colspan="1">dudi.acm{ade4}, mca{MASS}</td></tr><tr><td rowspan="1" colspan="1">ICA</td><td rowspan="1" colspan="1">Independent component analysis</td><td rowspan="1" colspan="1">fastICA{FastICA}</td></tr><tr><td rowspan="1" colspan="1">sIPCA</td><td rowspan="1" colspan="1">Sparse independent PCA (combines sPCA and ICA)</td><td rowspan="1" colspan="1">sipca{mixOmics} ipca{mixOmics}</td></tr><tr><td rowspan="1" colspan="1">plots</td><td rowspan="1" colspan="1">Graphical resources</td><td rowspan="1" colspan="1">R packages including scatterplot3d, ggord<xref ref-type="table-fn" rid="bbv108-TF2"><sup>b</sup></xref>, ggbiplot<xref ref-type="table-fn" rid="bbv108-TF3"><sup>c</sup></xref>, plotly<xref ref-type="table-fn" rid="bbv108-TF4"><sup>d</sup></xref>, explor</td></tr></tbody></table><table-wrap-foot><fn id="bbv108-TF1"><p><sup>a</sup>Available in Bioconductor.</p></fn><fn id="bbv108-TF2"><p><sup>b</sup>On github: devtools::install_github (&#x02018;fawda123/ggord').</p></fn><fn id="bbv108-TF3"><p><sup>c</sup>On github: devtools::install_github (&#x02018;ggbiplot', &#x02018;vqv').</p></fn><fn id="bbv108-TF4"><p><sup>d</sup>On github: devtools::install_github (&#x02018;ropensci/plotly').</p></fn></table-wrap-foot></table-wrap>
<table-wrap id="bbv108-T3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Dimension reduction methods for pairs of data sets</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Method</th><th rowspan="1" colspan="1">Description</th><th rowspan="1" colspan="1">Feature selection</th><th rowspan="1" colspan="1">R Function {package}</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">CCA<xref ref-type="table-fn" rid="bbv108-TF5"><sup>a</sup></xref></td><td rowspan="1" colspan="1">Canonical correlation analysis. Limited to <italic>n</italic>&#x02009;&#x0003e;&#x02009;p<xref ref-type="table-fn" rid="bbv108-TF5"><sup>a</sup></xref></td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">cc{cca} CCorA{vegan},</td></tr><tr><td rowspan="1" colspan="1">CCA<xref ref-type="table-fn" rid="bbv108-TF5"><sup>a</sup></xref></td><td rowspan="1" colspan="1">Canonical correspondence analysis is a constrained correspondence analysis, which is popular in ecology<xref ref-type="table-fn" rid="bbv108-TF5"><sup>a</sup></xref></td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">cca{ade4} cca{vegan} cancor{stats}</td></tr><tr><td rowspan="1" colspan="1">RDA</td><td rowspan="1" colspan="1">Redundancy analysis is a constrained PCA. Popular in ecology</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">rda{vegan}</td></tr><tr><td rowspan="1" colspan="1">Procrutes</td><td rowspan="1" colspan="1">Procrutes rotation rotates a matrix to maximum similarity with a target matrix minimizing sum of squared differences</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">procrustes{vegan} procuste{ade4}</td></tr><tr><td rowspan="1" colspan="1">rCCA</td><td rowspan="1" colspan="1">Regularized canonical correlation</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">rcc{cca}</td></tr><tr><td rowspan="1" colspan="1">sCCA</td><td rowspan="1" colspan="1">Sparse CCA</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">CCA{pma}</td></tr><tr><td rowspan="1" colspan="1">pCCA</td><td rowspan="1" colspan="1">Penalized CCA</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">spCCA{spCCA} supervised version</td></tr><tr><td rowspan="1" colspan="1">WAPLS</td><td rowspan="1" colspan="1">Weighted averaging PLS regression</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">WAPLS{rioja}, wapls{paltran}</td></tr><tr><td rowspan="1" colspan="1">PLS</td><td rowspan="1" colspan="1">Partial least squares of K-tables (multi-block PLS)</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">mbpls{ade4}, plsda{caret}</td></tr><tr><td rowspan="1" colspan="1">sPLS pPLS</td><td rowspan="1" colspan="1">Sparse PLS Penalized PLS</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">spls{spls} spls{mixOmics} ppls{ppls}</td></tr><tr><td rowspan="1" colspan="1">sPLS-DA</td><td rowspan="1" colspan="1">Sparse PLS-discriminant analysis</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">splsda{mixOmics}, splsda{caret}</td></tr><tr><td rowspan="1" colspan="1">cPCA</td><td rowspan="1" colspan="1">Consensus PCA</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">cpca{mogsa}</td></tr><tr><td rowspan="1" colspan="1">CIA</td><td rowspan="1" colspan="1">Coinertia analysis</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">coinertia{ade4} cia{made4}</td></tr></tbody></table><table-wrap-foot><fn id="bbv108-TF5"><p><sup>a</sup>A source for confusion, CCA is widely used as an acronym for both Canonical &#x02018;Correspondence' Analysis and Canonical &#x02018;Correlation' Analysis. Throughout this article we use CCA for canonical &#x02018;correlation' analysis. Both methods search for the multivariate relationships between two data sets. Canonical &#x02018;correspondence' analysis is an extension and constrained form of &#x02018;correspondence' analysis [<xref rid="bbv108-B23" ref-type="bibr">22</xref>]. Both canonical &#x02018;correlation' analysis and RDA assume a linear model; however, RDA is a constrained PCA (and assumes one matrix is the dependent variable and one independent), whereas canonical correlation analysis considers both equally. See [<xref rid="bbv108-B23" ref-type="bibr">23</xref>] for more explanation.</p></fn></table-wrap-foot></table-wrap>
<table-wrap id="bbv108-T4" orientation="portrait" position="float"><label>Table 4.</label><caption><p>Dimension reduction methods for multiple (more than two) data sets</p></caption><table frame="hsides" rules="groups"><thead align="left"><tr><th rowspan="1" colspan="1">Method</th><th rowspan="1" colspan="1">Description</th><th rowspan="1" colspan="1">Feature selection</th><th rowspan="1" colspan="1">Matched cases</th><th rowspan="1" colspan="1">R Function {package}</th></tr></thead><tbody align="left"><tr><td rowspan="1" colspan="1">MCIA</td><td rowspan="1" colspan="1">Multiple coinertia analysis</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">mcia{omicade4}, mcoa{ade4}</td></tr><tr><td rowspan="1" colspan="1">gCCA</td><td rowspan="1" colspan="1">Generalized CCA</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">regCCA{dmt}</td></tr><tr><td rowspan="1" colspan="1">rGCCA</td><td rowspan="1" colspan="1">Regularized generalized CCA</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">regCCA{dmt} rgcca{rgcca} wrapper.rgcca{mixOmics}</td></tr><tr><td rowspan="1" colspan="1">sGCCA</td><td rowspan="1" colspan="1">Sparse generalized canonical correlation analysis</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">sgcca{rgcca} wrapper.sgcca{mixOmics}</td></tr><tr><td rowspan="1" colspan="1">STATIS</td><td rowspan="1" colspan="1">Structuration des Tableaux &#x000e1; Trois Indices de la Statistique (STATIS). Family of methods which include X-statis</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">statis{ade4}</td></tr><tr><td rowspan="1" colspan="1">CANDECOMP/ PARAFAC / Tucker3</td><td rowspan="1" colspan="1">Higher order generalizations of SVD and PCA. Require matched variables and cases.</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">CP{ThreeWay}, T3{ThreeWay}, PCAn{PTaK}, CANDPARA{PTaK}</td></tr><tr><td rowspan="1" colspan="1">PTA</td><td rowspan="1" colspan="1">Partial triadic analysis</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">pta{ade4},</td></tr><tr><td rowspan="1" colspan="1">statico</td><td rowspan="1" colspan="1">Statis and CIA (find structure between two pairs of K-tables)</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">statico{ade4}</td></tr></tbody></table></table-wrap></p><p>We denote matrices with boldface uppercase letters. The rows of a matrix contain the observations, while the columns hold the variables. In an omics study, the variables (also referred to as features) generally measure tissue or cell attributes including abundance of mRNAs, proteins and metabolites. All vectors are columns vectors and are denoted with boldface lowercase letters. Scalars are indicated by italic letters.</p><p>Given an omics data set, <bold>X</bold>, which is an<italic> n&#x000d7;p</italic> matrix, of<italic> n</italic> observations and <italic>p</italic> variables, it can be represented by:
<disp-formula id="bbv108-M1"><label>(1)</label><mml:math id="MM6"><mml:mrow><mml:mtext mathvariant="bold">X</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi mathvariant="normal">1</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi mathvariant="normal">2</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>...</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mtext mathvariant="bold">x</mml:mtext><mml:mi mathvariant="normal">p</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <bold>x</bold> are vectors of length <italic>n</italic>, and are measurements of mRNA or other biological variables for <italic>n</italic> observations (samples). In a typical omics study, <italic>p</italic> ranges from several hundred to millions. Therefore, observations (samples) are represented in large dimensional spaces &#x0211d;<sup>p</sup>. The goal of dimension reduction is to identify a (set of) new variable(s) using a linear combination of the original variables, such that the number of new variables is much smaller than <italic>p</italic>. An example of such a linear combination is shown in <xref ref-type="disp-formula" rid="bbv108-M2">Equation (2)</xref>;
<disp-formula id="bbv108-M2"><label>(2)</label><mml:math id="MM7"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>f</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>2</mml:mn></mml:mstyle></mml:msub><mml:mo>+</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>p</mml:mi></mml:mstyle></mml:msub></mml:mrow></mml:math></disp-formula>
</p><p>or expressed in a matrix form:
<disp-formula id="bbv108-M3"><label>(3)</label><mml:math id="MM8"><mml:mrow><mml:mtext mathvariant="bold">f</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">Xq</mml:mtext></mml:mrow></mml:math></disp-formula>
</p><p>In <xref ref-type="disp-formula" rid="bbv108-M2">Equations (2)</xref> and <xref ref-type="disp-formula" rid="bbv108-M3">(3)</xref>, <bold>f</bold> is a new variable, which is often called a latent variable or a component. Depending on the scientific field, <bold>f</bold> may also be called principal axis, eigenvector or latent factor. <bold>q</bold>=(<italic>q<sub>1</sub></italic>, <italic>q<sub>2</sub></italic>,&#x02026;, <italic>q<sub>p</sub></italic>)<sup>T</sup> is a <italic>p</italic>-length vector of coefficients of scalar values in which at least one of the coefficients is different from zero. These coefficients are also called &#x02018;loadings'. Dimension reduction analysis introduces constraints to obtain a meaningful solution; we find the set of <bold>q</bold>&#x02019;s that maximize the variance of components <bold>f</bold>&#x02019;s. In doing so, a smaller number of variables, <bold>f</bold>, capture most of the variance in the data. Different optimization and constraint criteria distinguish between different dimension reduction methods. <xref ref-type="table" rid="bbv108-T2">Table 2</xref> provides a nonexhaustive list of these methods, which includes PCA, linear discriminant analysis and factor analysis.</p></sec><sec><title>Principal component analysis</title><p>PCA is one of the most widely used dimension reduction methods [<xref rid="bbv108-B20" ref-type="bibr">20</xref>]. Given a column centered and scaled (unit variance) matrix <bold>X</bold>, PCA finds a set of new variables <bold>f</bold><sup>i</sup>=<bold>Xq</bold><sup>i</sup> where <italic>i</italic> is the <italic>i</italic>th component and <bold>q</bold><sup>i</sup> is the variable loading for the <italic>i</italic>th principal component (PC; superscript denotes the component or the dimension). The variance of <bold>f</bold><sup>i</sup> is maximized, that is:
<disp-formula id="bbv108-M4"><label>(4)</label><mml:math id="MM9"><mml:mrow><mml:msub><mml:mrow><mml:mtext>arg&#x000a0;max</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:msub><mml:mtext>&#x000a0;var(</mml:mtext><mml:mi mathvariant="bold">X</mml:mi><mml:msup><mml:mi mathvariant="bold">q</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
with the constraints that ||<bold>q</bold><sup>i</sup>|| = 1 and each pair of components (<bold>f</bold><sup>i</sup>,<bold>f</bold><sup>j</sup>) are orthogonal to each other (or uncorrelated, i.e. <bold>f</bold><sup>iT</sup><bold>f</bold><sup>j</sup> = 0 for j &#x02260; i).</p><p>PCA can be computed using different algorithms including eigen analysis, latent variable analysis, factor analysis, singular value decomposition (SVD) [<xref rid="bbv108-B21" ref-type="bibr">21</xref>] or linear regression [<xref rid="bbv108-B3" ref-type="bibr">3</xref>]. Among them, SVD is the most widely used approach. Given <bold>X</bold>, an <italic>n&#x000d7;p</italic> matrix, with rank r (r&#x02009;&#x02009;min[n,&#x02009;p]), SVD decomposes <bold>X</bold> into three matrices:
<disp-formula id="bbv108-M5"><label>(5)</label><mml:math id="MM10"><mml:mrow><mml:mtext mathvariant="bold">X</mml:mtext><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">USQ</mml:mtext></mml:mrow><mml:mtext>T</mml:mtext></mml:msup><mml:msup><mml:mrow><mml:mi mathvariant="normal">subject to the constraint that</mml:mi><mml:mtext mathvariant="bold">U</mml:mtext></mml:mrow><mml:mtext>T</mml:mtext></mml:msup><mml:mtext mathvariant="bold">U</mml:mtext><mml:mo>=</mml:mo><mml:msup><mml:mtext mathvariant="bold">Q</mml:mtext><mml:mi mathvariant="normal">T</mml:mi></mml:msup><mml:mtext mathvariant="bold">Q</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">I</mml:mtext></mml:mrow></mml:math></disp-formula>
where <bold>U</bold> is an <italic>n&#x000d7;r </italic>matrix and <bold>Q</bold> is a <italic>p&#x000d7;r</italic> matrix. The columns of <bold>U</bold> and <bold>Q</bold> are the orthogonal left and right singular vectors, respectively. <bold>S</bold> is an <italic>r&#x000d7;r </italic>diagonal matrix of singular values, which are proportional to the standard deviations associated with <italic>r </italic>singular vectors. The singular vectors are ordered such that their associated variances are monotonically decreasing. In a PCA of <bold>X</bold>, the PCs comprise an <italic>n&#x000d7;r</italic> matrix, <bold>F</bold>, which is defined as:
<disp-formula id="bbv108-M6"><label>(6)</label><mml:math id="MM11"><mml:mrow><mml:mtext mathvariant="bold">F</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">US</mml:mtext><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mtext mathvariant="bold">USQ</mml:mtext></mml:mrow><mml:mtext>T</mml:mtext></mml:msup><mml:mtext mathvariant="bold">Q</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">XQ</mml:mtext></mml:mrow></mml:math></disp-formula>
where the columns of matrix <bold>F</bold> are the PCs and the matrix <bold>Q</bold> is called the loadings matrix and contains the linear combination coefficients of the variables for each PC (<bold>q</bold> in <xref ref-type="disp-formula" rid="bbv108-M3">Equation (3)</xref>). Therefore, we represent the variance of <bold>X</bold> in lower dimension <italic>r</italic>. The above formula also emphasizes that <bold>Q</bold> is a matrix that projects the observations in <bold>X</bold> onto the PCs. The sum of squared values of the columns in <bold>U</bold> equals 1 (<xref ref-type="disp-formula" rid="bbv108-M5">Equation (5)</xref>), therefore, the variance of the <italic>i</italic>th PC, <inline-formula><mml:math id="MM12"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, can be calculated as
<disp-formula id="bbv108-M7"><label>(7)</label><mml:math id="MM13"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
where <italic>s<sup>i</sup></italic> is the <italic>i</italic>th diagonal element in <bold>S</bold>. The variance reflects the amount of information (underling structure) captured by each PC. The squared correlations between PCs and the original variables are informative and often illustrated using a correlation circle plot. These can be calculated by:
<disp-formula id="bbv108-M8"><label>(8)</label><mml:math id="MM14"><mml:mrow><mml:mtext mathvariant="bold">C</mml:mtext><mml:mo>=</mml:mo><mml:mtext mathvariant="bold">QD</mml:mtext><mml:mo>&#x02009;</mml:mo></mml:mrow></mml:math></disp-formula>
where <bold>D</bold> = (d<sup>1</sup>, d<sup>2</sup>,&#x02026;, d<sup>r</sup>)<sup>T</sup> is a diagonal matrix of the standard deviation of the PCs.</p><p>In contrast to SVD, which calculates all PCs simultaneously, PCA can also be calculated using the Nonlinear Iterative Partial Least Squares (NIPALS) algorithm, which uses an iterative regression procedure to calculate PCs. Computation can be performed on data with missing values and it is faster than SVD when applied to large matrices. Furthermore, NIPALS may be generalized to discover the correlated structure in more than one data set (see sections on the analysis of multi-omics data sets). Please refer to the <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/lookup/suppl/doi:10.1093/bib/bbv108/-/DC1">Supplementary Information</ext-link> for additional details on NIPALS.</p></sec><sec><title>Visualizing and interpreting results of dimension reduction analysis</title><p>We present an example to illustrate how to interpret results of a PCA. PCA was applied to analyze mRNA gene expression data of a subset of cell lines from the NCI-60 panel; those of melanoma, leukemia and central nervous system (CNS) tumors. The results of PCA can be easily interpreted by visualizing the observations and variables in the same space using a biplot. <xref ref-type="fig" rid="bbv108-F1">Figure 1</xref>A is a biplot of the first (PC1) and second PC (PC2), where points and arrows from the plot origin, represent observations and genes, respectively. Cell lines (points) with correlated gene expression profiles have similar scores and are projected close to each other on PC1 and PC2. We see that cell lines from the same anatomical location are clustered.
<fig id="bbv108-F1" orientation="portrait" position="float"><label>Figure 1.</label><caption><p>Results of a PCA analysis of mRNA gene expression data of melanoma (ME), leukemia (LE) and central nervous system (CNS) cell lines from the NCI-60 cell line panel. All variables were centered and scaled. Results show (<bold>A</bold>) a biplot where observations (cell lines) are points and gene expression profiles are arrows; (<bold>B</bold>) a heatmap showing the gene expression of the same 20 genes in the cell lines; red to blue scale represent high to low gene expression (light to dark gray represent high to low gene expression on the black and white figure); (<bold>C</bold>) correlation circle; (<bold>D</bold>) variance barplot of the first ten PCs. To improve the readability of the biplot, some labels of the variables (genes) in (<bold>A</bold>) have been moved slightly. A colour version of this figure is available online at BIB online: <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org">http://bib.oxfordjournals.org</ext-link>.</p></caption><graphic xlink:href="bbv108f1p"/></fig>
</p><p>Both the direction and length of the mRNA gene expression vectors can be interpreted. Gene expression vectors point in the direction of the latent variable (PC) to which it is most similar (squared correlation). Gene vectors with the same direction (e.g. FAM69B, PNPLA2, PODXL2) have similar gene expression profiles. The length of the gene expression vector is proportional to the squared multiple correlation between the fitted values for the variable and the variable itself.</p><p>A gene expression vector and a cell line projected in the same direction from the origin are positively associated. For example in <xref ref-type="fig" rid="bbv108-F1">Figure 1</xref>A, FAM69B, PNPLA2, PODXL2 are active (have higher gene expression) in melanoma cell lines. Similarly, genes DPPA5 and RPL34P1 are among those genes that are highly expressed in most leukemia cell lines. By contrast, genes WASL and HEXB point in the opposite direction to most leukemia cell lines indicating low association. In <xref ref-type="fig" rid="bbv108-F1">Figure 1</xref>B it is clear that these genes are not expressed in leukemia cell lines and are colored blue in the heatmap (these are colored dark gray in grayscale).</p><p>The sum of the squared correlation coefficients between a variable and all the components (calculated in equation 8) equals 1. Therefore, variables are often shown within a correlation circle (<xref ref-type="fig" rid="bbv108-F1">Figure 1</xref>C). Variables positioned on the unit circle represent variables that are perfectly represented by the two dimensions displayed. Those not on the unit circle may require additional components to be represented.</p><p>In most analyses, only the first few PCs are plotted and studied, as these explain the most variant trends in the data. Generally, the selection of components is subjective and depends on the purpose of the EDA. An informal elbow test may help to determine the number of PCs to retain and examine [<xref rid="bbv108-B24" ref-type="bibr">24</xref>, <xref rid="bbv108-B25" ref-type="bibr">25</xref>]. From the scree plot of PC eigenvalues in <xref ref-type="fig" rid="bbv108-F1">Figure 1</xref>D, we might decide to examine the first three PCs because the decrease in PC variance becomes relatively moderate after PC3. Another approach that is widely used is to include (or retain) PCs that cumulatively capture a certain proportion of variance; for example, 70% of variance is modeled with three PCs. If a parsimony model is preferred, the variance proportion cutoff can be as low as 50% [<xref rid="bbv108-B24" ref-type="bibr">24</xref>]. More formal tests, including the Q<sup>2</sup> statistic, are also available (for details, see [<xref rid="bbv108-B24" ref-type="bibr">24</xref>]). In practice, the first component might explain most of the variance and the remaining axes may simply be attributed to noise from technical or biological sources in a study with low complexity (e.g. cell line replicates of controls and one treatment condition). However, a complex data set (for example, a set of heterogeneous tumors) may require multiple PCs to capture the same amount of variance.</p></sec><sec><title>Different dimension reduction approaches are optimized for different data</title><p>There are many dimension reduction approaches related to PCA (<xref ref-type="table" rid="bbv108-T2">Table 2</xref>), including principal co-ordinate analysis (PCoA), correspondence analysis (CA) and nonsymmetrical correspondence analysis (NSCA). These may be computed by SVD, but differ in how the data are transformed before decomposition [<xref rid="bbv108-B21" ref-type="bibr">21</xref>, <xref rid="bbv108-B26" ref-type="bibr">26</xref>, <xref rid="bbv108-B27" ref-type="bibr">27</xref>], and therefore, each is optimized for specific data properties. PCoA (also known as Classical Multidimensional Scaling) is versatile, as it is a SVD of a distance matrix that can be applied to decompose distance matrices of binary, count or continuous data. It is frequently applied in the analysis of microbiome data [<xref rid="bbv108-B28" ref-type="bibr">28</xref>].</p><p>PCA is designed for the analysis of multi-normal distributed data. If data are strongly skewed or extreme outliers are present, the first few axes may only separate those objects with extreme values instead of displaying the main axes of variation. If data are unimodal or display nonlinear trends, one may see distortions or artifacts in the resulting plots, in which the second axis is an arched function of the first axis. In PCA, this is called the horseshoe effect and it is well described, including illustrations, in Legendre and Legendre [<xref rid="bbv108-B3" ref-type="bibr">3</xref>]. Both nonmetric Multi-Dimensional Scaling (MDS) and CA perform better than PCA in these cases [<xref rid="bbv108-B26" ref-type="bibr">26</xref>, <xref rid="bbv108-B29" ref-type="bibr">29</xref>]. Unlike PCA, CA can be applied to sparse count data with many zeros. Although designed for contingency tables of nonnegative count data, CA and NSCA, decompose a chi-squared matrix [<xref rid="bbv108-B30" ref-type="bibr">30</xref>, <xref rid="bbv108-B31" ref-type="bibr">31</xref>], but have been successfully applied to continuous data including gene expression and protein profiles [<xref rid="bbv108-B32" ref-type="bibr">32</xref>, <xref rid="bbv108-B33" ref-type="bibr">33</xref>]. As described by Fellenberg <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B33" ref-type="bibr">33</xref>], gene and protein expression can be seen as an approximation of the number of corresponding molecules present in the cell during a certain measured condition. Additionally, Greenacre [<xref rid="bbv108-B27" ref-type="bibr">27</xref>] emphasized that the descriptive nature of CA and NSCA allows their application on data tables in general, not only on count data. These two arguments support the suitability of CA and NSCA as analysis methods for omics data. While CA investigates symmetric associations between two variables, NSCA captures asymmetric relations between variables. Spectral map analysis is related to CA, and performs comparably with CA, each outperforming PCA in the identification of clusters of leukemia gene expression profiles [<xref rid="bbv108-B26" ref-type="bibr">26</xref>]. All dimension reduction methods can be formulated in terms of the duality diagram. Details on this powerful framework are included in the <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/lookup/suppl/doi:10.1093/bib/bbv108/-/DC1">Supplementary Information</ext-link>.</p><p>Nonnegative matrix factorization (NMF) [<xref rid="bbv108-B34" ref-type="bibr">34</xref>] forces a positive or nonnegative constraint on the resulting data matrices and, similar to Independent Component Analysis (ICA) [<xref rid="bbv108-B35" ref-type="bibr">35</xref>], there is no requirement for orthogonality or independence in the components. The nonnegative constraint guarantees that only the additive combinations of latent variables are allowed. This may be more intuitive in biology where many biological measurements (e.g. protein concentrations, count data) are represented by positive values. NMF is described in more detail in the <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/lookup/suppl/doi:10.1093/bib/bbv108/-/DC1">Supplemental Information</ext-link>. ICA was recently applied to molecular subtype discovery in bladder cancer [<xref rid="bbv108-B4" ref-type="bibr">4</xref>]. Biton <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B4" ref-type="bibr">4</xref>] applied ICA to gene expression data of 198 bladder cancers and examined 20 components. ICA successfully decomposed and extracted multiple layers of signal from the data. The first two components were associated with batch effects but other components revealed new biology about tumor cells and the tumor microenvironment. They also applied ICA to non-bladder cancers and, by comparing the correlation between components, were able to identify a set of bladder cancer-specific components and their associated genes.</p><p>As omics data sets tend to have high dimensionality (<italic>p</italic> &#x0226b; n) it is often useful to reduce the number of variables. Several recent extensions of PCA include variable selection, often via a regularization step or L-1 penalization (e.g. Least Absolute Shrinkage and Selection Operator, LASSO) [<xref rid="bbv108-B36" ref-type="bibr">36</xref>]. The NIPALS algorithm uses an iterative regression approach to calculate the components and loadings, which is easily extended to have a sparse operator that can be included during regression on the component [<xref rid="bbv108-B37" ref-type="bibr">37</xref>]. A cross-validation approach can be used to determine the level of sparsity. Sparse, penalized and regularized extensions of PCA and related methods have been described recently [<xref rid="bbv108-B36" ref-type="bibr">36</xref>, <xref rid="bbv108-B38" ref-type="bibr">38&#x02013;41</xref>].</p></sec><sec><title>Integrative analysis of two data sets</title><p>One-table dimension reduction methods have been extended to the EDA of two matrices and can simultaneously decompose and integrate a pair of matrices that measure different variables on the same observations (<xref ref-type="table" rid="bbv108-T3">Table 3</xref>). Methods include generalized SVD [<xref rid="bbv108-B42" ref-type="bibr">42</xref>], Co-Inertia Analysis (CIA) [<xref rid="bbv108-B43" ref-type="bibr">43</xref>, <xref rid="bbv108-B44" ref-type="bibr">44</xref>], sparse or penalized extensions of Partial Least Squares (PLS), Canonical Correspondence analysis (CCA) and Canonical Correlation Analysis (CCA) [<xref rid="bbv108-B36" ref-type="bibr">36</xref>, <xref rid="bbv108-B45" ref-type="bibr">45&#x02013;47</xref>]. Note both canonical correspondence analysis and canonical correlation analysis are referred to by the acronym CCA. Canonical correspondence analysis is a constrained form of CA that is widely used in ecological statistics [<xref rid="bbv108-B46" ref-type="bibr">46</xref>]; however, it is yet to be adopted by the genomics community in analysis of pairs of omics data. By contrast, several groups have applied extensions of canonical correlation analysis to omics data integration. Therefore, in this review, we use CCA to describe canonical correlation analysis.</p></sec><sec><title>Canonical correlation analysis</title><p>Two omics data sets <bold>X</bold> (dimension <italic>n&#x000d7;p<sub>x</sub></italic>) and <bold>Y</bold> (dimension <italic>n&#x000d7;p<sub>y</sub></italic>) can be expressed by the following latent component decomposition problem:
<disp-formula id="bbv108-M9"><label>(9)</label><mml:math id="MM15"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="bold">X</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>&#x02009;</mml:mo><mml:mtext>=&#x000a0;</mml:mtext><mml:msub><mml:mi mathvariant="bold">F</mml:mi><mml:mi mathvariant="bold">y</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">y</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi mathvariant="bold">y</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <bold>F<sub>x</sub></bold> and <bold>F<sub>y</sub></bold> are <italic>n&#x000d7;r</italic> matrices, with <italic>r</italic> columns of components that explain the co-structure between <bold>X</bold> and <bold>Y</bold>. The columns of <bold>Q<sub>x</sub></bold> and <bold>Q<sub>y</sub></bold> are variable loading vectors for <bold>X</bold> and <bold>Y</bold>, respectively. <bold>E<sub>x</sub></bold> and <bold>E<sub>y</sub></bold> are error terms.</p><p>Proposed by Hotelling in 1936 [<xref rid="bbv108-B47" ref-type="bibr">47</xref>], CCA searches for associations or correlations among the variables of <bold>X</bold> and <bold>Y</bold> [<xref rid="bbv108-B47" ref-type="bibr">47</xref>], by maximizing the correlation between <bold>Xq<sub>x</sub></bold><sup>i</sup> and<bold> Yq<sub>y</sub></bold><sup>i</sup>:
<disp-formula id="bbv108-M10"><label>(10)</label><mml:math id="MM16"><mml:mrow><mml:mtext>for&#x000a0;the</mml:mtext><mml:mi>i</mml:mi><mml:mtext>th&#x000a0;component</mml:mtext><mml:mo>:</mml:mo><mml:mi>arg</mml:mi><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">q</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mtext>i</mml:mtext></mml:msubsup><mml:msubsup><mml:mi mathvariant="bold">q</mml:mi><mml:mi mathvariant="bold">y</mml:mi><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo>&#x02009;</mml:mo></mml:mrow></mml:msub><mml:mtext>cor(</mml:mtext><mml:mi mathvariant="bold">X</mml:mi><mml:msubsup><mml:mi mathvariant="bold">q</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo>,</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:msubsup><mml:mi mathvariant="bold">q</mml:mi><mml:mi mathvariant="bold">y</mml:mi><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p>In CCA, the components <bold>Xq<sub>x</sub></bold><sup>i</sup> and <bold>Yq<sub>y</sub></bold><sup>i</sup> are called canonical variates and their correlations are the canonical correlations.</p></sec><sec><title>Sparse canonical correlation analysis</title><p>The main limitation of applying CCA to omics data is that it requires an inversion of the correlation or covariance matrix [<xref rid="bbv108-B38" ref-type="bibr">38</xref>, <xref rid="bbv108-B49" ref-type="bibr">49</xref>, <xref rid="bbv108-B50" ref-type="bibr">50</xref>], which cannot be calculated when the number of variables exceeds the number of observations [<xref rid="bbv108-B46" ref-type="bibr">46</xref>]. In high-dimensional omics data where <italic>p</italic> &#x0226b; <italic>n</italic>, application of these methods requires a regularization step. This may be accomplished by adding a ridge penalty, that is, adding a multiple of the identity matrix to the covariance matrix [<xref rid="bbv108-B51" ref-type="bibr">51</xref>]. A sparse solution of the loading vectors (<bold>Q<sub>x</sub></bold> and <bold>Q<sub>y</sub></bold>) filters the number of variables and simplifies the interpretation of results. For this purpose, penalized CCA [<xref rid="bbv108-B52" ref-type="bibr">52</xref>], sparse CCA [<xref rid="bbv108-B53" ref-type="bibr">53</xref>], CCA-l1 [<xref rid="bbv108-B54" ref-type="bibr">54</xref>], CCA elastic net (CCA-EN) [<xref rid="bbv108-B45" ref-type="bibr">45</xref>] and CCA-group sparse [<xref rid="bbv108-B55" ref-type="bibr">55</xref>] have been introduced and applied to the integrative analysis of two omics data sets. Witten <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B36" ref-type="bibr">36</xref>] provided an elegant comparison of various CCA extensions accompanied by a unified approach to compute both penalized CCA and sparse PCA. In addition, Witten and Tibshirani [<xref rid="bbv108-B54" ref-type="bibr">54</xref>] extended sparse CCA into a supervised framework, which allows the integration of two data sets with a quantitative phenotype; for example, selecting variables from both genomics and transcriptomics data and linking them to drug sensitivity data.</p><sec><title>Partial least squares analysis</title><p>PLS is an efficient dimension reduction method in the analysis of high-dimensional omics data. PLS maximizes the covariance rather than the correlation between components, making it more resistant to outliers than CCA. Additionally, PLS does not suffer from the <italic>p</italic> &#x0226b; <italic>n</italic> problem as CCA does. Nonetheless, a sparse solution is desired in some cases. For example, Le Cao <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B56" ref-type="bibr">56</xref>] proposed a sparse PLS method for the feature selection by introducing a LASSO penalty for the loading vectors. In a recent comparison, sPLS performed similarly to sparse CCA [<xref rid="bbv108-B45" ref-type="bibr">45</xref>]. There are many implementations of PLS, which optimize different objective functions with different constraints, several of which are described by Boulesteix <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B57" ref-type="bibr">57</xref>].</p></sec><sec><title>Co-Inertia analysis</title><p>CIA is a descriptive nonconstrained approach for coupling pairs of data matrices. It was originally proposed to link two ecological tables [<xref rid="bbv108-B13" ref-type="bibr">13</xref>, <xref rid="bbv108-B58" ref-type="bibr">58</xref>], but has been successfully applied in integrative analysis of omics data [<xref rid="bbv108-B32" ref-type="bibr">32</xref>, <xref rid="bbv108-B43" ref-type="bibr">43</xref>]. CIA is implemented and formulated under the duality diagram framework (<ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/lookup/suppl/doi:10.1093/bib/bbv108/-/DC1">Supplementary Information</ext-link>). CIA is performed in two steps: (i) application of a dimension reduction technique such as PCA, CA or NSCA to the initial data sets depending on the type of data (binary, categorical, discrete counts or continuous data) and (ii) constraining the projections of the orthogonal axes such that they are maximally covariant [<xref rid="bbv108-B43" ref-type="bibr">43</xref>, <xref rid="bbv108-B58" ref-type="bibr">58</xref>]. CIA does not require an inversion step of the correlation or covariance matrix; thus, it can be applied to high-dimensional genomics data without regularization or penalization.</p><p>Though closely related to CCA [<xref rid="bbv108-B49" ref-type="bibr">49</xref>], CIA maximizes the squared covariance between the linear combination of the preprocessed matrix, that is,
<disp-formula id="bbv108-M11"><label>(11)</label><mml:math id="MM17"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>for&#x000a0;the</mml:mtext><mml:mi>i</mml:mi><mml:mtext>th&#x000a0;dimension</mml:mtext><mml:mo>:</mml:mo><mml:mi>argma</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mtext>i</mml:mtext></mml:msubsup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mtext>i</mml:mtext></mml:msubsup></mml:mrow></mml:msub><mml:mi>cov</mml:mi><mml:msup><mml:mi>&#x02009;</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mtext>i</mml:mtext></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
</p><p><xref ref-type="disp-formula" rid="bbv108-M11">Equation (11)</xref> can be decomposed as:
<disp-formula id="bbv108-M12"><label>(12)</label><mml:math id="MM18"><mml:mrow><mml:msup><mml:mrow><mml:mtext>cov</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Xq</mml:mtext></mml:mrow><mml:mtext mathvariant="bold">x</mml:mtext></mml:msub><mml:msup><mml:mrow/><mml:mtext>i</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Yq</mml:mtext></mml:mrow><mml:mtext mathvariant="bold">y</mml:mtext></mml:msub><mml:msup><mml:mrow/><mml:mtext>i</mml:mtext></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mtext>cor</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Xq</mml:mtext></mml:mrow><mml:mtext mathvariant="bold">x</mml:mtext></mml:msub><mml:msup><mml:mrow/><mml:mtext>i</mml:mtext></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Yq</mml:mtext></mml:mrow><mml:mtext mathvariant="bold">y</mml:mtext></mml:msub><mml:msup><mml:mrow/><mml:mtext>i</mml:mtext></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mtext>var</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Xq</mml:mtext></mml:mrow><mml:mtext mathvariant="bold">x</mml:mtext></mml:msub><mml:msup><mml:mrow/><mml:mtext>i</mml:mtext></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>&#x000b7;</mml:mo><mml:mtext>var</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="bold">Xq</mml:mtext></mml:mrow><mml:mtext mathvariant="bold">y</mml:mtext></mml:msub><mml:msup><mml:mrow/><mml:mtext>i</mml:mtext></mml:msup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p><p>CIA decomposition of covariance maximizes the variance and the correlation between matrices and, thus is less sensitive to outliers. The relationship between CIA, Procrustes analysis [<xref rid="bbv108-B13" ref-type="bibr">13</xref>] and CCA have been well described [<xref rid="bbv108-B49" ref-type="bibr">49</xref>]. A comparison between sCCA (with elastic net penalty), sPLS and CIA is provided by Le Cao <italic>et&#x000a0;al</italic><italic>.</italic> [<xref rid="bbv108-B45" ref-type="bibr">45</xref>]. In summary, CIA and sPLS both maximize the covariance between eigenvectors and efficiently identify joint and individual variance in paired data. In contrast, CCA-EN maximizes the correlation between eigenvectors and will discover effects present in both data sets, but may fail to discover strong individual effects [<xref rid="bbv108-B45" ref-type="bibr">45</xref>]. Both sCCA and sPLS are sparse methods that select similar subsets of variables, whereas CIA does not include a feature selection step; thus, in terms of feature selection, results of CIA are more likely to contain redundant information in comparison with sparse methods [<xref rid="bbv108-B45" ref-type="bibr">45</xref>].</p><p>Similar to classical dimension reduction approaches, the number of dimensions to be examined needs to be considered and can be visualized using a scree plot (similar to <xref ref-type="fig" rid="bbv108-F1">Figure 1</xref>D). Components may be evaluated by their associated variance [<xref rid="bbv108-B25" ref-type="bibr">25</xref>], the elbow test or Q<sup>2</sup> statistics, as described previously. For example, the Q<sup>2</sup> statistic was applied to select the number of dimensions in the predictive mode of PLS [<xref rid="bbv108-B56" ref-type="bibr">56</xref>]. In addition, when a sparse factor is introduced in the loading vectors, cross-validation approaches may be used to determine the number of variables selected from each pair of components. Selection of the number of components and optimization of these parameters is still an open research question and is an active area of research.</p></sec></sec><sec><title>Integrative analysis of multi-assay data</title><p>There is a growing need to integrate more than two data sets in genomics. Generalizations of dimension reduction methods to three or more data sets are sometimes called <italic>K</italic>-table methods [<xref rid="bbv108-B59" ref-type="bibr">59&#x02013;61</xref>], and a number of them have been applied to multi-assay data (<xref ref-type="table" rid="bbv108-T4">Table 4</xref>). Simultaneous decomposition and integration of multiple matrices is more complex than an analysis of a single data set or paired data because each data set may have different numbers of variables, scales or internal structure and thus have different variance. This might produce global scores that are dominated by one or a few data sets. Therefore, data are preprocessed before decomposition. Preprocessing is often performed on two levels. On the variable levels, variables are often centered and normalized so that their sum of squared values or variance equals 1. This procedure enables all the variables to have equal contribution to the total inertia (sum of squares of all elements) of a data set. However, the number of variables may vary between data sets, or filtering/preprocessing steps may generate data sets that have a higher variance contribution to the final result. Therefore, a data set level normalization is also required. In the simplest <italic>K</italic>-table analysis, all matrices have equal weights. More commonly, data sets are weighted by their expected contribution or expected data quality, for example, by the square root of their total inertia or by the square root of the numbers of columns of each data set [<xref rid="bbv108-B62" ref-type="bibr">62</xref>]. Alternatively, greater weights can be given to smaller or less redundant matrices, matrices that have more stable predictive information or those that share more information with other matrices. Such weighting approaches are implemented in multiple-factor analysis (MFA), principal covariates regression [<xref rid="bbv108-B63" ref-type="bibr">63</xref>] and STATIS.</p><p>The simplest multi-omics data integration is when the data sets have the same variables and observations, that is, matched rows and matched columns. In genomics, these could be produced when variables from different multi-assay data sets are mapped to a common set of genomic coordinates or gene identifiers, thus generating data sets with matched variables and matched observations. Alternatively, a repeated analysis, or longitudinal analysis of the same samples and the same variables, could produce such data (one should note that these dimension reduction approaches do not model the time correlation between different datasets). There is a history of such analyses in ecology where counts of species and environment variables are measured over different seasons [<xref rid="bbv108-B49" ref-type="bibr">49</xref>, <xref rid="bbv108-B64" ref-type="bibr">64</xref>, <xref rid="bbv108-B65" ref-type="bibr">65</xref>] and in psychology where different standardized tests are measured multiple times on study populations [<xref rid="bbv108-B66" ref-type="bibr">66</xref>, <xref rid="bbv108-B67" ref-type="bibr">67</xref>]. Analysis of such <italic>variables x samples x time</italic> data are called a three-mode decomposition, triadic, cube or three-way table analysis, tensor decomposition, three-way PCA, three-mode PCA, three-mode Factor Analysis, Tucker-3 model, Tucker3, TUCKALS3, multi-block analysis, among others (<xref ref-type="table" rid="bbv108-T4">Table 4</xref>). The relationship between various tensor or higher decompositions for multi-block analysis are reviewed by Kolda and Bader [<xref rid="bbv108-B68" ref-type="bibr">68</xref>].</p><p>More frequently, we need to find associations in multi-assay data that have matched observations but have different and unmatched numbers of variables. For example, TCGA generated miRNA and mRNA transcriptome (RNAseq, microarray), DNA copy number, DNA mutation, DNA methylation and proteomics molecular profiles on each tumor. The NCI-60 and the Cancer Cell Line Encyclopedia projects have measured pharmacological compound profiles in addition to exome sequencing and transcriptomic profiles. Methods that can be applied to EDA of <italic>K</italic>-table of multi-assay data with different variables include MCIA, MFA, Generalized CCA (GCCA) and Consensus PCA (CPCA).</p><p>The <italic>K</italic>-table methods can be generally expressed by the following model:
<disp-formula id="bbv108-M13"><label>(13)</label><mml:math id="MM19"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">1</mml:mi></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">1</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi mathvariant="bold">1</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">k</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mi mathvariant="bold">F</mml:mi><mml:msubsup><mml:mi mathvariant="bold">Q</mml:mi><mml:mi mathvariant="bold">K</mml:mi><mml:mi mathvariant="bold">T</mml:mi></mml:msubsup><mml:mo>&#x02009;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#x02009;</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi mathvariant="bold">K</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where there are <italic>K</italic> matrices or omics data sets <bold>X<sub>1</sub>,&#x02026;,X<sub>K</sub></bold>. For convenience, we assume that the rows of <bold>X<sub>k</sub></bold> share a common set of observations but the columns of <bold>X<sub>k</sub></bold> may each have different variables. <bold>F</bold> is the &#x02018;global score' matrix. Its columns are the PCs and are interpreted similarly to PCs from a PCA of a single data set. The global score matrix, which is identical in all decompositions, integrates information from all data sets. Therefore, it is not specific to any single data set, rather it represents the common pattern defined by all data sets. The matrices <bold>Q<sub>k</sub></bold>, with k ranging from 1 to <italic>K</italic>, are the loadings or coefficient matrices. A high positive value indicates a strong positive contribution of the corresponding variable to the &#x02018;global score'. While the above methods are formulated for multiple data sets with different variables but the same observations, most can be similarly formulated for multiple data sets with the same variables but different observations [<xref rid="bbv108-B69" ref-type="bibr">69</xref>].</p><sec><title>Multiple co-inertia analysis</title><p>MCIA is an extension of CIA which aims to analyze multiple matrices through optimizing a covariance criterion [<xref rid="bbv108-B60" ref-type="bibr">60</xref>, <xref rid="bbv108-B70" ref-type="bibr">70</xref>]. MCIA simultaneously projects <italic>K</italic> data sets into the same dimensional space. Instead of maximizing the covariance between scores from two data sets as in CIA, the optimization problem used by MCIA is as following:
<disp-formula id="bbv108-M14"><label>(14)</label><mml:math id="MM20"><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mn>1</mml:mn><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi></mml:mstyle></mml:msubsup><mml:mn>..</mml:mn><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mi>k</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi></mml:mstyle></mml:msubsup><mml:mn>..</mml:mn><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mi>K</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>i</mml:mi></mml:mstyle></mml:msubsup></mml:mrow></mml:msub><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mi>k</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">X</mml:mi><mml:mrow/><mml:mi>i</mml:mi></mml:msubsup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>q</mml:mi></mml:mstyle><mml:mrow/><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula>
for dimension <italic>i</italic> with the constraints that ||<bold>q</bold><sub>k</sub><italic><sup>i</sup></italic>|| = var (<bold>X</bold><italic><sup>i</sup></italic><bold>q</bold><italic><sup>i</sup></italic>) = 1, where <bold>X</bold> = (<bold>X<sub>1</sub></bold>|&#x02026;|<bold>X<sub>k</sub></bold>|&#x02026;|<bold>X<sub>K</sub></bold>) and <bold>q</bold><italic><sup>i</sup></italic> holds the corresponding loading values (&#x02018;global' loading vector) [<xref rid="bbv108-B60" ref-type="bibr">60</xref>, <xref rid="bbv108-B69" ref-type="bibr">69</xref>, <xref rid="bbv108-B70" ref-type="bibr">70</xref>].</p><p>MCIA derives a set of &#x02018;block scores' <bold>X</bold><sub>k</sub><italic><sup>i</sup></italic><bold>q</bold><sub>k</sub><italic><sup>i </sup></italic>using linear combinations of the original variables from each individual matrix. The global score <bold>X</bold><italic><sup>i</sup></italic><bold>q</bold><italic><sup>i</sup></italic> is then further defined as the linear combination of &#x02018;block scores'. In practice, the global scores represent a correlated structure defined by multiple data sets, whereas the concordance and discrepancy between these data sets may be revealed by the block scores (for detail see &#x02018;Example case study' section). MCIA may be calculated with the <italic>ad hoc</italic> extension of the NIPALS PCA [<xref rid="bbv108-B71" ref-type="bibr">71</xref>]. This algorithm starts with an initialization step in which the global scores and the block loadings for the first dimension are computed. The residual matrices are calculated in an iterative step by removing the variance induced by the variable loadings (the &#x02018;deflation' step). For higher order solutions, the same procedure is applied to the residual matrices and re-iterated until the desired number of dimensions is reached. Therefore, the computation time strongly depends on the number of desired dimensions. MCIA is implemented in the R package omicade4 and has been applied to the integrative analysis of transcriptomic and proteomic data sets from the NCI-60 cell lines [<xref rid="bbv108-B60" ref-type="bibr">60</xref>].</p></sec></sec><sec><title>Generalized canonical correlation analysis</title><p>GCCA [<xref rid="bbv108-B71" ref-type="bibr">71</xref>] is a generalization of CCA to <italic>K</italic>-table analysis [<xref rid="bbv108-B73" ref-type="bibr">73&#x02013;75</xref>]. It has also been applied to the analysis of omics data [<xref rid="bbv108-B36" ref-type="bibr">36</xref>, <xref rid="bbv108-B76" ref-type="bibr">76</xref>]. Typically, MCIA and GCCA will produce similar results (for a more detailed comparison see [<xref rid="bbv108-B60" ref-type="bibr">60</xref>]). GCCA uses a different deflation strategy than MCIA: it calculates the residual matrices by removing the variance with respect to the &#x02018;block scores' (instead of &#x02018;variable loadings' used by MCIA or &#x02018;global scores' used by CPCA; see later). When applied to omics data where <italic>p</italic> &#x0226b; <italic>n</italic>, a variable selection step is often integrated within the GCCA approach, which cannot be done in case of MCIA. In addition, as block scores are better representations of a single data set (in contrast to the global score), GCCA is more likely to find common variables across data sets. Witten and Tibshirani [<xref rid="bbv108-B54" ref-type="bibr">54</xref>] applied sparse multiple CCA to analyze gene expression and Copy Number Variation (CNV) data from diffuse large B-cell lymphoma patients and successfully identified &#x02018;<italic>cis</italic> interactions' that are both up-regulated in CNV and mRNA data.</p><sec><title>Consensus PCA</title><p>CPCA is closely related to GCCA and MCIA, but has had less exposure to the omics data community. CPCA optimizes the same criterion as GCCA and MCIA and is subject to the same constraints as MCIA [<xref rid="bbv108-B71" ref-type="bibr">71</xref>]. The deflation step of CPCA relies on the &#x02018;global score'. As a result, it guarantees the orthogonality of the global score only and tends to find common patterns in the data sets. This characteristic makes it is more suitable for the discovery of joint patterns in multiple data sets, such as the joint clustering problem.</p></sec><sec><title>Regularized generalized canonical correlation analysis</title><p>Recently, Tenenhaus and Tenenhaus [<xref rid="bbv108-B69" ref-type="bibr">69</xref>, <xref rid="bbv108-B74" ref-type="bibr">74</xref>] proposed regularized generalized CCA (RGCCA), which provides a unified framework for different <italic>K</italic>-table multivariate methods. The RGCCA model introduces some extra parameters, particularly a shrinkage parameter and a linkage parameter. The linkage parameter is defined so that the connection between matrices may be customized. The shrinkage parameter ranges from 0 to 1. Setting this parameter to 0 will force the correlation criterion (criterion used by GCCA), whereas a shrinkage parameter of 1 will apply the covariance criterion (used by MCIA and CPCA). A value between 0 and 1 leads to a compromise between the two options. In practice, the correlation criterion is better in explaining the correlated structure across data sets, while discarding the variance within each individual data set. The introduction of the extra parameters make RGCCA highly versatile. GCCA, CIA and CPCA can be described as special cases of RGCCA (see [<xref rid="bbv108-B69" ref-type="bibr">69</xref>] and <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/lookup/suppl/doi:10.1093/bib/bbv108/-/DC1">Supplementary Information</ext-link>). In addition, RGCCA also integrates a feature selection procedure, named sparse GCCA (SGCCA). Tenenhaus <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B76" ref-type="bibr">76</xref>] applied SGCCA to combine gene expression, comparative genomic hybridization and a qualitative phenotype measured on a set of 53 children with glioma. Sparse multiple CCA [<xref rid="bbv108-B54" ref-type="bibr">54</xref>] and SGCCA [<xref rid="bbv108-B76" ref-type="bibr">76</xref>] are available in the R packages PMA and RGCCA, respectively. Similarly, a higher order implementation of spare PLS is described in Zhao <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B77" ref-type="bibr">77</xref>].</p></sec><sec><title>Joint NMF</title><p>NMF has also been extended to jointly factorize multiple matrices. In joint NMF, the values in the global score <bold>F</bold> and in the coefficient matrices (<bold>Q<sub>1</sub></bold>,&#x02026;,<bold>Q<sub>K</sub></bold>) are nonnegative and there is no explicit definition of the block loadings. An optimization algorithm is applied to minimize an objective function, typically the sum of squared errors, i.e. <inline-formula><mml:math id="MM21"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mtext>k</mml:mtext><mml:mo>&#x02009;</mml:mo><mml:mo>=</mml:mo><mml:mo>&#x02009;</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mtext>E</mml:mtext><mml:mi mathvariant="normal">k</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>. This approach can be considered to be a nonnegative implementation of PARAFAC, although it has also been implemented using the Tucker model [<xref rid="bbv108-B78" ref-type="bibr">78&#x02013;80</xref>]. Zhang <italic>et&#x000a0;al.</italic> [<xref rid="bbv108-B81" ref-type="bibr">81</xref>] apply joint NMF to a three-way analysis of DNA methylation, gene expression and miRNA expression data to identify modules in each of these regulatory layers that are associated with each other.</p></sec></sec><sec><title>Advantages of dimension reduction when integrating multi-assay data</title><p>Dimension reduction or latent variable approaches provide EDA, integrate multi-assay data, highlight global correlations across data sets, and discover outliers or batch effects in individual data sets. Dimension reduction approaches also facilitate down-stream analysis of both observations and variables (genes). Compared with cluster analysis of individual data sets, cluster analysis of the global score matrix (<bold>F</bold> matrix) is robust, as it aggregates observations across data sets and is less likely to reflect a technical or batch effect of a single data set. Similarly, dimension reduction of multi-assay data facilities downstream gene set, pathway and network analysis of variables. MCIA transforms variables from each data set onto the same scale, and their loadings (<bold>Q</bold> matrix) rank the variables by their contribution to the global data structure (<xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>). Meng <italic>et&#x000a0;al.</italic> report that pathway or gene set enrichment analysis (GSEA) of the transformed variables is more sensitive than GSEA of each individual data set. This is both because of the re-weighting and transformation of variables, but also because GSEA on the combined data has greater coverage of variables (genes) thus compensating for missing or unreliable information in any single data set. For example, in <xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>B, we integrate and transform mRNA, proteomics and miRNA data on the same scale, allowing us to extract and study the union of all variables.
<fig id="bbv108-F2" orientation="portrait" position="float"><label>Figure 2.</label><caption><p>MCIA of mRNA, miRNA and proteomics profiles of melanoma (ME), leukemia (LE) and central nervous system (CNS) cell lines. (<bold>A</bold>) shows a plot of the first two components in sample space (sample &#x02018;type' is coded by the point shape; circles for mRNAs, triangles for proteins and squares for miRNAs). Each sample (cell line) is represented by a &#x0201c;star&#x0201d;, where the three omics data for each cell line are connected by lines to a center point, which is the global score (<bold>F</bold>) for that cell line, the shorter the line, the higher the level of concordance between the data types and the global structure. (<bold>B</bold>) shows the variable space of MCIA. A variable that is highly expressed in a cell line will be projected with a high weight (far from the origin) in the direction of that cell line. Some miRNAs with a large distance from the origin are labeled, as these miRNAs are the strongly associated with cancer tissue of origin. (<bold>C</bold>) shows the correlation coefficients of the proteome profiling of SR with other cell lines. The proteome profiling of SR cell line is more correlated with melanoma cell line. There may be a technical issue with the LE.SR proteomics data. (<bold>D</bold>) A scree plot of the eigenvalues and (<bold>E</bold>) a plot of data weighting space. A colour version of this figure is available online at BIB online: <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org">http://bib.oxfordjournals.org</ext-link>.</p></caption><graphic xlink:href="bbv108f2p"/></fig>
</p></sec><sec><title>Example case study</title><p>To demonstrate the integration of multi-data sets using dimensions reduction, we applied MCIA to analyze mRNA, miRNA and proteomics expression profiles of melanoma, leukemia and CNS cells lines from the NCI-60 panel. The graphical output from this analysis, a plot of the sample space, variable space and data weighting space are provided in <xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>A, B and E. The eigenvalues can be interpreted similarly to PCA, a higher eigenvalue contributes more information to the global score. As with PCA, researchers may be subjective in their selection of the number of components [<xref rid="bbv108-B24" ref-type="bibr">24</xref>]. The scree plot in <xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>D shows the eigenvalues of each global score. In this case, the first two eigenvalues were significantly larger, so we visualized the cell lines and variables on PC1 and PC2.</p><p>In the observation space (<xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>A), assay data (mRNA, miRNA and proteomics) are distinguished by shape. The coordinates of each cell line (<bold>F</bold><sub>k</sub> in <xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>A) are connected by lines to the global scores (<bold>F</bold>). Short lines between points and cell line global scores reflect high concordance in cell line data. Most cell lines have concordant information between data sets (mRNA, miRNA, protein) as indicated by relatively short lines. In addition, the RV coefficient [<xref rid="bbv108-B82" ref-type="bibr">82</xref>, <xref rid="bbv108-B83" ref-type="bibr">83</xref>], which is a generalized Pearson correlation coefficient for matrices, may be used to estimate the correlation between two transformed data sets. The RV coefficient has values between 0 and 1, where a higher value indicates higher co-structure. In this example, we observed relatively high RV coefficients between the three data sets, ranging from 0.78 to 0.84. It was recently reported that the RV coefficient is biased toward large data sets, and a modified RV coefficient has been proposed [<xref rid="bbv108-B84" ref-type="bibr">84</xref>].</p><p>In this analysis (<xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>A), cell lines originating from the same anatomical source are projected close to each other and converge in clusters. The first PC separates the leukemia cell lines (positive end of PC1) from the other two cell lines (negative end of PC1), and PC2 separates the melanoma and CNS cell lines. The melanoma cell line LOX-IMVI, which lacks the melanogenesis, is projected close to the origin, away from the melanoma cluster. We were surprised to see that the proteomics profile of leukemia cell line SR was projected closer to melanoma rather than leukemia cell lines. We examined within tumor type correlations to the SR cell line (<xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>C). We observed that the SR proteomics data had higher correlation with melanoma compared with to leukemia cell lines. Given that the mRNA and miRNA profiles of LE_SR are closer to the leukemia cell lines, it suggests that there may have been a technical error in generating the proteomics data on the SR cell line (<xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>A and C).</p><p>MCIA projects all variables into the same space. The variable space (<bold>Q<sub>1</sub></bold>,&#x02026;, <bold>Q<sub>K</sub></bold>) is visualized in <xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>B. Variables and samples projected in the same direction are associated. This allows one to select the variables most strongly associated with specific observations from each data set for subsequent analysis. In our previous study [<xref rid="bbv108-B85" ref-type="bibr">85</xref>], we have shown that the genes and proteins highly weighted on the melanoma side (positive end of second dimension) are enriched with melanogenesis functions, and genes/proteins highly weighted on the protein side are highly enriched in T-cell or immune-related functions.</p><p>We examined the miRNA data to extract the miRNAs with the most extreme weights on the first two dimensions. miR-142 and miR-223, which are active and expressed in leukemia [<xref rid="bbv108-B82" ref-type="bibr">82</xref>, <xref rid="bbv108-B83" ref-type="bibr">83</xref>, <xref rid="bbv108-B86" ref-type="bibr">86&#x02013;88</xref>], had high weights on the positive end of both the first and second axis (close to the leukemia cell lines sample space, <xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>A). miR-142 plays an essential role in T-lymphocyte development. miR-223 is regulated by the Notch and NF-kB signaling pathways in T-cell acute lymphoblastic leukemia [<xref rid="bbv108-B89" ref-type="bibr">89</xref>].</p><p>The miRNA with strongest association to CNS cell lines was miR-409. This miRNA is reported to promote the epithelial-to-mesenchymal transition in prostate cancer [<xref rid="bbv108-B90" ref-type="bibr">90</xref>]. In the NCI-60 cell line data, CNS cell lines are characterized more by a pronounced mesenchymal phenotype, which is consistent with high expression of this miRNA. On the positive end of the second axis and negative end of the first axis (which corresponds to melanoma cell lines in the sample space, <xref ref-type="fig" rid="bbv108-F2">Figure 2</xref>A), we found miR-509, miR-513 and miR-506 strongly associated with melanoma cell lines, which are reported to initiate melanocyte transformation and promote melanoma growth [<xref rid="bbv108-B85" ref-type="bibr">85</xref>].</p></sec><sec><title>Challenges in integrative data analysis</title><p>EDA is widely used and well accepted in the analysis of single omics data sets, but there is an increasing need for methods that integrate multi-omics data, particularly in cancer research. Recently, 20 leading scientists were invited to a meeting organized by <italic>Nature Medicine</italic>, <italic>Nature Biotechnology</italic> and the Volkswagen Foundation. The meeting identified the need to simultaneously characterize DNA sequence, epigenome, transcriptome, protein, metabolites and infiltrating immune cells in both the tumor and the stroma [<xref rid="bbv108-B91" ref-type="bibr">91</xref>]. The TCGA pan-cancer project plans to comprehensively interrogate multi-omics data across 33 human cancers [<xref rid="bbv108-B92" ref-type="bibr">92</xref>]. The data are biologically complex. In addition to tumor heterogeneity [<xref rid="bbv108-B91" ref-type="bibr">91</xref>] there may be technical issues, batch effects and outliers. EDA approaches for complex multi-omics data are needed.</p><p>We describe emerging applications of multivariate approaches to omics data analysis. These are descriptive approaches that do not test a hypothesis or generate a <italic>P</italic>-value. They are not optimized for variable or biomarker discovery, although the introduction of sparsity in variable loadings may help in the selection of variables for downstream analysis. Few comparisons of different methods exist, and the numbers of components and the sparsity level have to be optimized. Cross-validation approaches are potentially useful but this still remains an open area of research.</p><p>Another limitation of these methods is that, although variables may vary among data sets, the observations need to be matchable. Therefore, researchers need to have careful experimental design in the early stage of a study. There is an extension of CIA for the analysis of unmatched samples [<xref rid="bbv108-B93" ref-type="bibr">93</xref>], which combines a Hungarian algorithm with CIA to iteratively pair samples that are similar but not matched. Multi-block and multi-group methods (data sets with matched variables) have been reviewed recently by Tenenhaus and Tenenhaus [<xref rid="bbv108-B69" ref-type="bibr">69</xref>].</p><p>The number of variables in genomics data is a challenge to traditional EDA visualization tools. Most visualization approaches were designed for data sets with fewer variables. Within R, new packages including ggord are being developed. Dynamic data visualization is possible using ggvis, plotly, explor and other packages. However, interpretation of long lists of biological variables (genes, proteins, miRNAs) is challenging. One way to gain more insight into lists of omics variables is to perform a network, gene set enrichment or pathway analysis [<xref rid="bbv108-B94" ref-type="bibr">94</xref>]. An attractive feature of decomposition methods is that variable annotation, such as Gene Ontology or Reactome, can be projected into the same space, to determine a score for each gene set (or pathway) in that space [<xref rid="bbv108-B32" ref-type="bibr">32</xref>, <xref rid="bbv108-B33" ref-type="bibr">33</xref>, <xref rid="bbv108-B60" ref-type="bibr">60</xref>].</p></sec><sec sec-type="conclusions"><title>Conclusion</title><p>Dimension reduction methods have a long history. Many similar methods have been developed in parallel by multiple fields. In this review, we provided an overview of dimension reduction techniques that are both well-established and maybe new to the multi-omics data community. We reviewed methods for single-table, two-table and multi-table analysis. There are significant challenges in extracting biologically and clinically actionable results from multi-omics data, however, the field may leverage the varied and rich resource of dimension reduction approaches that other disciplines have developed.</p><p><boxed-text id="bbv108-BOX1" position="float" orientation="portrait"><caption><p>Key Points</p></caption><p>
<list list-type="bullet"><list-item><p>There are many dimension-reduction methods, which can be applied to exploratory data analysis of a single data set, or integrated analysis of a pair or multiple data sets. In addition to exploratory analysis, these can be extended to clustering, supervised and discriminant analysis.</p></list-item><list-item><p>The goal of dimension reduction is to map data onto a new set of variables so that most of the variance (or information) in the data is explained by a few new (latent) variables.</p></list-item><list-item><p>Multi-data set methods such as multiple co-inertia analysis (MCIA), multiple factor analysis (MFA) or canonical correlations analysis (CCA) identify correlated structure between data sets with matched observations (samples). Each data set may have different variables (genes, proteins, miRNA, mutations, drug response, etc).</p></list-item><list-item><p>MCIA, MFA, CCA and related methods provide a visualization of consensus and incongruence in and between data sets, enabling discovery of potential outliers, batch effects or technical errors.</p></list-item><list-item><p>Multi-dataset methods transform diverse variables from each data set onto the same space and scale, facilitating integrative variable selection, gene set analysis, pathway and downstream analyses.</p></list-item></list></p></boxed-text></p></sec><sec><title>R Supplement</title><p>R code to re-generate all figures in this article is available as a <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/lookup/suppl/doi:10.1093/bib/bbv108/-/DC1">Supplementary File</ext-link>. Data and code are also available on the github repository <ext-link ext-link-type="uri" xlink:href="https://github.com/aedin/NCI60Example">https://github.com/aedin/NCI60Example</ext-link>.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Data</title><p><ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/lookup/suppl/doi:10.1093/bib/bbv108/-/DC1">Supplementary data</ext-link> are available online at <ext-link ext-link-type="uri" xlink:href="http://bib.oxfordjournals.org/">http://bib.oxfordjournals.org/</ext-link>.</p>
<supplementary-material id="PMC_1" content-type="local-data">
<caption>
<title>Supplementary Data</title>
</caption>
<media mimetype="text" mime-subtype="html" xlink:href="supp_17_4_628__index.html"/>
<media xlink:role="associated-file" mimetype="application" mime-subtype="msword"
xlink:href="supp_bbv108_Supplemental_Information.docx"/>
</supplementary-material>
</sec></body><back><ack><title>Acknowledgements</title><p>We are grateful to Drs. John Platig and Marieke Kuijjer for reading the manuscript and for their comments.</p><sec><title>Funding</title><p>GGT was supported by the <funding-source>Austrian Ministry of Science, Research and Economics</funding-source> [OMICS Center HSRSM initiative]. AC was supported by Dana-Farber <funding-source>Cancer Institute BCB Research Scientist Developmental Funds</funding-source>, <funding-source>National Cancer Institute</funding-source>
<award-id>2P50 CA101942-11</award-id> and the <funding-source>Assistant Secretary of Defense Health Program</funding-source>, through the <funding-source>Breast Cancer Research Program</funding-source> under Award No. <award-id>W81XWH-15-1-0013</award-id>. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the Department of Defense.</p></sec></ack><bio id="d36e39"><p><bold>Chen Meng</bold> is a senior graduate student currently completing his thesis entitled &#x02018;Application of multivariate methods to the integrative analysis of omics data' in Bernhard Kuster's group at Technische Universit&#x000e4;t M&#x000fc;nchen, Germany.</p></bio><bio id="d36e52"><p><bold>Oana Zeleznik</bold> has recently defended her thesis &#x02018;Integrative Analysis of Omics Data. Enhancement of Existing Methods and Development of a Novel Gene Set Enrichment Approach&#x02019; in Gerhard Thallinger's group at the Graz University of Technology, Austria.</p></bio><bio id="d36e63"><p><bold>Gerhard Thallinger </bold>is a principal investigator at Graz University of Technology, Austria. His research interests include analysis of next-generation sequencing, microbiome and lipidomics data. He supervised Oana Zeleznik&#x02019;s graduate studies.</p></bio><bio id="d36e74"><p><bold>Bernhard Kuster</bold> is Full Professor for Proteomics and Bioanalytics at the Technical University Munich and Co-Director of the Bavarian Biomolecular Mass Spectrometry Center. His research focuses on mass spectrometry-based proteomics and chemical biology. He is supervising the graduate studies of Chen Meng.</p></bio><bio id="d36e85"><p><bold>Amin M. Gholami</bold> is a bioinformatics scientist in the Division of Vaccine Discovery at La Jolla Institute for Allergy and Immunology. He is working on big data integrative and comparative analysis of multi-omics data. He was previously a bioinformatics group leader at Technische Universit&#x000e4;t M&#x000fc;nchen, where he supervised Chen Meng.</p></bio><bio id="d36e96"><p><bold>Aed&#x000ed;n Culhane</bold> is a research scientist in Biostatistics and Computational Biology at Dana-Farber Cancer Institute and Harvard T.H. Chan School of Public Health. She develops and applies methods for integrative analysis of omics data in cancer. Dr Zeleznik was a visiting student at the Dana-Faber Cancer Institute in Dr Culhane&#x02019;s Lab during her PhD. Dr Culhane co-supervises Dr Zeleznik and Mr Meng on several projects.</p></bio><ref-list><title>References</title><ref id="bbv108-B1"><label>1</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brazma</surname><given-names>A</given-names></name><name><surname>Culhane</surname><given-names>AC</given-names></name></person-group>
<article-title>Algorithms for gene expression analysis</article-title>. In: <person-group person-group-type="author"><name><surname>Jorde</surname><given-names>LB</given-names></name><name><surname>Little</surname><given-names>PFR</given-names></name><name><surname>Dunn</surname><given-names>MJ</given-names></name><name><surname>Subramaniam</surname><given-names>S</given-names></name></person-group> (eds). <source>Encyclopedia of Genetics, Genomics, Proteomics and Bioinformatics</source>. <publisher-loc>London</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>, <year>2005</year>, <fpage>3148</fpage>&#x02013;<lpage>59</lpage>.</mixed-citation></ref><ref id="bbv108-B2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leek</surname><given-names>JT</given-names></name><name><surname>Scharpf</surname><given-names>RB</given-names></name><name><surname>Bravo</surname><given-names>HC</given-names></name><etal/></person-group>
<article-title>Tackling the widespread and critical impact of batch effects in high-throughput data</article-title>. <source>Nat Rev Genet</source>
<year>2010</year>;<volume>11</volume>:<fpage>733</fpage>&#x02013;<lpage>9</lpage>.<pub-id pub-id-type="pmid">20838408</pub-id></mixed-citation></ref><ref id="bbv108-B3"><label>3</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Legendre</surname><given-names>P</given-names></name><name><surname>Legendre</surname><given-names>LFJ</given-names></name></person-group>
<source>Numerical Ecology</source>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Elsevier Science</publisher-name>; <edition>3rd edition</edition>
<year>2012</year>.</mixed-citation></ref><ref id="bbv108-B4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biton</surname><given-names>A</given-names></name><name><surname>Bernard-Pierrot</surname><given-names>I</given-names></name><name><surname>Lou</surname><given-names>Y</given-names></name><etal/></person-group>
<article-title>Independent component analysis uncovers the landscape of the bladder tumor transcriptome and reveals insights into luminal and basal subtypes</article-title>. <source>Cell Rep</source>
<year>2014</year>,<volume>9</volume>:<fpage>1235</fpage>&#x02013;<lpage>45</lpage>.<pub-id pub-id-type="pmid">25456126</pub-id></mixed-citation></ref><ref id="bbv108-B5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoadley</surname><given-names>KA</given-names></name><name><surname>Yau</surname><given-names>C</given-names></name><name><surname>Wolf</surname><given-names>DM</given-names></name><etal/></person-group>
<article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>. <source>Cell</source>
<year>2014</year>;<volume>158</volume>:<fpage>929</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="pmid">25109877</pub-id></mixed-citation></ref><ref id="bbv108-B6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhaak</surname><given-names>RG</given-names></name><name><surname>Tamayo</surname><given-names>P</given-names></name><name><surname>Yang</surname><given-names>JY</given-names></name><etal/></person-group>
<article-title>Prognostically relevant gene signatures of high-grade serous ovarian carcinoma</article-title>. <source>J Clin Invest</source>
<year>2013</year>;<volume>123</volume>:<fpage>517</fpage>&#x02013;<lpage>25</lpage>.<pub-id pub-id-type="pmid">23257362</pub-id></mixed-citation></ref><ref id="bbv108-B7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senbabaoglu</surname><given-names>Y</given-names></name><name><surname>Michailidis</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>JZ</given-names></name></person-group>
<article-title>Critical limitations of consensus clustering in class discovery</article-title>. <source>Sci Rep</source>
<year>2014</year>;<volume>4</volume>:<fpage>6207</fpage>.<pub-id pub-id-type="pmid">25158761</pub-id></mixed-citation></ref><ref id="bbv108-B8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gusenleitner</surname><given-names>D</given-names></name><name><surname>Howe</surname><given-names>EA</given-names></name><name><surname>Bentink</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>iBBiG: iterative binary bi-clustering of gene sets</article-title>. <source>Bioinformatics</source>
<year>2012</year>;<volume>28</volume>:<fpage>2484</fpage>&#x02013;<lpage>92</lpage>.<pub-id pub-id-type="pmid">22789589</pub-id></mixed-citation></ref><ref id="bbv108-B9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearson</surname><given-names>K</given-names></name></person-group>
<article-title>On lines and planes of closest fit to systems of points in space</article-title>. <source>Philos Magazine Series</source>
<year>1901</year>;<volume>6</volume>(<issue>2</issue>):<fpage>559</fpage>&#x02013;<lpage>72</lpage>.</mixed-citation></ref><ref id="bbv108-B10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotelling</surname><given-names>H</given-names></name></person-group>
<article-title>Analysis of a complex of statistical variables into principal components</article-title>. <source><italic>J Educ </italic>Psychol</source>
<year>1933</year>;<volume>24</volume>:<fpage>417</fpage>.</mixed-citation></ref><ref id="bbv108-B11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bland</surname><given-names>M</given-names></name></person-group>
<comment><italic>An Introduction to Medical Statistics</italic>. 3rd edn. Oxford; New York: Oxford University Press, 2000. xvi, 405</comment>.</mixed-citation></ref><ref id="bbv108-B12"><label>12</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Warner</surname><given-names>RM</given-names></name></person-group>
<source><italic>Applied Statistics: From Bivariate through Multivariate Techniques</italic></source>. <publisher-loc>London</publisher-loc>: <publisher-name>SAGE Publications, Inc</publisher-name>; <edition>2nd Edition</edition>
<year>2012</year>, <fpage>1004</fpage>.</mixed-citation></ref><ref id="bbv108-B13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dray</surname><given-names>S</given-names></name><name><surname>Chessel</surname><given-names>D</given-names></name><name><surname>Thioulouse</surname><given-names>J</given-names></name></person-group>
<article-title>Co-inertia analysis and the linking of ecological data tables</article-title>. <source>Ecology</source>
<year>2003</year>;<volume>84</volume>:<fpage>3078</fpage>&#x02013;<lpage>89</lpage>.</mixed-citation></ref><ref id="bbv108-B14"><label>14</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Golub</surname><given-names>GH</given-names></name><name><surname>Loan</surname><given-names>CF</given-names></name></person-group>
<source>Matrix Computations</source>. <publisher-loc>Baltimore</publisher-loc>: <publisher-name>JHU Press</publisher-name>, <year>2012</year>.</mixed-citation></ref><ref id="bbv108-B15"><label>15</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Anthony</surname><given-names>M</given-names></name><name><surname>Harvey</surname><given-names>M</given-names></name></person-group>
<source>Linear Algebra: Concepts and Methods</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <edition>1st edition</edition>
<year>2012</year>, <fpage>149</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="bbv108-B16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McShane</surname><given-names>LM</given-names></name><name><surname>Cavenagh</surname><given-names>MM</given-names></name><name><surname>Lively</surname><given-names>TG</given-names></name><etal/></person-group>
<article-title>Criteria for the use of omics-based predictors in clinical trials: explanation and elaboration</article-title>. <source>BMC Med</source>
<year>2013</year>;<volume>11</volume>:<fpage>220</fpage>.<pub-id pub-id-type="pmid">24228635</pub-id></mixed-citation></ref><ref id="bbv108-B17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Elisseeff</surname><given-names>A</given-names></name></person-group>
<article-title>An introduction to variable and feature selection</article-title>. <source>J Mach Learn Res</source>
<year>2003</year>;<volume>3</volume>:<fpage>1157</fpage>&#x02013;<lpage>82</lpage>.</mixed-citation></ref><ref id="bbv108-B18"><label>18</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tukey</surname><given-names>JW</given-names></name></person-group>
<source>Exploratory Data Analysis</source>. <publisher-loc>Boston</publisher-loc>: <publisher-name>Addison-Wesley Publishing Company</publisher-name>
<year>1977</year>.</mixed-citation></ref><ref id="bbv108-B19"><label>19</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hogben</surname><given-names>L</given-names></name></person-group>
<source>Handbook of Linear Algebra</source>, <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>Chapman &#x00026; Hall/CRC Press</publisher-name>; <edition>1st edition</edition>
<year>2007</year>, <fpage>20</fpage>.</mixed-citation></ref><ref id="bbv108-B20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ringner</surname><given-names>M</given-names></name></person-group>
<article-title>What is principal component analysis?</article-title>
<source>Nat Biotechnol</source>
<year>2008</year>;<volume>26</volume>:<fpage>303</fpage>&#x02013;<lpage>4</lpage>.<pub-id pub-id-type="pmid">18327243</pub-id></mixed-citation></ref><ref id="bbv108-B21"><label>21</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wall</surname><given-names>ME</given-names></name><name><surname>Rechtsteiner</surname><given-names>A</given-names></name><name><surname>Rocha</surname><given-names>LM</given-names></name></person-group> (<year>2003</year>) <comment>Singular value decomposition and principal component analysis. In: <italic>A Practical Approach to Microarray</italic>. Data Analysis. Berrar, D.P., Dubitzky, W., Granzow, M. (eds) Norwell, MA: Kluwer, 2003, 91&#x02013;109</comment>.</mixed-citation></ref><ref id="bbv108-B22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>terBraak</surname><given-names>CJF</given-names></name><name><surname>Prentice</surname><given-names>IC</given-names></name></person-group>
<article-title>A theory of gradient analysis</article-title>. <source>Adv. Ecol. Res.</source>
<year>1988</year>;<volume>18</volume>:<fpage>271</fpage>&#x02013;<lpage>313</lpage>.</mixed-citation></ref><ref id="bbv108-B23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>terBraak</surname><given-names>CJF</given-names></name><name><surname>Smilauer</surname><given-names>P</given-names></name></person-group>
<article-title>CANOCO Reference Manual and User's Guide to Canoco for Windows: Software for Canonical Community Ordination (version 4)</article-title>. <comment>Microcomputer Power (Ithaca, NY USA) <ext-link ext-link-type="uri" xlink:href="http://www.microcomputerpower.com/">http://www.microcomputerpower.com/</ext-link>1998;352</comment>.</mixed-citation></ref><ref id="bbv108-B24"><label>24</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Williams</surname><given-names>LJ</given-names></name></person-group>
<source>Principal Component Analysis</source>. <publisher-loc>Hoboken</publisher-loc>: <publisher-name>John Wiley &#x00026; SonsInterdisciplinary Reviews: Computational Statistics</publisher-name>
<year>2010</year>;<volume>2</volume>(<issue>4</issue>):<fpage>433</fpage>&#x02013;<lpage>59</lpage>.</mixed-citation></ref><ref id="bbv108-B25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dray</surname><given-names>S</given-names></name></person-group>
<article-title>On the number of principal components: a test of dimensionality based on measurements of similarity between matrices</article-title>. <source>Comput Stat Data Anal</source>
<year>2008</year>;<volume>52</volume>:<fpage>2228</fpage>&#x02013;<lpage>37</lpage>.</mixed-citation></ref><ref id="bbv108-B26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wouters</surname><given-names>L</given-names></name><name><surname>Gohlmann</surname><given-names>HW</given-names></name><name><surname>Bijnens</surname><given-names>L</given-names></name><etal/></person-group>
<article-title>Graphical exploration of gene expression data: a comparative study of three multivariate methods</article-title>. <source>Biometrics</source>
<year>2003</year>;<volume>59</volume>:<fpage>1131</fpage>&#x02013;<lpage>9</lpage>.<pub-id pub-id-type="pmid">14969494</pub-id></mixed-citation></ref><ref id="bbv108-B27"><label>27</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Greenacre</surname><given-names>M</given-names></name></person-group>
<source>Correspondence Analysis in Practice</source>. <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>Chapman and Hall/CRC</publisher-name>; <edition>2 edition </edition>
<year>2007</year>, <fpage>201</fpage>&#x02013;<lpage>11</lpage>.</mixed-citation></ref><ref id="bbv108-B28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodrich</surname><given-names>JK</given-names></name><name><surname>Rienzi</surname><given-names>DSC</given-names></name><name><surname>Poole</surname><given-names>AC</given-names></name><etal/></person-group>
<article-title>Conducting a microbiome study</article-title>. <source>Cell</source>
<year>2014</year>;<volume>158</volume>:<fpage>250</fpage>&#x02013;<lpage>62</lpage>.<pub-id pub-id-type="pmid">25036628</pub-id></mixed-citation></ref><ref id="bbv108-B29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fasham</surname><given-names>MJR</given-names></name></person-group>
<article-title>A comparison of nonmetric multidimensional scaling, principal components and reciprocal averaging for the ordination of simulated coenoclines, and coenoplanes</article-title>. <source>Ecology</source>
<year>1977</year>;<volume>58</volume>:<fpage>551</fpage>&#x02013;<lpage>61</lpage>.</mixed-citation></ref><ref id="bbv108-B30"><label>30</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beh</surname><given-names>EJ</given-names></name><name><surname>Lombardo</surname><given-names>R</given-names></name></person-group>
<source>Correspondence Analysis: Theory, Practice and New Strategies</source>, <publisher-loc>Hoboken</publisher-loc>: <publisher-name>John Wiley &#x00026; Sons</publisher-name>; <edition>1st edition</edition>
<year>2014</year>, <fpage>120</fpage>&#x02013;<lpage>76</lpage>.</mixed-citation></ref><ref id="bbv108-B31"><label>31</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Greenacre</surname><given-names>MJ</given-names></name></person-group>
<source>Theory and Applications of Correspondence Analysis</source>. <publisher-loc>Waltham, MA</publisher-loc>: <publisher-name>Academic Press</publisher-name>; <edition>3rd Edition</edition>
<year>1993</year>, <fpage>83</fpage>&#x02013;<lpage>125</lpage>.</mixed-citation></ref><ref id="bbv108-B32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fagan</surname><given-names>A</given-names></name><name><surname>Culhane</surname><given-names>AC</given-names></name><name><surname>Higgins</surname><given-names>DG</given-names></name></person-group>
<article-title>A multivariate analysis approach to the integration of proteomic and gene expression data</article-title>. <source>Proteomics</source>
<year>2007</year>;<volume>7</volume>:<fpage>2162</fpage>&#x02013;<lpage>71</lpage>.<pub-id pub-id-type="pmid">17549791</pub-id></mixed-citation></ref><ref id="bbv108-B33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fellenberg</surname><given-names>K</given-names></name><name><surname>Hauser</surname><given-names>NC</given-names></name><name><surname>Brors</surname><given-names>B</given-names></name><etal/></person-group>
<article-title>Correspondence analysis applied to microarray data</article-title>. <source>Proc Natl Acad Sci USA</source>
<year>2001</year>;<volume>98</volume>:<fpage>10781</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="pmid">11535808</pub-id></mixed-citation></ref><ref id="bbv108-B34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>DD</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group>
<article-title>Learning the parts of objects by non-negative matrix factorization</article-title>. <source>Nature</source>
<year>1999</year>;<volume>401</volume>:<fpage>788</fpage>&#x02013;<lpage>91</lpage>.<pub-id pub-id-type="pmid">10548103</pub-id></mixed-citation></ref><ref id="bbv108-B35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Comon</surname><given-names>P</given-names></name></person-group>
<article-title>Independent component analysis, a new concept?</article-title>
<source>Signal Processing</source>
<year>1994</year>;<volume>36</volume>:<fpage>287</fpage>&#x02013;<lpage>314</lpage>.</mixed-citation></ref><ref id="bbv108-B36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name></person-group>
<article-title>A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</article-title>. <source>Biostatistics</source>
<year>2009</year>;<volume>10</volume>:<fpage>515</fpage>&#x02013;<lpage>34</lpage>.<pub-id pub-id-type="pmid">19377034</pub-id></mixed-citation></ref><ref id="bbv108-B37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>H</given-names></name><name><surname>Huang</surname><given-names>JZ</given-names></name></person-group>
<article-title>Sparse principal component analysis via regularized low rank matrix approximation</article-title>. <source>J Multivar Anal</source>
<year>2008</year>;<volume>99</volume>:<fpage>1015</fpage>&#x02013;<lpage>34</lpage>.</mixed-citation></ref><ref id="bbv108-B38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Epstein</surname><given-names>MP</given-names></name><name><surname>Duncan</surname><given-names>R</given-names></name><etal/></person-group>
<article-title>Sparse principal component analysis for identifying ancestry-informative markers in genome-wide association studies</article-title>. <source>Genet Epidemiol</source>
<year>2012</year>;<volume>36</volume>:<fpage>293</fpage>&#x02013;<lpage>302</lpage>.<pub-id pub-id-type="pmid">22508067</pub-id></mixed-citation></ref><ref id="bbv108-B39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sill</surname><given-names>M</given-names></name><name><surname>Saadati</surname><given-names>M</given-names></name><name><surname>Benner</surname><given-names>A</given-names></name></person-group>
<article-title>Applying stability selection to consistently estimate sparse principal components in high-dimensional molecular data</article-title>. <source>Bioinformatics</source>
<year>2015</year>;<volume>31</volume>:<fpage>2683</fpage>&#x02013;<lpage>90</lpage>.<pub-id pub-id-type="pmid">25861969</pub-id></mixed-citation></ref><ref id="bbv108-B40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J</given-names></name></person-group>
<article-title>Efficient model selection for mixtures of probabilistic PCA via hierarchical BIC</article-title>. <source>IEEE Trans Cybern</source>
<year>2014</year>;<volume>44</volume>:<fpage>1871</fpage>&#x02013;<lpage>83</lpage>.<pub-id pub-id-type="pmid">25222728</pub-id></mixed-citation></ref><ref id="bbv108-B41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Q</given-names></name><name><surname>Shi</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><etal/></person-group>
<article-title>Integrative analysis of &#x02018;-omics&#x02019; data using penalty functions</article-title>. <source>Wiley Interdiscip Rev Comput Stat</source>
<year>2015</year>;<volume>7</volume>:<fpage>10</fpage>.</mixed-citation></ref><ref id="bbv108-B42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alter</surname><given-names>O</given-names></name><name><surname>Brown</surname><given-names>PO</given-names></name><name><surname>Botstein</surname><given-names>D</given-names></name></person-group>
<article-title>Generalized singular value decomposition for comparative analysis of genome-scale expression data sets of two different organisms</article-title>. <source>Proc Natl Acad Sci USA</source>
<year>2003</year>;<volume>100</volume>:<fpage>3351</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="pmid">12631705</pub-id></mixed-citation></ref><ref id="bbv108-B43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culhane</surname><given-names>AC</given-names></name><name><surname>Perriere</surname><given-names>G</given-names></name><name><surname>Higgins</surname><given-names>DG</given-names></name></person-group>
<article-title>Cross-platform comparison and visualisation of gene expression data using co-inertia analysis</article-title>. <source>BMC Bioinformatics</source>
<year>2003</year>;<volume>4</volume>:<fpage>59</fpage>.<pub-id pub-id-type="pmid">14633289</pub-id></mixed-citation></ref><ref id="bbv108-B44"><label>44</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dray</surname><given-names>S</given-names></name></person-group>
<article-title>Analysing a pair of tables: coinertia analysis and duality diagrams</article-title>. In: <person-group person-group-type="editor"><name><surname>J</surname><given-names>Blasius</given-names></name><name><surname>M</surname><given-names>Greenacre</given-names></name></person-group> (eds). <source>Visualization and verbalization of data</source>, <publisher-loc>Boca Raton</publisher-loc>: <publisher-name>CRC Press</publisher-name>, <year>2014</year>; <fpage>289</fpage>&#x02013;<lpage>300</lpage>.</mixed-citation></ref><ref id="bbv108-B45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Cao</surname><given-names>KA</given-names></name><name><surname>Martin</surname><given-names>PG</given-names></name><name><surname>Robert-Granie</surname><given-names>C</given-names></name><etal/></person-group>
<article-title>Sparse canonical methods for biological data integration: application to a cross-platform study</article-title>. <source>BMC Bioinformatics</source>
<year>2009</year>;<volume>10</volume>:<fpage>34</fpage>.<pub-id pub-id-type="pmid">19171069</pub-id></mixed-citation></ref><ref id="bbv108-B46"><label>46</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braak</surname><given-names>CJF</given-names></name></person-group>
<article-title>Canonical correspondence analysis: a new eigenvector technique for multivariate direct gradient analysis</article-title>. <source>Ecology</source>
<year>1986</year>;<volume>67</volume>:<fpage>1167</fpage>&#x02013;<lpage>79</lpage>.</mixed-citation></ref><ref id="bbv108-B47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotelling</surname><given-names>H</given-names></name></person-group>
<article-title>Relations between two sets of variates</article-title>. <source>Biometrika</source>
<year>1936</year>;<volume>28</volume>:<fpage>321</fpage>&#x02013;<lpage>77</lpage>.</mixed-citation></ref><ref id="bbv108-B48"><label>48</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>McGarigal</surname><given-names>K</given-names></name><name><surname>Landguth</surname><given-names>E</given-names></name><name><surname>Stafford</surname><given-names>S</given-names></name></person-group>
<source>Multivariate Statistics for Wildlife and Ecology Research</source>, <publisher-loc>New York</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2002</year>.</mixed-citation></ref><ref id="bbv108-B49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thioulouse</surname><given-names>J</given-names></name></person-group>
<article-title>Simultaneous analysis of a sequence of paired ecological tables: a comparison of several methods</article-title>. <source>Ann Appl Stat</source>
<year>2011</year>;<volume>5</volume>:<fpage>2300</fpage>&#x02013;<lpage>25</lpage>.</mixed-citation></ref><ref id="bbv108-B50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Jin</surname><given-names>L</given-names></name><etal/></person-group>
<article-title>Canonical correlation analysis for RNA-seq co-expression networks</article-title>. <source>Nucleic Acids Res</source>
<year>2013</year>;<volume>41</volume>:<fpage>e95</fpage>.<pub-id pub-id-type="pmid">23460206</pub-id></mixed-citation></ref><ref id="bbv108-B51"><label>51</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mardia</surname><given-names>KV</given-names></name><name><surname>Kent</surname><given-names>JT</given-names></name><name><surname>Bibby</surname><given-names>JM</given-names></name></person-group>
<comment><italic>Multivariate Analysis</italic>. London, New York, NY, Toronto, Sydney, San Francisco, CA: Academic Press, 1979</comment>.</mixed-citation></ref><ref id="bbv108-B52"><label>52</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waaijenborg</surname><given-names>S</given-names></name><name><surname>Zwinderman</surname><given-names>AH</given-names></name></person-group>
<article-title>Sparse canonical correlation analysis for identifying, connecting and completing gene-expression networks</article-title>. <source>BMC Bioinformatics</source>
<year>2009</year>;<volume>10</volume>:<fpage>315</fpage>.<pub-id pub-id-type="pmid">19785734</pub-id></mixed-citation></ref><ref id="bbv108-B53"><label>53</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkhomenko</surname><given-names>E</given-names></name><name><surname>Tritchler</surname><given-names>D</given-names></name><name><surname>Beyene</surname><given-names>J</given-names></name></person-group>
<article-title>Sparse canonical correlation analysis with application to genomic data integration</article-title>. <source>Stat Appl Genet Mol Biol</source>
<year>2009</year>;<volume>8</volume>:<fpage>article 1</fpage>.<pub-id pub-id-type="pmid">19222376</pub-id></mixed-citation></ref><ref id="bbv108-B54"><label>54</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Tibshirani</surname><given-names>RJ</given-names></name></person-group>
<article-title>Extensions of sparse canonical correlation analysis with applications to genomic data</article-title>. <source>Stat Appl Genet Mol Biol</source>
<year>2009</year>;<volume>8</volume>:<fpage>article 28</fpage>.</mixed-citation></ref><ref id="bbv108-B55"><label>55</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><etal/></person-group>
<article-title>Group sparse canonical correlation analysis for genomic data integration</article-title>. <source>BMC Bioinformatics</source>
<year>2013</year>;<volume>14</volume>:<fpage>245</fpage>.<pub-id pub-id-type="pmid">23937249</pub-id></mixed-citation></ref><ref id="bbv108-B56"><label>56</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Cao</surname><given-names>KA</given-names></name><name><surname>Boitard</surname><given-names>S</given-names></name><name><surname>Besse</surname><given-names>P</given-names></name></person-group>
<article-title>Sparse PLS discriminant analysis: biologically relevant feature selection and graphical displays for multiclass problems</article-title>. <source>BMC Bioinform</source>
<year>2011</year>;<volume>12</volume>:<fpage>253</fpage>.</mixed-citation></ref><ref id="bbv108-B57"><label>57</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boulesteix</surname><given-names>AL</given-names></name><name><surname>Strimmer</surname><given-names>K</given-names></name></person-group>
<article-title>Partial least squares: a versatile tool for the analysis of high-dimensional genomic data</article-title>. <source>Brief Bioinform</source>
<year>2007</year>;<volume>8</volume>:<fpage>32</fpage>&#x02013;<lpage>44</lpage>.<pub-id pub-id-type="pmid">16772269</pub-id></mixed-citation></ref><ref id="bbv108-B58"><label>58</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dol&#x000e9;dec</surname><given-names>S</given-names></name><name><surname>Chessel</surname><given-names>D</given-names></name></person-group>
<article-title>Co-inertia analysis: an alternative method for studying species&#x02013;environment relationships</article-title>. <source>Freshwater Biol</source>
<year>1994</year>;<volume>31</volume>:<fpage>277</fpage>&#x02013;<lpage>94</lpage>.</mixed-citation></ref><ref id="bbv108-B59"><label>59</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponnapalli</surname><given-names>SP</given-names></name><name><surname>Saunders</surname><given-names>MA</given-names></name><name><surname>Van Loan</surname><given-names>CF</given-names></name><etal/></person-group>
<article-title>A higher-order generalized singular value decomposition for comparison of global mRNA expression from multiple organisms</article-title>. <source>PLoS One</source>
<year>2011</year>;<volume>6</volume>:<fpage>e28072</fpage>.<pub-id pub-id-type="pmid">22216090</pub-id></mixed-citation></ref><ref id="bbv108-B60"><label>60</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meng</surname><given-names>C</given-names></name><name><surname>Kuster</surname><given-names>B</given-names></name><name><surname>Culhane</surname><given-names>AC</given-names></name><etal/></person-group>
<article-title>A multivariate approach to the integration of multi-omics datasets</article-title>. <source>BMC Bioinformatics</source>
<year>2014</year>;<volume>15</volume>:<fpage>162</fpage>.<pub-id pub-id-type="pmid">24884486</pub-id></mixed-citation></ref><ref id="bbv108-B61"><label>61</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Tayrac</surname><given-names>M</given-names></name><name><surname>Le</surname><given-names>S</given-names></name><name><surname>Aubry</surname><given-names>M</given-names></name><etal/></person-group>
<article-title>Simultaneous analysis of distinct Omics data sets with integration of biological knowledge: multiple factor analysis approach</article-title>. <source>BMC Genomics</source>
<year>2009</year>;<volume>10</volume>:<fpage>32</fpage>.<pub-id pub-id-type="pmid">19154582</pub-id></mixed-citation></ref><ref id="bbv108-B62"><label>62</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Williams</surname><given-names>LJ</given-names></name><name><surname>Valentin</surname><given-names>D</given-names></name></person-group>
<source>Statis and Distatis: Optimum Multitable Principal Component Analysis and Three Way Metric Multidimensional Scaling</source>. <publisher-name>Wiley Interdisciplinary</publisher-name>, <year>2012</year>;<volume>4</volume>:<fpage>124</fpage>&#x02013;<lpage>67</lpage>.</mixed-citation></ref><ref id="bbv108-B63"><label>63</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Jong</surname><given-names>S</given-names></name><name><surname>Kiers</surname><given-names>HAL</given-names></name></person-group>
<article-title>Principal covariates regression: Part I Theory</article-title>. <source>Chemometrics and Intelligent Laboratory Systems</source>
<year>1992</year>;<volume>14</volume>:<fpage>155</fpage>&#x02013;<lpage>64</lpage>.</mixed-citation></ref><ref id="bbv108-B64"><label>64</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>B&#x000e9;nass&#x000e9;ni</surname><given-names>J</given-names></name><name><surname>Dosse</surname><given-names>MB</given-names></name></person-group>
<article-title>Analyzing multiset data by the power STATIS-ACT method</article-title>. <source>Adv Data Anal Classification</source>
<year>2012</year>;<volume>6</volume>:<fpage>49</fpage>&#x02013;<lpage>65</lpage>.</mixed-citation></ref><ref id="bbv108-B65"><label>65</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escoufier</surname><given-names>Y</given-names></name></person-group>
<article-title>The duality diagram: a means for better practical applications</article-title>. <source>Devel Num Ecol</source>
<year>1987</year>;<volume>14</volume>:<fpage>139</fpage>&#x02013;<lpage>56</lpage>.</mixed-citation></ref><ref id="bbv108-B66"><label>66</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordani</surname><given-names>P</given-names></name><name><surname>Kiers</surname><given-names>HAL</given-names></name><name><surname>Ferraro</surname><given-names>DMA</given-names></name></person-group>
<article-title>Three-way component analysis using the R package threeway</article-title>. <source>J Stat Software</source>
<year>2014</year>;<volume>57</volume>:<fpage>7</fpage>.</mixed-citation></ref><ref id="bbv108-B67"><label>67</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leibovici</surname><given-names>DG</given-names></name></person-group>
<article-title>Spatio-temporal multiway decompositions using principal tensor analysis on k-modes: the R package PTAk</article-title>. <source>J Stat Software</source>
<year>2010</year>;<volume>34</volume>(<issue>10</issue>):<fpage>1</fpage>&#x02013;<lpage>34</lpage></mixed-citation></ref><ref id="bbv108-B68"><label>68</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolda</surname><given-names>TG</given-names></name><name><surname>Bader</surname><given-names>BW</given-names></name></person-group>
<article-title>Tensor decompositions and applications</article-title>. <source>SIAM Rev</source>
<year>2009</year>;<volume>51</volume>:<fpage>455</fpage>&#x02013;<lpage>500</lpage>.</mixed-citation></ref><ref id="bbv108-B69"><label>69</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenhaus</surname><given-names>A</given-names></name><name><surname>Tenenhaus</surname><given-names>M</given-names></name></person-group>
<article-title>Regularized generalized canonical correlation analysis for multiblock or multigroup data analysis</article-title>. <source>Eur J Oper Res</source>
<year>2014</year>;<volume>238</volume>:<fpage>391</fpage>&#x02013;<lpage>403</lpage>.</mixed-citation></ref><ref id="bbv108-B70"><label>70</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chessel</surname><given-names>D</given-names></name><name><surname>Hanafi</surname><given-names>M</given-names></name></person-group>
<article-title>Analyses de la co-inertie de K nuages de points</article-title>. <source>Rev Stat Appl</source>
<year>1996</year>;<volume>44</volume>:<fpage>35</fpage>&#x02013;<lpage>60</lpage>.</mixed-citation></ref><ref id="bbv108-B71"><label>71</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanafi</surname><given-names>M</given-names></name><name><surname>Kohler</surname><given-names>A</given-names></name><name><surname>Qannari</surname><given-names>EM</given-names></name></person-group>
<article-title>Connections between multiple co-inertia analysis and consensus principal component analysis</article-title>. <source>Chemometrics and Intelligent Laboratory</source>, <year>2011</year>;<volume>106</volume>(<issue>1</issue>):<fpage>37</fpage>&#x02013;<lpage>40</lpage>.</mixed-citation></ref><ref id="bbv108-B72"><label>72</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carroll</surname><given-names>JD</given-names></name></person-group>
<article-title>Generalization of canonical correlation analysis to three or more sets of variables</article-title>. <source>Proc. 76th Convent. Am. Psych. Assoc.</source>
<year>1968</year>;<volume>3</volume>:<fpage>227</fpage>&#x02013;<lpage>8</lpage>.</mixed-citation></ref><ref id="bbv108-B73"><label>73</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takane</surname><given-names>Y</given-names></name><name><surname>Hwang</surname><given-names>H</given-names></name><name><surname>Abdi</surname><given-names>H</given-names></name></person-group>
<article-title>Regularized multiple-set canonical correlation analysis</article-title>. <source>Psychometrika</source>
<year>2008</year>;<volume>73</volume>:<fpage>753</fpage>&#x02013;<lpage>75</lpage>.</mixed-citation></ref><ref id="bbv108-B74"><label>74</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenhaus</surname><given-names>A</given-names></name><name><surname>Tenenhaus</surname><given-names>M</given-names></name></person-group>
<article-title>Regularized generalized canonical correlation analysis</article-title>. <source>Psychometrika</source>
<year>2011</year>;<volume>76</volume>:<fpage>257</fpage>&#x02013;<lpage>84</lpage>.</mixed-citation></ref><ref id="bbv108-B75"><label>75</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>van de Velden</surname><given-names>M</given-names></name></person-group>
<article-title>On generalized canonical correlation analysis</article-title>. In: <source>Proceedings of the 58th World Statistical Congress</source>, <publisher-loc>Dublin</publisher-loc>, <year>2011</year>.</mixed-citation></ref><ref id="bbv108-B76"><label>76</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenhaus</surname><given-names>A</given-names></name><name><surname>Philippe</surname><given-names>C</given-names></name><name><surname>Guillemot</surname><given-names>V</given-names></name><etal/></person-group>
<article-title>Variable selection for generalized canonical correlation analysis</article-title>. <source>Biostatistics</source>
<year>2014</year>;<volume>15</volume>:<fpage>569</fpage>&#x02013;<lpage>83</lpage>.<pub-id pub-id-type="pmid">24550197</pub-id></mixed-citation></ref><ref id="bbv108-B77"><label>77</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Q</given-names></name><name><surname>Caiafa</surname><given-names>CF</given-names></name><name><surname>Mandic</surname><given-names>DP</given-names></name><etal/></person-group>
<article-title>Higher order partial least squares (HOPLS): a generalized multilinear regression method</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>
<year>2013</year>;<volume>35</volume>:<fpage>1660</fpage>&#x02013;<lpage>73</lpage>.<pub-id pub-id-type="pmid">23681994</pub-id></mixed-citation></ref><ref id="bbv108-B78"><label>78</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Eld&#x000e9;n</surname><given-names>L</given-names></name></person-group>
<comment>Non-negative tensor factorization based on alternating large-scale non-negativity-constrained least squares, In: <italic>Proceedings of the 7th IEEE Symposium on Bioinformatics and Bioengineering (BIBE)</italic>; Dublin 2007: 1147&#x02013;51</comment>.</mixed-citation></ref><ref id="bbv108-B79"><label>79</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morup</surname><given-names>M</given-names></name><name><surname>Hansen</surname><given-names>LK</given-names></name><name><surname>Arnfred</surname><given-names>SM</given-names></name></person-group>
<article-title>Algorithms for sparse nonnegative Tucker decompositions</article-title>. <source>Neural Comput</source>
<year>2008</year>;<volume>20</volume>:<fpage>2112</fpage>&#x02013;<lpage>31</lpage>.<pub-id pub-id-type="pmid">18386984</pub-id></mixed-citation></ref><ref id="bbv108-B80"><label>80</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>HQ</given-names></name><name><surname>Zheng</surname><given-names>CH</given-names></name><name><surname>Zhao</surname><given-names>XM</given-names></name></person-group>
<article-title>jNMFMA: a joint non-negative matrix factorization meta-analysis of transcriptomics data</article-title>. <source>Bioinformatics</source>
<year>2015</year>;<volume>31</volume>:<fpage>572</fpage>&#x02013;<lpage>80</lpage>.<pub-id pub-id-type="pmid">25411328</pub-id></mixed-citation></ref><ref id="bbv108-B81"><label>81</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>CC</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><etal/></person-group>
<article-title>Discovery of multi-dimensional modules by integrative analysis of cancer genomic data</article-title>. <source>Nucleic Acids Res</source>
<year>2012</year>;<volume>40</volume>:<fpage>9379</fpage>&#x02013;<lpage>91</lpage>.<pub-id pub-id-type="pmid">22879375</pub-id></mixed-citation></ref><ref id="bbv108-B82"><label>82</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lv</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Jia</surname><given-names>H</given-names></name><etal/></person-group>
<article-title>An oncogenic role of miR-142-3p in human T-cell acute lymphoblastic leukemia (T-ALL) by targeting glucocorticoid receptor-alpha and cAMP/PKA pathways</article-title>. <source>Leukemia</source>
<year>2012</year>;<volume>26</volume>:<fpage>769</fpage>&#x02013;<lpage>77</lpage>.<pub-id pub-id-type="pmid">21979877</pub-id></mixed-citation></ref><ref id="bbv108-B83"><label>83</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulikkan</surname><given-names>JA</given-names></name><name><surname>Dengler</surname><given-names>V</given-names></name><name><surname>Peramangalam</surname><given-names>PS</given-names></name><etal/></person-group>
<article-title>Cell-cycle regulator E2F1 and microRNA-223 comprise an autoregulatory negative feedback loop in acute myeloid leukemia</article-title>. <source>Blood</source>
<year>2010</year>;<volume>115</volume>:<fpage>1768</fpage>&#x02013;<lpage>78</lpage>.<pub-id pub-id-type="pmid">20029046</pub-id></mixed-citation></ref><ref id="bbv108-B84"><label>84</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smilde</surname><given-names>AK</given-names></name><name><surname>Kiers</surname><given-names>HA</given-names></name><name><surname>Bijlsma</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Matrix correlations for high-dimensional data: the modified RV-coefficient</article-title>. <source>Bioinformatics</source>
<year>2009</year>;<volume>25</volume>:<fpage>401</fpage>&#x02013;<lpage>5</lpage>.<pub-id pub-id-type="pmid">19073588</pub-id></mixed-citation></ref><ref id="bbv108-B85"><label>85</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Streicher</surname><given-names>KL</given-names></name><name><surname>Zhu</surname><given-names>W</given-names></name><name><surname>Lehmann</surname><given-names>KP</given-names></name><etal/></person-group>
<article-title>A novel oncogenic role for the miRNA-506-514 cluster in initiating melanocyte transformation and promoting melanoma growth</article-title>. <source>Oncogene</source>
<year>2012</year>;<volume>31</volume>:<fpage>1558</fpage>&#x02013;<lpage>70</lpage>.<pub-id pub-id-type="pmid">21860416</pub-id></mixed-citation></ref><ref id="bbv108-B86"><label>86</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiaretti</surname><given-names>S</given-names></name><name><surname>Messina</surname><given-names>M</given-names></name><name><surname>Tavolaro</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Gene expression profiling identifies a subset of adult T-cell acute lymphoblastic leukemia with myeloid-like gene features and over-expression of miR-223</article-title>. <source>Haematologica</source>
<year>2010</year>;<volume>95</volume>:<fpage>1114</fpage>&#x02013;<lpage>21</lpage>.<pub-id pub-id-type="pmid">20418243</pub-id></mixed-citation></ref><ref id="bbv108-B87"><label>87</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dahlhaus</surname><given-names>M</given-names></name><name><surname>Roolf</surname><given-names>C</given-names></name><name><surname>Ruck</surname><given-names>S</given-names></name><etal/></person-group>
<article-title>Expression and prognostic significance of hsa-miR-142-3p in acute leukemias</article-title>. <source>Neoplasma</source>
<year>2013</year>;<volume>60</volume>:<fpage>432</fpage>&#x02013;<lpage>8</lpage>.<pub-id pub-id-type="pmid">23581416</pub-id></mixed-citation></ref><ref id="bbv108-B88"><label>88</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eyholzer</surname><given-names>M</given-names></name><name><surname>Schmid</surname><given-names>S</given-names></name><name><surname>Schardt</surname><given-names>JA</given-names></name><etal/></person-group>
<article-title>Complexity of miR-223 regulation by CEBPA in human AML</article-title>. <source>Leuk Res</source>
<year>2010</year>;<volume>34</volume>:<fpage>672</fpage>&#x02013;<lpage>6</lpage>.<pub-id pub-id-type="pmid">20018373</pub-id></mixed-citation></ref><ref id="bbv108-B89"><label>89</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>V</given-names></name><name><surname>Palermo</surname><given-names>R</given-names></name><name><surname>Talora</surname><given-names>C</given-names></name><etal/></person-group>
<article-title>Notch and NF-kB signaling pathways regulate miR-223/FBXW7 axis in T-cell acute lymphoblastic leukemia</article-title>. <source>Leukemia</source>
<year>2014</year>;<volume>28</volume>:<fpage>2324</fpage>&#x02013;<lpage>35</lpage>.<pub-id pub-id-type="pmid">24727676</pub-id></mixed-citation></ref><ref id="bbv108-B90"><label>90</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Josson</surname><given-names>S</given-names></name><name><surname>Gururajan</surname><given-names>M</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><etal/></person-group>
<article-title>miR-409-3p/-5p promotes tumorigenesis, epithelial-to-mesenchymal transition, and bone metastasis of human prostate cancer</article-title>. <source>Clin Cancer Res</source>
<year>2014</year>;<volume>20</volume>:<fpage>4636</fpage>&#x02013;<lpage>46</lpage>.<pub-id pub-id-type="pmid">24963047</pub-id></mixed-citation></ref><ref id="bbv108-B91"><label>91</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alizadeh</surname><given-names>AA</given-names></name><name><surname>Aranda</surname><given-names>V</given-names></name><name><surname>Bardelli</surname><given-names>A</given-names></name><etal/></person-group>
<article-title>Toward understanding and exploiting tumor heterogeneity</article-title>. <source>Nat Med</source>
<year>2015</year>;<volume>21</volume>:<fpage>846</fpage>&#x02013;<lpage>53</lpage>.<pub-id pub-id-type="pmid">26248267</pub-id></mixed-citation></ref><ref id="bbv108-B92"><label>92</label><mixed-citation publication-type="journal"><collab>Cancer Genome Atlas Research Network</collab>, <person-group person-group-type="author"><name><surname>Weinstein</surname><given-names>JN</given-names></name><name><surname>Collisson</surname><given-names>EA</given-names></name><name><surname>Mills</surname><given-names>GB</given-names></name><etal/></person-group>
<article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title>. <source>Nat Genet</source>
<year>2013</year>;<volume>45</volume>:<fpage>1113</fpage>&#x02013;<lpage>20</lpage>.<pub-id pub-id-type="pmid">24071849</pub-id></mixed-citation></ref><ref id="bbv108-B93"><label>93</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gholami</surname><given-names>AM</given-names></name><name><surname>Fellenberg</surname><given-names>K</given-names></name></person-group>
<article-title>Cross-species common regulatory network inference without requirement for prior gene affiliation</article-title>. <source>Bioinformatics</source>
<year>2010</year>;<volume>26</volume>(<issue>8</issue>):<fpage>1082</fpage>&#x02013;<lpage>90</lpage>.<pub-id pub-id-type="pmid">20200011</pub-id></mixed-citation></ref><ref id="bbv108-B94"><label>94</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langfelder</surname><given-names>P</given-names></name><name><surname>Horvath</surname><given-names>S</given-names></name></person-group>
<article-title>WGCNA: an R package for weighted correlation network analysis</article-title>. <source>BMC Bioinformatics</source>
<year>2008</year>;<volume>9</volume>:<fpage>559</fpage>.<pub-id pub-id-type="pmid">19114008</pub-id></mixed-citation></ref></ref-list></back></article>