<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id><journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id><journal-title-group><journal-title>BMC Bioinformatics</journal-title></journal-title-group><issn pub-type="epub">1471-2105</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4355996</article-id><article-id pub-id-type="pmid">25886892</article-id><article-id pub-id-type="publisher-id">443</article-id><article-id pub-id-type="doi">10.1186/s12859-014-0443-6</article-id><article-categories><subj-group subj-group-type="heading"><subject>Methodology Article</subject></subj-group></article-categories><title-group><article-title>The application of sparse estimation of covariance matrix to quadratic discriminant analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Jiehuan</given-names></name><address><email>jiehuan.sun@yale.edu</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhao</surname><given-names>Hongyu</given-names></name><address><email>hongyu.zhao@yale.edu</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1">Department of Biostatitics, Yale School of Publich Health, 60 College Street, New Haven, 06511 CT USA </aff></contrib-group><pub-date pub-type="epub"><day>18</day><month>2</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>16</volume><elocation-id>48</elocation-id><history><date date-type="received"><day>30</day><month>9</month><year>2014</year></date><date date-type="accepted"><day>3</day><month>12</month><year>2014</year></date></history><permissions><copyright-statement>&#x000a9; Sun and Zhao; licensee BioMed Central. 2015</copyright-statement><license license-type="open-access"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/2.0">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p>Although Linear Discriminant Analysis (LDA) is commonly used for classification, it may not be directly applied in genomics studies due to the large <italic>p</italic>, small <italic>n</italic> problem in these studies. Different versions of sparse LDA have been proposed to address this significant challenge. One implicit assumption of various LDA-based methods is that the covariance matrices are the same across different classes. However, rewiring of genetic networks (therefore different covariance matrices) across different diseases has been observed in many genomics studies, which suggests that LDA and its variations may be suboptimal for disease classifications. However, it is not clear whether considering differing genetic networks across diseases can improve classification in genomics studies.</p></sec><sec><title>Results</title><p>We propose a sparse version of Quadratic Discriminant Analysis (SQDA) to explicitly consider the differences of the genetic networks across diseases. Both simulation and real data analysis are performed to compare the performance of SQDA with six commonly used classification methods.</p></sec><sec><title>Conclusions</title><p>SQDA provides more accurate classification results than other methods for both simulated and real data. Our method should prove useful for classification in genomics studies and other research settings, where covariances differ among classes.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Quadratic discriminant analysis</kwd><kwd>Sparse estimation of covariance matrix</kwd><kwd>Classification</kwd><kwd>Genomics</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2015</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p>Although Linear Discriminant Analysis (LDA) is commonly used for classification, it cannot be directly applied in the large p, small n setting, where the number of samples is far less than the number of features [<xref ref-type="bibr" rid="CR1">1</xref>]. This is due to the singularity of the sample covariance matrix.</p><p>One straightforward solution for the matrix singularity problem is to use the generalized inverse, e.g. the Moore-Penrose pseudo-inverse, as mentioned in [<xref ref-type="bibr" rid="CR2">2</xref>-<xref ref-type="bibr" rid="CR4">4</xref>] in deriving the LDA rule. Alternatively, several authors have proposed modified estimators of the covariance matrix to address the singularity problem, among which the most common approach is to shrink the sample covariance matrix towards a well-behaved matrix. For example, penalized discriminant analysis was proposed in [<xref ref-type="bibr" rid="CR5">5</xref>], where the sample covariance matrix is shrunken towards the identity matrix. It has also been proposed to shrink the sample covariance matrix towards a common diagonal covariance matrix or simply use the diagonal covariance matrix [<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref>]. The relative performance of using sample covariance matrix, common covariance matrix, diagonal sample covariance matrix, and diagonal common covariance matrix as an estimator for the covariance matrix in discriminant analysis was studied in [<xref ref-type="bibr" rid="CR6">6</xref>]. Another version of the penalized discriminant analysis was proposed in [<xref ref-type="bibr" rid="CR8">8</xref>], where the shrunken estimator of covariance matrix proposed in [<xref ref-type="bibr" rid="CR5">5</xref>] was combined with the application of &#x0201c;nearest shrunken centroids&#x0201d; proposed in [<xref ref-type="bibr" rid="CR9">9</xref>] to estimate the sample means. A modified linear discriminant analysis was later proposed in [<xref ref-type="bibr" rid="CR10">10</xref>], where a well-conditioned estimator for high-dimensional covariance matrix proposed in [<xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref>] was used. Other authors have addressed the singularity problem through the eigenvalue decomposition by discarding small eigenvalues [<xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR13">13</xref>]. Most recently, progress on sparse estimation of covariance matrix and precision matrix offers another approach for LDA in the large p, small n setting where sparsity assumption is imposed on the covariance matrix or precision matrix. This class of methods is called Sparse LDA, where there are different ways of imposing sparsity on the covariance matrix and/or mean difference through different optimization methods [<xref ref-type="bibr" rid="CR14">14</xref>-<xref ref-type="bibr" rid="CR16">16</xref>].</p><p>Quadratic discriminant analysis (QDA) is closely related to LDA, where each class is assumed to have its own covariance matrix. This is in contrast to LDA where the covariance matrices for different classes are assumed to be the same. For many practical problems, the assumption of common covariance matrix across different classes is inappropriate. For example, in the case of using gene expression data to distinguish tumor samples from normal controls, or to distinguish tissue types, many studies have shown that the correlation patterns among genes do differ between cancer and normal samples and among tissues. In fact, such rewiring of genetic networks in patients and across tissues are common and sometimes offer valuable information for classifications [<xref ref-type="bibr" rid="CR17">17</xref>-<xref ref-type="bibr" rid="CR19">19</xref>]. These observations suggest that the covariance or precision matrices of different classes are different and QDA may be more appropriate than LDA for classifications in these settings.</p><p>Although many efforts have been made to improve LDA in high-dimensional settings, as far as we are aware, relatively little has been done to improve QDA for the large p, small n problem. This is partly due to the fact that it is already challenging to estimate the covariance or precision matrix in the high-dimensional setting when all the classes are assumed to have the same form for LDA, it will be more difficult to estimate covariances in QDA, since more parameters need to be estimated where each class is allowed to have a different covariance matrix. In this paper, we propose a novel QDA based classification method that estimates separate sparse covariance matrices for different classes, called SQDA, and compare its performance with existing methods, including diagonal linear discriminant analysis (DLDA), diagonal quadratic discriminant analysis (DQDA), regularized discriminant analysis with shrunken centroids (SCRDA), nearest neighbor (NN) [<xref ref-type="bibr" rid="CR20">20</xref>], support vector machine (SVM) [<xref ref-type="bibr" rid="CR21">21</xref>], and random forests (RF) [<xref ref-type="bibr" rid="CR22">22</xref>], where NN, SVM and RF are commonly used in genomics studies. The authors of [<xref ref-type="bibr" rid="CR23">23</xref>] proposed a related method on sparse QDA, in which joint estimation of precision matrices for different classes is applied to each block determined by hierarchical clustering based on the correlation matrices, where block diagonal structure is imposed on the covariance matrices. However, their method is computationally more involved in high-dimensional settings and hence we didn&#x02019;t include their method in the comparisons.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Simulations</title><p>For our proposed method, the covariance matrices for different classes are assumed to be block-diagonal with each block having the same size and variable selection by blocks is performed. For block-diagonal matrices, the block size has to be determined. For variable selection by blocks, the blocks with cross validation errors exceeding the smallest error among all blocks by predefined error margin will be excluded.</p><p>In this section, we study the effects of block size, error margin, and sample size on our method. We also compare the performance of our method with other classification methods under different simulation settings. The detailed simulations can be found in the Materials and Methods section.</p><sec id="Sec4"><title>Simulation settings</title><p>In the first simulation setting, Independent Structure Same Covariance (ISSC), the covariance matrices are the same and diagonal for all classes. We let the number of genes be 10,000 and generate 100 training samples consisting of 50 &#x0201c;tumor&#x0201d; and 50 &#x0201c;normal&#x0201d; samples. The gene expression values in the tumor samples are drawn from <italic>N</italic>(<italic>&#x003bc;</italic>
<sub>1</sub>,<italic>&#x003a3;</italic>
<sub>1</sub>) and those in the normal samples are drawn from <italic>N</italic>(<italic>&#x003bc;</italic>
<sub>2</sub>,<italic>&#x003a3;</italic>
<sub>2</sub>), where <italic>&#x003a3;</italic>
<sub>1</sub>=<italic>&#x003a3;</italic>
<sub>2</sub>=<italic>I</italic> are the identity matrix. We assume that the first 400 elements in <italic>&#x003bc;</italic>
<sub>1</sub> are 0.5 and the rest are 0 while all elements in <italic>&#x003bc;</italic>
<sub>2</sub> are 0. The testing dataset has 500 tumor samples and 500 normal samples.</p><p>In the second simulation setting, Independent Structure Different Covariance (ISDC), the set-up is the same as above except that <italic>&#x003a3;</italic>
<sub>1</sub> is changed from the identity matrix with the first 400 diagonal elements replaced by 1.5.</p><p>The third simulation setting, Dependent Structure Same Covariance (DSSC), puts some dependences on the genes. More specifically, <italic>&#x003a3;</italic>
<sub>1</sub> and <italic>&#x003a3;</italic>
<sub>2</sub> have the same blockwise autoregression correlation structure <italic>&#x003a3;</italic>
<disp-formula id="Equ1"><label>(1)</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ {\fontsize{9.5pt}{9.6pt}\selectfont{\begin{aligned} \Sigma= \left(\begin{array}{cccccc} \Sigma_{\rho} &#x00026; 0 &#x00026; 0 &#x00026;\cdots&#x00026;\cdots&#x00026;\cdots \\ 0 &#x00026; \Sigma_{-\rho} &#x00026; 0 &#x00026;0&#x00026;\cdots&#x00026;\vdots \\ 0 &#x00026; 0 &#x00026;\Sigma_{\rho} &#x00026;0 &#x00026;\cdots&#x00026;\vdots\\ \vdots &#x00026;0&#x00026; 0 &#x00026;\Sigma_{-\rho} &#x00026;0 &#x00026;\vdots\\ \vdots &#x00026;\vdots&#x00026; \vdots &#x00026;0 &#x00026;\ddots &#x00026;\vdots\\ \cdots &#x00026;\cdots&#x00026; \cdots &#x00026;\cdots &#x00026;\cdots &#x00026;\cdots\\ \end{array} \right)_{10,000\times 10,000} \end{aligned}}}  $$
\end{document}</tex-math><mml:math id="M2"><mml:mtable><mml:mtr><mml:mtd><mml:mi>&#x003a3;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd></mml:mtr><mml:mtr/></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2014_443_Equ1.gif" position="anchor"/></alternatives></disp-formula>
</p><p>and
<disp-formula id="Equ2"><label>(2)</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ {\fontsize{9.7pt}{9.6pt}\selectfont{\begin{aligned} \Sigma_{\rho}= \left(\begin{array}{ccccc} 1&#x00026; \rho&#x00026; \cdots&#x00026;\rho^{198}&#x00026;\rho^{199} \\ \rho&#x00026; 1 &#x00026; \ddots &#x00026;\cdots&#x00026;\rho^{198} \\ \vdots &#x00026;\ddots&#x00026; \ddots &#x00026;\ddots &#x00026;\vdots\\ \rho^{198} &#x00026;\cdots&#x00026; \ddots &#x00026;\ddots &#x00026;\rho \\ \rho^{199}&#x00026;\rho^{198}&#x00026; \cdots &#x00026;\rho&#x00026;1\\ \end{array} \right)_{200\times 200} \end{aligned}}}  $$
\end{document}</tex-math><mml:math id="M4"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mi>&#x003c1;</mml:mi></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow><mml:mrow><mml:mn>198</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow><mml:mrow><mml:mn>199</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>&#x003c1;</mml:mi></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow><mml:mrow><mml:mn>198</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022ee;</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow><mml:mrow><mml:mn>198</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mo>&#x022f1;</mml:mo></mml:mtd><mml:mtd><mml:mi>&#x003c1;</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow><mml:mrow><mml:mn>199</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:msup><mml:mrow><mml:mi>&#x003c1;</mml:mi></mml:mrow><mml:mrow><mml:mn>198</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd><mml:mo>&#x022ef;</mml:mo></mml:mtd><mml:mtd><mml:mi>&#x003c1;</mml:mi></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr/></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>200</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2014_443_Equ2.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where <italic>&#x003c1;</italic>=0.95 and <italic>&#x003a3;</italic>
<sub>&#x02212;<italic>&#x003c1;</italic></sub> has the same structure as <italic>&#x003a3;</italic>
<sub><italic>&#x003c1;</italic></sub>, but with -0.95 for <italic>&#x003c1;</italic>.</p><p>In the fourth simulation setting, Dependent Structure Different Covariance (DSDC), <italic>&#x003a3;</italic>
<sub>1</sub> and <italic>&#x003a3;</italic>
<sub>2</sub> are the blockwise autoregression correlation structure as in Equation (<xref rid="Equ1" ref-type="">1</xref>) except that the order of <italic>&#x003a3;</italic>
<sub><italic>&#x003c1;</italic></sub> and <italic>&#x003a3;</italic>
<sub>&#x02212;<italic>&#x003c1;</italic></sub> is reversed in <italic>&#x003a3;</italic>
<sub>2</sub> for the first two blocks.</p><p>For each simulation setting, we generate 10 datasets and the misclassification rates are reported as the average of the errors on the 10 testing datasets. The standard deviations of the misclassification rates are also reported together with the number of features selected for classification.</p></sec><sec id="Sec5"><title>Effects of block size and error margin</title><p>We study the effects of two important parameters, block size and error margin, on the performance of SQDA.</p><p>For the fourth simulation setting DSDC, we compare the performance of SQDA under different block sizes and error margins. The block sizes are varied from 100, 150, 200, 250, 300, 350, to 400, and the error margins are varied from 0.05, 0.10, to 0.15. The average misclassification rates under different combinations of block sizes and error margins are shown in Figure <xref rid="Fig1" ref-type="fig">1</xref>. We can see that when the block size is 100, the misclassification rates are consistently low for different error margins. Similarly, the misclassification rates are low for different block sizes when the error margin is 0.05. The optimal block size depends on the true underlying block structure in the data, which is 200 in simulations. It can be seen that the performance of our method is satisfactory when the block size is 100. Also, the optimal error margin depends on the sample size. When the sample size is small, where the cross validation error itself has large variation, a larger error margin is preferred to include more predictive features. In contrast, when the sample size is large, a smaller error margin might be better to exclude pure noises. For simplicity, we use block size 100 and error margin 0.05 for all later analyses.
<fig id="Fig1"><label>Figure 1</label><caption><p>
<bold>The effects of block size and error margin on SQDA.</bold> The effects of two important parameters, block size and error margin, on SQDA are shown in this figure based on the simulated data.</p></caption><graphic xlink:href="12859_2014_443_Fig1_HTML" id="MO1"/></fig>
</p></sec><sec id="Sec6"><title>Effect of sample size</title><p>The performance of any classification depends on sample size, which may be especially so for our method, since the number of parameters to be estimated is large and low sample size may lead to unstable results. To study the effect of sample size on our method and other methods, all the parameters are kept the same except that the sample size is varied among 20, 25, 30, 35, 40, 45 and 50 under the fourth simulation setting.</p><p>The average misclassification rates are shown in Figure <xref rid="Fig2" ref-type="fig">2</xref> for all methods. We consider variable selection by blocks for DLDA2 and DQDA2 the same as in our method except that the sparse estimation of covariance matrix is replaced with diagonalized estimator for covariance matrix. By comparing the performance of SQDA to DLDA2 and DQDA2, we can see the benefit of sparse estimation of different covariance matrices for different classes in addition to that from the variable selection by blocks procedure. It is clear that the performance of all methods is equally poor when the sample size is small whereas the improvement is largest for our method when the sample size increases.
<fig id="Fig2"><label>Figure 2</label><caption><p>
<bold>The effect of sample size on the seven classification methods.</bold> The effect of sample size on SQDA and six other classificaiton methods is shown in this figure based on the simulated data.</p></caption><graphic xlink:href="12859_2014_443_Fig2_HTML" id="MO2"/></fig>
</p></sec><sec id="Sec7"><title>Performance of different methods on simulated data</title><p>In this part, we compare the performance of different methods on the simulated data, where we consider sample size of 50, block size of 100, and error margin of 0.05.</p><p>It is clear from Table <xref rid="Tab1" ref-type="table">1</xref> that our method has the best or close to the best performance among all the methods considered across different simulation scenarios.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>
<bold>Comparisons of seven classification methods on simulated data</bold>
</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">
<bold>Methods</bold>
</th><th align="left">
<bold>ISSC</bold>
</th><th align="left">
<bold>ISDC</bold>
</th><th align="left">
<bold>DSSC</bold>
</th><th align="left">
<bold>DSDC</bold>
</th></tr></thead><tbody><tr><td align="left">DLDA</td><td align="left">0.048 (0.015, 50)</td><td align="left">0.083 (0.015, 100)</td><td align="left">0.228 (0.02, 1025)</td><td align="left">0.217 (0.04, 1175)</td></tr><tr><td align="left">DQDA</td><td align="left">0.049 (0.021, 50)</td><td align="left">0.013 (0.007, 50)</td><td align="left">0.243 (0.025, 1400)</td><td align="left">0.214 (0.032, 825)</td></tr><tr><td align="left">NN</td><td align="left">0.056 (0.02, 100)</td><td align="left">0.424 (0.021, 50)</td><td align="left">0.27 (0.034, 575)</td><td align="left">0.112 (0.061, 475)</td></tr><tr><td align="left">SVM</td><td align="left">0.054 (0.029, 50)</td><td align="left">0.095 (0.024, 100)</td><td align="left">0.127 (0.047, 500)</td><td align="left">0.255 (0.05, 1050)</td></tr><tr><td align="left">SCRDA</td><td align="left">0.019 (0.036, 651)</td><td align="left">0.024 (0.012, 2089)</td><td align="left">0.217 (0.041, 587)</td><td align="left">0.241 (0.069, 317)</td></tr><tr><td align="left">RF</td><td align="left">0.109 (0.012, NA)</td><td align="left">0.038 (0.009, NA)</td><td align="left">0.262 (0.018, NA)</td><td align="left">0.21 (0.041, NA)</td></tr><tr><td align="left">SQDA</td><td align="left">0.005 (0.002, 300)</td><td align="left">0.001 (0.001, 300)</td><td align="left">0.108 (0.042, 200)</td><td align="left">0.001 (0.002, 400)</td></tr><tr><td align="left">DLDA2</td><td align="left">0.002 (0.001, 600)</td><td align="left">0.04 (0.005, 500)</td><td align="left">0.224 (0.03, 700)</td><td align="left">0.217 (0.055, 600)</td></tr><tr><td align="left">DQDA2</td><td align="left">0.003 (0.001, 500)</td><td align="left">0 (0.001, 600)</td><td align="left">0.231 (0.033, 600)</td><td align="left">0.224 (0.058, 400)</td></tr></tbody></table><table-wrap-foot><p>The reported numbers in each table entry in the form of <italic>a (b,c)</italic> mean: <italic>a</italic> is the average prediction error, <italic>b</italic> is the standard deviation, and <italic>c</italic> is the median number of predictors selected.</p></table-wrap-foot></table-wrap>
</p><p>When the true covariance matrix is diagonal and the same for different classes (ISSC), DLDA2 performs the best and is much better than DLDA.</p><p>When the true covariance matrix is diagonal and different for different classes (ISDC), DQDA2 performs the best and is much better than DQDA.</p><p>Also, comparing the number of predictors selected by DLDA and DLDA2 or DQDA and DQDA2 suggests variable selection by the blocks method is favorable over variable selection by cross-validation. The number of predictors selected by cross-validation is either too small (50 or 100 for simple underlying covariance structure) or too large (1000+ for complex underlying covariance structure) while the number of predictors selected by variable selection by blocks is closer to the true number of predictors, which is 400 for all scenarios.</p><p>When the covariance structure is complex (DSSC and DSDC), the performance of SQDA is much better than all the other methods. The relative performance of SQDA with that of DLDA2 and DQDA2 suggests that the sparse estimation of different covariance matrix for different classes can indeed improve sample classification.</p></sec></sec><sec id="Sec8"><title>Real data</title><p>In this section, we compare the performance of different classification methods on the TCGA dataset, which we downloaded from the UCSC cancer browser (<ext-link ext-link-type="uri" xlink:href="https://genome-cancer.ucsc.edu">https://genome-cancer.ucsc.edu</ext-link>), with the level 3 RNAseq data for all cancers log2-transformed and mean normalized across all cancer cohorts. We consider all cancer cohorts with at least 40 normal tissue samples, including Liver Cancer (LC, 191 tumor and 50 normal), Colon Cancer (CC, 262 tumor and 41 normal), Head and Neck Cancer (HNC, 497 tumor and 43 normal), Breast Cancer (BC, 1040 tumor and 112 normal), Lung Adenocarcinoma (LUAD, 488 tumor and 58 normal), Prostate Cancer (PC, 333 tumor and 50 normal), Kidney Clear Cell Carcinoma(KC, 518 tumor and 72 normal), Lung Squamous Cell Carcinoma (LUSC, 490 tumor and 50 normal) and Thyroid Cancer (TC, 498 tumor and 59 normal).</p><p>For all datasets, 20 tumor samples and 20 normal samples are randomly selected to be the training data and the remaining samples are treated as the testing data. Tuning parameters of all methods are selected using the training data and then the misclassification rates are estimated using the testing data. The whole process is repeated 10 times and the average misclassification rate and standard deviation for each method are reported together with the median number of features (i.e. genes) selected.</p><p>For real data analysis, we choose the block size to be 100 and error margin to be 0.05 for all datasets, as suggested by simulations. The results are shown in Table <xref rid="Tab2" ref-type="table">2</xref>. We can see that SQDA performs best or is comparable to the best for all datasets except the PC datasets. Because the sample size for each class is 20, we increase the error margin to include more true signals. In fact, the performance of SQDA can improve when the error margin is 0.1 (results not shown).
<table-wrap id="Tab2"><label>Table 2</label><caption><p>
<bold>Comparisons of seven classification methods on TCGA data</bold>
</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">
<bold>Methods</bold>
</th><th align="left">
<bold>LUSC</bold>
</th><th align="left">
<bold>LUAD</bold>
</th><th align="left">
<bold>TC</bold>
</th></tr></thead><tbody><tr><td align="left">DLDA</td><td align="left">0.013 (0.007, 50)</td><td align="left">0.035 (0.027, 50)</td><td align="left">0.1 (0.042, 50)</td></tr><tr><td align="left">DQDA</td><td align="left">0.008 (0.006,50)</td><td align="left">0.018 (0.019, 50)</td><td align="left">0.08 (0.047, 50)</td></tr><tr><td align="left">NN</td><td align="left">0.014 (0.01, 50)</td><td align="left">0.027 (0.016, 50)</td><td align="left">0.085 (0.053, 50)</td></tr><tr><td align="left">SVM</td><td align="left">0.01 (0.007, 50)</td><td align="left">0.024 (0.017, 50)</td><td align="left">0.088 (0.041, 50)</td></tr><tr><td align="left">SCRDA</td><td align="left">0.039 (0.031, 128)</td><td align="left">0.044 (0.026, 95)</td><td align="left">0.122 (0.068, 502)</td></tr><tr><td align="left">RF</td><td align="left">0.007 (0.002, NA)</td><td align="left">0.018 (0.009, NA)</td><td align="left">0.08 (0.039, NA)</td></tr><tr><td align="left">SQDA</td><td align="left">0.003 (0.003, 1900)</td><td align="left">0.011 (0.009, 1900)</td><td align="left">0.036 (0.021, 2900)</td></tr><tr><td align="left">DLDA2</td><td align="left">0.017 (0.005, 12100)</td><td align="left">0.031 (0.013, 8900)</td><td align="left">0.114 (0.038, 2300)</td></tr><tr><td align="left">DQDA2</td><td align="left">0.008 (0.004,10000)</td><td align="left">0.023 (0.008, 8300)</td><td align="left">0.107 (0.05, 5800)</td></tr><tr><td align="left">
<bold>Methods</bold>
</td><td align="left">
<bold>PC</bold>
</td><td align="left">
<bold>HNC</bold>
</td><td align="left">
<bold>LC</bold>
</td></tr><tr><td align="left">DLDA</td><td align="left">0.125 (0.024, 50)</td><td align="left">0.034 (0.012, 50)</td><td align="left">0.055 (0.017, 50)</td></tr><tr><td align="left">DQDA</td><td align="left">0.11 (0.022, 50)</td><td align="left">0.03 (0.016, 50)</td><td align="left">0.045 (0.021, 50)</td></tr><tr><td align="left">NN</td><td align="left">0.094 (0.029, 50)</td><td align="left">0.032 (0.013, 50)</td><td align="left">0.051(0.015, 50)</td></tr><tr><td align="left">SVM</td><td align="left">0.116 (0.031, 150)</td><td align="left">0.037 (0.023, 50)</td><td align="left">0.04 (0.014, 50)</td></tr><tr><td align="left">SCRDA</td><td align="left">0.094 (0.037, 1989)</td><td align="left">0.039 (0.021, 2200)</td><td align="left">0.069 (0.026, 56)</td></tr><tr><td align="left">RF</td><td align="left">0.11 (0.013, NA)</td><td align="left">0.033 (0.013, NA)</td><td align="left">0.048 (0.018, NA)</td></tr><tr><td align="left">SQDA</td><td align="left">0.206 (0.134, 1300)</td><td align="left">0.021 (0.015, 2200)</td><td align="left">0.04 (0.041, 500)</td></tr><tr><td align="left">DLDA2</td><td align="left">0.128 (0.026, 3400)</td><td align="left">0.033 (0.01, 6600)</td><td align="left">0.068 (0.02, 7800)</td></tr><tr><td align="left">DQDA2</td><td align="left">0.205 (0.066, 3100)</td><td align="left">0.049 (0.022, 5900)</td><td align="left">0.089 (0.027, 6100)</td></tr><tr><td align="left">
<bold>Methods</bold>
</td><td align="left">
<bold>BC</bold>
</td><td align="left">
<bold>KC</bold>
</td><td align="left">
<bold>CC</bold>
</td></tr><tr><td align="left">DLDA</td><td align="left">0.035 (0.017, 50)</td><td align="left">0.028 (0.018, 50)</td><td align="left">0.006 (0.008, 50)</td></tr><tr><td align="left">DQDA</td><td align="left">0.018 (0.009, 50)</td><td align="left">0.037 (0.03, 50)</td><td align="left">0.004 (0.006, 50)</td></tr><tr><td align="left">NN</td><td align="left">0.021 (0.013, 50)</td><td align="left">0.031 (0.019, 50)</td><td align="left">0.005 (0.009, 50)</td></tr><tr><td align="left">SVM</td><td align="left">0.018 (0.012, 50)</td><td align="left">0.028 (0.018, 50)</td><td align="left">0.004 (0.006, 50)</td></tr><tr><td align="left">SCRDA</td><td align="left">0.045 (0.019, 452)</td><td align="left">0.047 (0.011, 78)</td><td align="left">0.023 (0.014, 49)</td></tr><tr><td align="left">RF</td><td align="left">0.027 (0.013, NA)</td><td align="left">0.025 (0.014, NA)</td><td align="left">0.011 (0.011, NA)</td></tr><tr><td align="left">SQDA</td><td align="left">0.021 (0.008, 2800)</td><td align="left">0.009 (0.005, 6100)</td><td align="left">0.007 (0.008, 5900)</td></tr><tr><td align="left">DLDA2</td><td align="left">0.036 (0.015, 8000)</td><td align="left">0.039 (0.007, 10200)</td><td align="left">0.02 (0.014, 11700)</td></tr><tr><td align="left">DQDA2</td><td align="left">0.069 (0.033, 7400)</td><td align="left">0.045 (0.035, 9600)</td><td align="left">0.021 (0.014, 10200)</td></tr></tbody></table><table-wrap-foot><p>The reported numbers in each table entry in the form of <italic>a (b,c)</italic> mean: <italic>a</italic> is the average prediction error, <italic>b</italic> is the standard deviation, and <italic>c</italic> is the median number of predictors selected.</p></table-wrap-foot></table-wrap>
</p></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussion</title><p>In this paper, we have proposed a sparse version of QDA to take into account differing genetic networks across different classes in sample classifications. When the proposed method, SQDA, was compared with six commonly used classification methods on both simulated data and real data, we found that SQDA has good performance, especially when different classes have different covariance matrices.</p><p>In order to alleviate the intensive computation burden, we have imposed the block-diagonal structure assumption on the covariance matrix. We further assumed that the block size is the same for all blocks. Although this assumption seems too simple to characterize the complex relationships among genes, it does offer a good compromise between the diagonal covariance matrix assumption made in previous methods and the more complex covariance matrix structures that may require much more data to model. The better performance of this approach has been demonstrated in both simulations and, more importantly, the TCGA datasets.</p><p>In our comparison of SQDA with six other classification methods on simulated and real data, we consider block size of 100 and error margin of 0.05 based on the simulation. In practice, the block size and error margin can be selected using cross-validation. However, due to the small sample size in real data, tuning too many parameters using cross-validation may lead to more variations in the results. As sample size increases, cross validation-based or other methods may be used to select block size and error margin to improve the performance of our method.</p><p>Instead of determining the blocks based on two sample <italic>t</italic> statistics and using the same size for each block, a data-driven way of determining the blocks might be better. For example, as suggested in [<xref ref-type="bibr" rid="CR23">23</xref>], hierarchical clustering based on the correlation matrix summarized across all classes could be used to determine the blocks, where the number of clusters (i.e. blocks) is determined using cross-validation. However, when using cross-validation to choose the number of clusters, the cluster size (i.e. block size) could be larger than 1000, which makes it computationally prohibitive to tune the sparsity parameters in estimating the covariance matrix for those large blocks.</p><p>We have considered binary classification for both simulations and real data analysis. We note that SQDA can be easily extended to multi-class classification problems.</p></sec><sec id="Sec10" sec-type="conclusion"><title>Conclusions</title><p>In summary, we have proposed a sparse version of QDA, which has better or similar performance than commonly used classification methods based on both simulated data and real data. We believe SQDA will prove useful for classification in genomics studies and other research settings, where covariances differ among classes. A R package, SQDA, can be used to perform sparse quadratic discriminant data analysis and is freely available on CRAN (<ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org">http://cran.r-project.org</ext-link>).</p></sec><sec id="Sec11"><title>Methods</title><p>In this section, we will first review the existing methods and then introduce our method.</p><sec id="Sec12"><title>LDA, QDA, DLDA, and DQDA</title><p>Assume we collect data from <italic>n</italic> samples with each sample having <italic>p</italic> features. We further assume that the samples are drawn from <italic>K</italic> classes. Let <italic>Y</italic> denote the class label, i.e. <italic>Y</italic>
<sub><italic>i</italic></sub>=<italic>k</italic> means the <italic>i</italic>
<sup><italic>t</italic><italic>h</italic></sup> sample belongs to the <italic>k</italic>
<sub><italic>th</italic></sub> class, where <italic>k</italic>=1,&#x02026;,<italic>K</italic>. Let <italic>X</italic> denote the vector of features, that is <italic>X</italic>
<sub><italic>i</italic></sub> is a <italic>p</italic>-dimensional vector with each coordinate being the value for the corresponding feature for <italic>i</italic>
<sup><italic>t</italic><italic>h</italic></sup> sample. In LDA and QDA, the features in each class are assumed to follow a multivariate Gaussian distribution, that is
<disp-formula id="Equ3"><label>(3)</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$${} X_{i} | Y_{i}=k\sim N\left(\mu_{k},\Sigma_{k}\right)  $$
\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>&#x0223c;</mml:mo><mml:mi>N</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2014_443_Equ3.gif" position="anchor"/></alternatives></disp-formula>
</p><p>
<disp-formula id="Equ4"><label>(4)</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$${} {\fontsize{8.8pt}{9.6pt}\selectfont{\begin{aligned} f(X_{i} | Y_{i}=k) =\frac{1}{\sqrt{(2\pi)^{p} |\Sigma_{k}|}}\exp\!\left(\!-\frac{1}{2} (X_{i}-\mu_{k})'\Sigma_{k}^{-1}(X_{i}-\mu_{k})\!\right)\!. \end{aligned}}}  $$
\end{document}</tex-math><mml:math id="M8"><mml:mtable><mml:mtr><mml:mtd><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>&#x003c0;</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>exp</mml:mo><mml:mspace width="0.3em"/><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mspace width="0.3em"/><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mspace width="0.3em"/></mml:mrow></mml:mfenced><mml:mspace width="0.3em"/><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2014_443_Equ4.gif" position="anchor"/></alternatives></disp-formula>
</p><p>Thus, we can assign the <italic>i</italic>
<sub><italic>th</italic></sub> sample to one of <italic>K</italic> based on the maximum likelihood rule, that is
<disp-formula id="Equ5"><label>(5)</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\begin{array}{@{}rcl@{}} {}Y_{i}&#x00026;=&#x00026;\underset{k=1,\ldots,K}{\arg\, \max}\, f(X_{i} |Y_{i}=k) \end{array} $$
\end{document}</tex-math><mml:math id="M10"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mo>arg max</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mtext/><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2014_443_Equ5.gif" position="anchor"/></alternatives></disp-formula>
</p><p>
<disp-formula id="Equ6"><label>(6)</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\begin{array}{@{}rcl@{}} {}&#x00026;=&#x00026;\underset{k=1,\ldots,K}{\arg\, \max} \log f(X_{i} |Y_{i}=k). \end{array} $$
\end{document}</tex-math><mml:math id="M12"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mo>arg max</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>log</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo><mml:mi>.</mml:mi></mml:mtd><mml:mtd><mml:mtext/><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2014_443_Equ6.gif" position="anchor"/></alternatives></disp-formula>
</p><p>In LDA, the covariance matrices are assumed to be the same for all classes, that is <italic>&#x003a3;</italic>
<sub><italic>k</italic></sub>=<italic>&#x003a3;</italic>, for <italic>k</italic>=1,&#x02026;,<italic>K</italic>, and hence the maximum likelihood rule can be simplified to
<disp-formula id="Equ7"><label>(7)</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\begin{array}{@{}rcl@{}} {}Y_{i}&#x00026;=&#x00026;\underset{k=1,\ldots,K}{\arg\, \min} \left(X_{i}-\mu_{k}\right)'\Sigma^{-1}\left(X_{i}-\mu_{k}\right) \end{array} $$
\end{document}</tex-math><mml:math id="M14"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mo>arg min</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mtext/><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2014_443_Equ7.gif" position="anchor"/></alternatives></disp-formula>
</p><p>
<disp-formula id="Equ8"><label>(8)</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\begin{array}{@{}rcl@{}} {}&#x00026;=&#x00026;\underset{k=1,\ldots,K}{\arg \, \min}\, \mu_{k}' \Sigma^{-1} \mu_{k} -2\mu_{k}'\Sigma^{-1}X_{i}. \end{array} $$
\end{document}</tex-math><mml:math id="M16"><mml:mtable class="eqnarray" columnalign="left center right"><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mo>arg min</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>.</mml:mi></mml:mtd><mml:mtd><mml:mtext/><mml:mtext/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2014_443_Equ8.gif" position="anchor"/></alternatives></disp-formula>
</p><p>In practice, the parameters <italic>&#x003bc;</italic>
<sub><italic>k</italic></sub> and <italic>&#x003a3;</italic> are unknown and need to be estimated. In general, they are estimated by the sample mean (<inline-formula id="IEq1"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\bar {x}_{k}$
\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2014_443_IEq1.gif"/></alternatives></inline-formula>) and sample covariance matrix (<inline-formula id="IEq2"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\Sigma }$
\end{document}</tex-math><mml:math id="M20"><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2014_443_IEq2.gif"/></alternatives></inline-formula>) respectively. In the high-dimensional setting where <italic>p</italic>&#x0226b;<italic>n</italic>, the sample covariance matrix (<inline-formula id="IEq3"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\Sigma }$
\end{document}</tex-math><mml:math id="M22"><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2014_443_IEq3.gif"/></alternatives></inline-formula>) is singular.</p><p>In DLDA, the covariance matrix (<italic>&#x003a3;</italic>) is estimated by the diagonal common sample covariance matrix, that is <inline-formula id="IEq4"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {\Sigma }$
\end{document}</tex-math><mml:math id="M24"><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2014_443_IEq4.gif"/></alternatives></inline-formula> is diagonal with each diagonal element being the pooled sample variance of the corresponding predictor.</p><p>In DQDA, the covariance matrix for each class (<italic>&#x003a3;</italic>
<sub><italic>k</italic></sub>) is estimated by the diagonal sample covariance matrix based on the samples belonging to class <italic>k</italic>.</p></sec><sec id="Sec13"><title>SCRDA</title><p>In SCRDA, the covariance matrix is estimated through a linear combination of sample covariance matrix and the identity matrix, that is
<disp-formula id="Equ9"><label>(9)</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \tilde{\Sigma}=\alpha \hat{\Sigma}+ (1-\alpha)I_{p},  $$
\end{document}</tex-math><mml:math id="M26"><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2014_443_Equ9.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where <italic>&#x003b1;</italic>&#x02208;[ 0,1].</p><p>Alternatively, shrinkage can be applied to the correlation matrix, that is
<disp-formula id="Equ10"><label>(10)</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \tilde{R}=\alpha \hat{R}+ (1-\alpha)I_{p},  $$
\end{document}</tex-math><mml:math id="M28"><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>)</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2014_443_Equ10.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where <italic>&#x003b1;</italic>&#x02208;[ 0,1], <inline-formula id="IEq5"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\hat {R}$
\end{document}</tex-math><mml:math id="M30"><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2014_443_IEq5.gif"/></alternatives></inline-formula> is the sample correlation matrix.</p><p>In addition to the shrunken covariance matrix estimator, the means for each class can also be estimated through shrinkage based on the &#x0201c;nearest shrunken centroids&#x0201d;, that is
<disp-formula id="Equ11"><label>(11)</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ \bar{x}^{*}_{k}=\text{sgn}\left(\bar{x}^{*}_{k}\right)\left(|\bar{x}^{*}_{k}| - \Delta\right)_{+},  $$
\end{document}</tex-math><mml:math id="M32"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>sgn</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>&#x00394;</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2014_443_Equ11.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where <inline-formula id="IEq6"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\bar {x}^{*}_{k}=\tilde {\Sigma }^{-1}\bar {x}_{k}$
\end{document}</tex-math><mml:math id="M34"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02217;</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>&#x003a3;</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2014_443_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$\bar {x}_{k}$
\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>&#x000af;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2014_443_IEq7.gif"/></alternatives></inline-formula> is the sample mean for class <italic>k</italic>.</p><p>In practice, (<italic>&#x003b1;</italic>,<italic>&#x00394;</italic>) need to be tuned based on cross-validation. If there are several pairs of (<italic>&#x003b1;</italic>,<italic>&#x00394;</italic>) having the same cross-validation error, we use the &#x0201c;MIN-MIN&#x0201d; rule mentioned in [<xref ref-type="bibr" rid="CR8">8</xref>] to choose their parameters. This is accomplished using the R package <italic>rda</italic>.</p></sec><sec id="Sec14"><title>NN (nearest neighbor)</title><p>In NN, a new sample with predictor <italic>X</italic>
<sup>&#x02032;</sup> is classified into one of the <italic>K</italic> classes based on the class labels of the <italic>h</italic> known samples that are closest to the new sample defined in terms of euclidean distance defined over all the predictors, where <italic>h</italic> is a pre-defined integer. The class label <italic>Y</italic>
<sup>&#x02032;</sup> for the new sample is usually inferred by the majority vote from these <italic>h</italic> selected samples, that is
<disp-formula id="Equ12"><label>(12)</label><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$ Y'=\underset{k=1,\ldots,K}{\arg\, \max} \sum_{i\in S}\delta(Y_{i}=k)  $$
\end{document}</tex-math><mml:math id="M38"><mml:msup><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>arg max</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>&#x02026;</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mi>&#x003b4;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2014_443_Equ12.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where <italic>&#x003b4;</italic>() is the indictor function and <italic>S</italic> is the index set.</p><p>In our comparison, <italic>h</italic> is usually chosen to be 3, a common practice in genomics data analysis. In our comparisons, we used the <italic>knn</italic> function implemented in R package <italic>class</italic>.</p></sec><sec id="Sec15"><title>SVM</title><p>In the binary classification problem, i.e. <italic>K</italic>=2, SVM aims to find a hyperplane that can separate the two classes. If perfect separating hyperplanes exist, that is the two class can be separated perfectly, then the maximum-margin hyperplane is chosen, which maximizes the total distance of the closest point in each class to the hyperplane. If no perfect separating hyperplane exists, the hyperplane maximizing the margins while keeping the misclassification rate as low as possible is chosen.</p><p>In SVM, the kernel function has to be predefined, which basically determines the shape of the hyperplanes. Popular kernel functions include linear kernel function, polynomial kernel function, and Gaussian radial basis function. When there are more than two classes, a single multi-class problem is often decomposed into multiple binary classification problems.</p><p>In our comparison, we used SVM with the linear kernel, because the sample size is small. We used the <italic>svm</italic> function in R package <italic>e</italic>1071 in our comparisons.</p></sec><sec id="Sec16"><title>RF</title><p>The basic idea of RF is to grow a forest of classification trees based on the training data and then use the classification trees to classify testing data. More specifically, for a given training dataset, <italic>B</italic> bootstrapped datasets are used to build R decision trees where a random subset of predictors are evaluated at each node [<xref ref-type="bibr" rid="CR24">24</xref>]. The Random Forest, which consists of <italic>B</italic> prediction trees, is used for classifying future samples. For a test sample, each prediction tree will assign it to one of the <italic>K</italic> classes and the class label of this sample is then determined by majority vote from the <italic>B</italic> decision trees.</p><p>We used the R package <italic>randomForest</italic> in our comparisons.</p></sec><sec id="Sec17"><title>Proposed method</title><p>In this article, we propose a modified version of QDA with sparse estimation of the covariance matrix. We call it SQDA.</p><p>In SQDA, we adopted the method introduced in [<xref ref-type="bibr" rid="CR25">25</xref>] to obtain a sparse estimator of the covariance matrix. The sparse estimate for the correlation matrix is first obtained by the following optimization criterion and then transformed back to the original scale using the sample variance, which yields a sparse estimate for the covariance matrix.
<disp-formula id="Equ13"><label>(13)</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$  \tilde{R}_{\lambda} = \underset{R\succ 0}{\arg \, \min}\, {|| R-\hat{R}||}_{F}^{2}/2 -\tau \log |R|+\lambda {|R^{-}|}_{1}  $$
\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>&#x003bb;</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>arg min</mml:mo></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>&#x0227b;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x02212;</mml:mo><mml:mi>&#x003c4;</mml:mi><mml:mo>log</mml:mo><mml:mo>|</mml:mo><mml:mi>R</mml:mi><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02212;</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math><graphic xlink:href="12859_2014_443_Equ13.gif" position="anchor"/></alternatives></disp-formula>
</p><p>where ||&#x000b7;||<sub><italic>F</italic></sub> is the Frobenius norm, |&#x000b7;|<sub>1</sub> is the <italic>L</italic>
<sub>1</sub> norm, <italic>&#x003c4;</italic> is a fixed small value, <italic>&#x003bb;</italic> is a tuning parameter, and <italic>R</italic>
<sup>&#x02212;</sup> means <italic>R</italic> with diagonal elements set to 0.</p><p>However, it is time consuming to estimate the covariance matrix for extremely large <italic>p</italic> based on Equation <xref rid="Equ13" ref-type="">13</xref>. To reduce computational burden, we assume covariance matrices for all classes have block-diagonal structure to allow us to estimate the covariance matrices one block at a time. The idea of using block-diagonal structure to approximate the inverse of covariance matrix has been applied in LDA by [<xref ref-type="bibr" rid="CR26">26</xref>]. However, the inverse of covariance matrix still has to be estimated in their method, which is time consuming.</p><p>Under the block-diagonal structure assumption for the covariance matrix, the order of features matters, that is we need to know which features form a block. In our algorithm, we sort the genes based on the absolute two sample t statistics and the blocks are chosen from the top to the bottom with each block having the same size. There were a couple of reasons that led us to choose our approach. First, genes with similar expression level differences across the classes are grouped together based on two sample t statistics so that genes with similar informativeness on classifications are likely to be selected or excluded together. Second, genes that are highly correlated are likely to have similar absolute values in terms of two sample t statistics, so if the covariance matrix needs to be approximated by a block-structured one, then grouping genes by t statistics is more likely to result in a better approximation of the true covariance matrix.</p><p>To illustrate the workflow of our proposed method (as shown in Figure <xref rid="Fig3" ref-type="fig">3</xref>), let us consider the example in the classification of tumor and normal tissues based on gene expression profiles. First, the two sample <italic>t</italic> statistics based on the training data are calculated for each gene. The genes are then ordered based on the absolute values of the t statistics. To be computationally more efficient, an optional step here is to do variable selection by blocks (see section <xref rid="Sec18" ref-type="sec">Variable selection</xref>). Secondly, the covariance matrices are estimated one block at a time with block size 100 from the top to the bottom. Lastly, the final prediction model is the quadratic discriminant analysis model with the covariance matrices obtained from all the blocks.
<fig id="Fig3"><label>Figure 3</label><caption><p>
<bold>Workflow of SQDA.</bold> This figure decribes the general workflow of SQDA based on a toy example of classifications of tumor and normal samples.</p></caption><graphic xlink:href="12859_2014_443_Fig3_HTML" id="MO3"/></fig>
</p><p>For each block in SQDA, we use the same <italic>&#x003bb;</italic> for all the classes and choose the value of <italic>&#x003bb;</italic> through cross-validation. When several <italic>&#x003bb;</italic> values lead to the same cross validation error, we choose the minimum value for <italic>&#x003bb;</italic>. To perform sparse estimation of the covariance matrix, the R package <italic>PDSCE</italic> is used.</p></sec><sec id="Sec18"><title>Variable selection</title><p>We use multiple gene expression data to compare the performance of different classification methods. For gene expression datasets, the number of genes is usually on the scale of thousands while the number of samples is on the scale of tens. Thus, it may be better to perform gene pre-screening to improve classification performance. In this paper, we use the R package <italic>limma</italic> to rank the genes based on the empirical Bayes-based <italic>t</italic> statistics (for binary classification problem) or the <italic>F</italic> statistics (for multi-class classification problem).</p><p>For DLDA, DQDA, NN, and SVM, we use five-fold cross validation based on the training sample to pick the number of top genes used for prediction ranging from 50 to 2000 with step size 50.</p><p>For SCRDA and RF, since they can perform variable selection and classification simultaneously, no variable selection step is performed.</p><p>For our method, SQDA, we do variable selection by blocks. We set the sparsity parameter <italic>&#x003bb;</italic> to be 0.2 and calculate cross-validation error for each block and then we choose the blocks with cross validation errors less than error margin + the smallest cross-validation error, where error margin can be any number from 0 to 0.5 and usually 0.05, 0.10 and 0.15 are used.</p><p>For DLDA and DQDA, we also perform variable selection by blocks, leading to DLDA2 and DQDA2, that is to use the same procedure as our method to do variable selection except that the sparse estimation of covariance matrix is replaced by diagonalized estimators for covariance matrix.</p></sec></sec></body><back><fn-group><fn><p><bold>Competing interests</bold></p><p>The authors declare that they have no competing interests.</p></fn><fn><p><bold>Authors&#x02019; contributions</bold></p><p>JS performed statistical analysis and drafted the manuscript. HZ conceived of the study, designed and coordinated the study and helped to draft the manuscript. All authors read and approved the final manuscript.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We thank the reviewers for their insightful comments. We also thank Yale University Biomedical High Performance Computing Center (NIH grant RR19895) for providing computation resources. Research was funded in part by the National Institutes of Health (NIH) grants R01 GM59507, P01 CA154295, and P30 CA016359 to the authors.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Pillo</surname><given-names>PJ</given-names></name></person-group><article-title>The application of bias to discriminant analysis</article-title><source>Commun Statistics-Theory Methods.</source><year>1976</year><volume>5</volume><issue>9</issue><fpage>843</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1080/03610927608827401</pub-id></element-citation></ref><ref id="CR2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>Z-Q</given-names></name><name><surname>Yang</surname><given-names>J-Y</given-names></name></person-group><article-title>Optimal discriminant plane for a small number of samples and design method of classifier on the plane</article-title><source>Pattern Recognit.</source><year>1991</year><volume>24</volume><issue>4</issue><fpage>317</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/0031-3203(91)90074-F</pub-id></element-citation></ref><ref id="CR3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Y-Q</given-names></name><name><surname>Zhuang</surname><given-names>Y-M</given-names></name><name><surname>Yang</surname><given-names>J-Y</given-names></name></person-group><article-title>Optimal fisher discriminant analysis using the rank decomposition</article-title><source>Pattern Recognit.</source><year>1992</year><volume>25</volume><issue>1</issue><fpage>101</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/0031-3203(92)90010-G</pub-id></element-citation></ref><ref id="CR4"><label>4</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ripley</surname><given-names>BD</given-names></name></person-group><source>Pattern Recognit and Neural Networks</source><year>1996</year><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="CR5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Buja</surname><given-names>A</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Penalized discriminant analysis</article-title><source>Ann Stat.</source><year>1995</year><volume>23</volume><issue>1</issue><fpage>73</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1214/aos/1176324456</pub-id></element-citation></ref><ref id="CR6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffbeck</surname><given-names>JP</given-names></name><name><surname>Landgrebe</surname><given-names>DA</given-names></name></person-group><article-title>Covariance matrix estimation and classification with limited training data</article-title><source>IEEE Trans Pattern Anal Mach Intell.</source><year>1996</year><volume>18</volume><issue>7</issue><fpage>763</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1109/34.506799</pub-id></element-citation></ref><ref id="CR7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dudoit</surname><given-names>S</given-names></name><name><surname>Fridlyand</surname><given-names>J</given-names></name><name><surname>Speed</surname><given-names>TP</given-names></name></person-group><article-title>Comparison of discrimination methods for the classification of tumors using gene expression data</article-title><source>J Am Stat Assoc.</source><year>2002</year><volume>97</volume><issue>457</issue><fpage>77</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1198/016214502753479248</pub-id></element-citation></ref><ref id="CR8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Regularized linear discriminant analysis and its application in microarrays</article-title><source>Biostat. (Oxford, England).</source><year>2007</year><volume>8</volume><issue>1</issue><fpage>86</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxj035</pub-id></element-citation></ref><ref id="CR9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Narasimhan</surname><given-names>B</given-names></name><name><surname>Chu</surname><given-names>G</given-names></name></person-group><article-title>Class prediction by nearest shrunken centroids, with applications to DNA microarrays</article-title><source>Stat Sci.</source><year>2003</year><volume>18</volume><issue>1</issue><fpage>104</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1214/ss/1056397488</pub-id></element-citation></ref><ref id="CR10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>P</given-names></name><name><surname>Brock</surname><given-names>GN</given-names></name><name><surname>Parrish</surname><given-names>RS</given-names></name></person-group><article-title>Modified linear discriminant analysis approaches for classification of high-dimensional microarray data</article-title><source>Comput Stat Data Anal.</source><year>2009</year><volume>53</volume><issue>5</issue><fpage>1674</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1016/j.csda.2008.02.005</pub-id></element-citation></ref><ref id="CR11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sch&#x000e4;fer</surname><given-names>J</given-names></name><name><surname>Strimmer</surname><given-names>K</given-names></name></person-group><article-title>A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics</article-title><source>Stat Appl Genet Mol Biol.</source><year>2005</year><volume>4</volume><issue>1</issue><fpage>1175</fpage><lpage>89</lpage></element-citation></ref><ref id="CR12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledoit</surname><given-names>O</given-names></name><name><surname>Wolf</surname><given-names>M</given-names></name></person-group><article-title>A well-conditioned estimator for large-dimensional covariance matrices</article-title><source>J Multivariate Anal.</source><year>2004</year><volume>88</volume><issue>2</issue><fpage>365</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1016/S0047-259X(03)00096-4</pub-id></element-citation></ref><ref id="CR13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bensmail</surname><given-names>H</given-names></name><name><surname>Celeux</surname><given-names>G</given-names></name></person-group><article-title>Regularized gaussian discriminant analysis through eigenvalue decomposition</article-title><source>J Am Stat Assoc.</source><year>1996</year><volume>91</volume><issue>436</issue><fpage>1743</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1080/01621459.1996.10476746</pub-id></element-citation></ref><ref id="CR14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shao</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Deng</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name></person-group><article-title>Sparse linear discriminant analysis by thresholding for high dimensional data</article-title><source>Ann Stat.</source><year>2011</year><volume>39</volume><issue>2</issue><fpage>1241</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1214/10-AOS870</pub-id></element-citation></ref><ref id="CR15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>T</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name></person-group><article-title>A direct estimation approach to sparse linear discriminant analysis</article-title><source>J Am Stat Assoc.</source><year>2011</year><volume>106</volume><issue>496</issue><fpage>1566</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1198/jasa.2011.tm11199</pub-id></element-citation></ref><ref id="CR16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witten</surname><given-names>DM</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><article-title>Covariance-regularized regression and classification for high dimensional problems</article-title><source>J R Stat Soc: Series B (Stat Methodol).</source><year>2009</year><volume>71</volume><issue>3</issue><fpage>615</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2009.00699.x</pub-id><pub-id pub-id-type="pmid">20084176</pub-id></element-citation></ref><ref id="CR17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name></person-group><article-title>A statistical method for identifying differential gene-gene co-expression patterns</article-title><source>Bioinformatics.</source><year>2004</year><volume>20</volume><issue>17</issue><fpage>3146</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bth379</pub-id><pub-id pub-id-type="pmid">15231528</pub-id></element-citation></ref><ref id="CR18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tesson</surname><given-names>BM</given-names></name><name><surname>Breitling</surname><given-names>R</given-names></name><name><surname>Jansen</surname><given-names>RC</given-names></name></person-group><article-title>DiffCoEx: a simple and sensitive method to find differentially coexpressed gene modules</article-title><source>BMC Bioinformatics.</source><year>2010</year><volume>11</volume><issue>1</issue><fpage>497</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-11-497</pub-id><pub-id pub-id-type="pmid">20925918</pub-id></element-citation></ref><ref id="CR19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>CK</given-names></name><name><surname>Cho</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name></person-group><article-title>Guilt by rewiring: gene prioritization through network rewiring in genome wide association studies</article-title><source>Hum Mol Genet.</source><year>2014</year><volume>23</volume><issue>10</issue><fpage>2780</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1093/hmg/ddt668</pub-id><pub-id pub-id-type="pmid">24381306</pub-id></element-citation></ref><ref id="CR20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altman</surname><given-names>NS</given-names></name></person-group><article-title>An introduction to kernel and nearest-neighbor nonparametric regression</article-title><source>Am Stat.</source><year>1992</year><volume>46</volume><issue>3</issue><fpage>175</fpage><lpage>85</lpage></element-citation></ref><ref id="CR21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title>Support-vector networks</article-title><source>Mach Learn.</source><year>1995</year><volume>20</volume><issue>3</issue><fpage>273</fpage><lpage>97</lpage></element-citation></ref><ref id="CR22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random forests</article-title><source>Mach Learn.</source><year>2001</year><volume>45</volume><issue>1</issue><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id></element-citation></ref><ref id="CR23"><label>23</label><mixed-citation publication-type="other">Le Y, Hastie T. Sparse quadratic discriminant analysis and community bayes. arXiv preprint arXiv:1407.4543. 2014.</mixed-citation></ref><ref id="CR24"><label>24</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name><name><surname>Stone</surname><given-names>CJ</given-names></name><name><surname>Olshen</surname><given-names>RA</given-names></name></person-group><source>Classification and regression trees</source><year>1984</year><publisher-loc>Boca Raton</publisher-loc><publisher-name>CRC press</publisher-name></element-citation></ref><ref id="CR25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothman</surname><given-names>AJ</given-names></name></person-group><article-title>Positive definite estimators of large covariance matrices</article-title><source>Biometrika.</source><year>2012</year><volume>99</volume><issue>3</issue><fpage>733</fpage><lpage>40</lpage><pub-id pub-id-type="doi">10.1093/biomet/ass025</pub-id></element-citation></ref><ref id="CR26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pavlenko</surname><given-names>T</given-names></name><name><surname>Bj&#x000f6;rkstr&#x000f6;m</surname><given-names>A</given-names></name><name><surname>Tillander</surname><given-names>A</given-names></name></person-group><article-title>Covariance structure approximation via gLasso in high-dimensional supervised classification</article-title><source>J Appl Stat.</source><year>2012</year><volume>39</volume><issue>8</issue><fpage>1643</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1080/02664763.2012.663346</pub-id></element-citation></ref></ref-list></back></article>