<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d2 20140930//EN" "JATS-archivearticle1.dtd"> 
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName A++V2.4.dtd?><?SourceDTD.Version 2.4?><?ConverterInfo.XSLTName springer2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">BMC Med Genomics</journal-id><journal-id journal-id-type="iso-abbrev">BMC Med Genomics</journal-id><journal-title-group><journal-title>BMC Medical Genomics</journal-title></journal-title-group><issn pub-type="epub">1755-8794</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4522082</article-id><article-id pub-id-type="publisher-id">116</article-id><article-id pub-id-type="doi">10.1186/s12920-015-0116-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Diagnostic biases in translational bioinformatics</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Han</surname><given-names>Henry</given-names></name><address><email>xhan9@fordham.edu</email></address><xref ref-type="aff" rid="Aff1"/><xref ref-type="aff" rid="Aff2"/></contrib><aff id="Aff1"><label/>Department of Computer and Information Science, Fordham University, New York, 10023 NY USA </aff><aff id="Aff2"><label/>Quantitative Proteomics Center, Columbia University, New York, NY USA </aff></contrib-group><pub-date pub-type="epub"><day>1</day><month>8</month><year>2015</year></pub-date><pub-date pub-type="pmc-release"><day>1</day><month>8</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>8</volume><elocation-id>46</elocation-id><history><date date-type="received"><day>29</day><month>12</month><year>2014</year></date><date date-type="accepted"><day>7</day><month>7</month><year>2015</year></date></history><permissions><copyright-statement>&#x000a9; Han. 2015</copyright-statement><license license-type="open-access"><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0">http://creativecommons.org/licenses/by/4.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly credited. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p>With the surge of translational medicine and computational omics research, complex disease diagnosis is more and more relying on massive omics data-driven molecular signature detection. However, how to detect and prevent possible diagnostic biases in translational bioinformatics remains an unsolved problem despite its importance in the coming era of personalized medicine.</p></sec><sec><title>Methods</title><p>In this study, we comprehensively investigate the diagnostic bias problem by analyzing benchmark gene array, protein array, RNA-Seq and miRNA-Seq data under the framework of support vector machines for different model selection methods. We further categorize the diagnostic biases into different types by conducting rigorous kernel matrix analysis and provide effective machine learning methods to conquer the diagnostic biases.</p></sec><sec><title>Results</title><p>In this study, we comprehensively investigate the diagnostic bias problem by analyzing benchmark gene array, protein array, RNA-Seq and miRNA-Seq data under the framework of support vector machines. We have found that the diagnostic biases happen for data with different distributions and SVM with different kernels. Moreover, we identify total three types of diagnostic biases: overfitting bias, label skewness bias, and underfitting bias in SVM diagnostics, and present corresponding reasons through rigorous analysis. Compared with the overfitting and underfitting biases, the label skewness bias is more challenging to detect and conquer because it can be easily confused as a normal diagnostic case from its deceptive accuracy. To tackle this problem, we propose a derivative component analysis based support vector machines to conquer the label skewness bias by achieving the rivaling clinical diagnostic results.</p></sec><sec><title>Conclusions</title><p>Our studies demonstrate that the diagnostic biases are mainly caused by the three major factors, i.e. kernel selection, signal amplification mechanism in high-throughput profiling, and training data label distribution. Moreover, the proposed DCA-SVM diagnosis provides a generic solution for the label skewness bias overcome due to the powerful feature extraction capability from derivative component analysis. Our work identifies and solves an important but less addressed problem in translational research. It also has a positive impact on machine learning for adding new results to kernel-based learning for omics data.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Translational bioinformatics</kwd><kwd>Omics</kwd><kwd>Diagnostic biases</kwd><kwd>Machine learning</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2015</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p>With the surge of translational medicine and computational omics research, complex disease diagnosis tends to more and more rely on disease signatures discovered from the sheer enormity of high-throughput omics data [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR4">4</xref>]. Identifying disease molecular signatures from different pathological states not only captures the subtlety between disease subtypes and controls, but also provides disease gene hunting, related pathway query, genome wide association (GAWS) investigations, and following drug target identification [<xref ref-type="bibr" rid="CR5">5</xref>&#x02013;<xref ref-type="bibr" rid="CR7">7</xref>]. The translational technologies in medicine along with the exponential growth of high-throughput data in genomics, transcriptomics, and proteomics are preparing for the coming era of personalized medicine to customize medical decisions and practices to individual patients [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR8">8</xref>].</p><p>Although different state-of-the-art classifiers have been widely employed in such a massive data driven disease diagnostics to enhance diagnostic accuracy, there was almost no investigation on their diagnostic biases that are essential for the success of translational medicine [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. A diagnostic bias simply refers that a classifier cannot unbiasedly conduct diagnosis for a given input omics data in our context. Instead, it may tend to favor some phenotype or even totally ignore the other, even if the diagnostic accuracy appears to be reasonable sometimes.</p><p>In other words, given a training data consisting of <italic>m</italic> normalized omics samples <italic>x</italic><sub><italic>i</italic></sub> and its corresponding labels <italic>y</italic><sub><italic>i</italic></sub> &#x02208; {&#x02212;1,+1}, i.e. <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\{x_{i},y_{i}\}{}_{i=1}^{m},$\end{document}</tex-math><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq1.gif"/></alternatives></inline-formula> the decision function <italic>f</italic>(<italic>x</italic>|<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>&#x022ef;<italic>x</italic><sub><italic>m</italic></sub>) inferred from the classifier demonstrates some bias in determining the class type (phenotype) of a new sample <italic>x</italic><sup>&#x02217;</sup>, which is assumed to follow a same normalization procedure as the training data, due to inappropriate parameter choice, model selection, biased label distribution, or even some special characteristics of input data. It is noted that we generally assume all training and testing samples are chosen from a normalized population data for the convenience of diagnosis in our context, which avoids possible renormalization and classifier retraining overhead for the following diagnosis. For example, a diagnostic results: <italic>f</italic>(<italic>x</italic><sup>&#x02217;</sup>|<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>&#x022ef;<italic>x</italic><sub><italic>m</italic></sub>)=1 is probably obtained because almost all training samples are labeled with <sup>&#x02032;</sup>+1<sup>&#x02032;</sup>, even if the true label of <italic>x</italic><sup>&#x02217;</sup> is <italic>y</italic><sup>&#x02217;</sup>=&#x02212;1.</p><p>As a result, inaccurate or even deceptive diagnostic results would be produced and lead to an inaccurate or even totally wrong clinical decision making. In particular, such a diagnostic bias can happen to any classifiers due to different decision models, input data distributions, and/or model selection choices.</p><p>As such, a comprehensive and rigorous investigation on the diagnostic bias problem are an urgent demand from translational research. This is because a robust disease diagnostic requires a classifier achieves both efficiency and security. The efficiency means the classifier can attain a high-level diagnostic accuracy with a good generalization capability. The security refers to the classifier can unbiasedly recognize each label type by avoiding possible biases in the classifier &#x02019;s decision function inference. There are quite a lot previous studies done on the efficiency problem, but almost no previous literature addressed the security issue, i.e. the diagnostic bias problem in translational research. In particular, we need to answer the following diagnostic bias related queries: <italic>when will it happen, why does it happen, and how to conquer it and achieve efficiency?</italic></p><p>To answer these key questions, we employ support vector machines (SVM) as a representative in this study to investigate disease diagnostic bias for its rigorous decision model, good scalability, and popularity in translational medicine [<xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR13">13</xref>]. We present the following novel findings from using benchmark gene array, protein array, RNA-Seq and miRNA-Seq data in this work.</p><p>First, diagnostic biases can happen for an SVM classifier under any kernels in different model selections, whereas it is more likely to occur under nonlinear kernels. Given input data with two different phenotypes, diagnostic biases usually reflect as extremely imbalanced sensitivity and specificity values, even if they appear to achieve a reasonable diagnostic accuracy. Moreover, it seems that diagnostic biases are irrespective of data distributions: we have observed it happens to normally distributed and negative binomial distributed data.</p><p>Second, there are three types of diagnostic biases: <italic>overfitting bias</italic>, <italic>label skewness bias</italic>, and <italic>underfitting bias</italic> in SVM diagnostics. The overfitting and label skewness biases both demonstrate a majority-count phenotype favor mechanism, i.e., only majority-count samples can be recognized in diagnosis. They are mainly caused by a built-in molecular signal amplification mechanism in omics data profiling, data label skewness, and inappropriate kernel selection respectively.</p><p>The built-in signal amplification mechanism is mainly responsible for the overfitting biases. It refers that all high-throughput omics profiling systems employ real-time PCR or similar approaches to amplify gene or protein expression levels exponentially [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. The data label skewness, which is mainly responsible for the label skewness biases, means that class label distributions are skewed to some specific type of samples (e.g., positive). We define the type of samples with more counts in the label set as the majority-count type for the convenience of description. The inappropriate kernel selection simply means a wrong kernel selection lets the corresponding SVM classifier lose diagnostic capability and result in the underfitting biases.</p><p>Third, the label skewness bias is more challenging to detect and conquer because it can be easily confused as a normal diagnostic case from its deceptive accuracy. To tackle this problem, we propose a derivative component analysis based support vector machines (DCA-SVM) to conquer the label skewness bias by comparing its performance with those of the state-of-the-art peers. The proposed DCA-SVM diagnosis not only conquers the label skewness bias but also achieves rivaling clinical diagnostic results by leverage the powerful feature extraction capabilities of derivative component analysis [<xref ref-type="bibr" rid="CR16">16</xref>].</p><p>It is noted that our studies comprehensively identify different diagnostic biases and present novel effective solutions for the important but less addressed problem, Compared with our previous work in conquering SVM overfitting [<xref ref-type="bibr" rid="CR10">10</xref>], this study provides more systematic and novel results to kernel-based learning for omics data and translational bioinformatics. In particular, our studies firstly identify the label skewness bias that is usually confused as a normal diagnostic case in the past literature and provides a rivaling clinical bias overcome method. As such, it will have positive impacts on translational research and machine learning fields.</p></sec><sec id="Sec2" sec-type="methods"><title>Methods</title><p>As a widely used diagnostic method for its good scalability, support vector machines (SVM) can be described as follows. Given a training data set <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\{(x_{i},y_{i}\})_{i=1}^{m}$\end{document}</tex-math><mml:math id="M4"><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq2.gif"/></alternatives></inline-formula>, <italic>x</italic><sub><italic>i</italic></sub>&#x02208;&#x0211c;<sup><italic>n</italic></sup> with labels <italic>y</italic><sub><italic>i</italic></sub>&#x02208;{&#x02212;1,+1}, an SVM computes an optimal separating hyperplane: (<italic>w</italic>&#x000b7;<italic>x</italic>)+<italic>b</italic>=0 to attain the maximum margin between the positive and negative observations (samples), where <italic>w</italic> is the normal and bias vector of the hyperplane respectively. The margin refers to the maximal width of two boundary hyperplanes parallel to the optimal separating hyperplane.</p><p>If the training data are linearly separable, it is equivalent to finding <italic>w</italic> and <italic>b</italic> that minimize the quadratic programming (QP) problem <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\arg \min _{w,b}\frac {1}{2}||w||^{2}$\end{document}</tex-math><mml:math id="M6"><mml:mo>arg</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq3.gif"/></alternatives></inline-formula> under the condition <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$y_{i}(w\cdot x_{i}+b)-1\geqslant 0,$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02265;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq4.gif"/></alternatives></inline-formula> for each observation <italic>x</italic><sub><italic>i</italic></sub> in the training data [<xref ref-type="bibr" rid="CR13">13</xref>]. The QP problem can be solved by seeking solutions of Lagrange multipliers <italic>&#x003b1;</italic><sub><italic>i</italic></sub>&#x02265;0,<italic>i</italic>=1,2&#x022ef;<italic>m</italic>, in the following dual problem, 
<disp-formula id="Equ1"><label>(1)</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \text{max}L_{d}(\alpha)=\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}(x_{i}\cdot x_{j})  $$ \end{document}</tex-math><mml:math id="M10"><mml:mtext>max</mml:mtext><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>&#x003b1;</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12920_2015_116_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p>where <italic>w</italic> and <italic>b</italic> can be calculated by <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$w=\sum _{i=1}^{m}\alpha _{i}y_{i}x_{i}$\end{document}</tex-math><mml:math id="M12"><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq5.gif"/></alternatives></inline-formula> and <italic>y</italic><sub><italic>i</italic></sub>(<italic>w</italic>&#x000b7;<italic>x</italic><sub><italic>i</italic></sub>+<italic>b</italic>)&#x02212;1=0 respectively. As a result, the class type of an unknown sample <italic>x</italic><sup>&#x02032;</sup> can be determined as <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(x')=sign((\sum _{i=1}^{m}\alpha _{i}(x_{i}\cdot x')+b).$\end{document}</tex-math><mml:math id="M14"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">sign</mml:mtext><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mi>.</mml:mi></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq6.gif"/></alternatives></inline-formula> That is, the support vectors, which are the training samples <italic>x</italic><sub><italic>i</italic></sub> corresponding to <italic>&#x003b1;</italic><sub><italic>i</italic></sub>&#x0003e;0, totally determine diagnostics according to the spatial locations of test samples with respect to them. Geometrically, the support vectors are the data points that are closest to the optimal separating hyperplane and can be usually identified in corresponding visualization.</p><p>If the training data are not linearly separable, it means the SVM classifier can find only the optimal separating hyperplane that separates many but not all training samples. In other words, the SVM classifier permits misclassification errors in this soft margin case [<xref ref-type="bibr" rid="CR12">12</xref>]. Mathematically, it is equivalent to adding slack variables <italic>&#x003be;</italic><sub><italic>i</italic></sub> and a penalty parameter <italic>C</italic> to the original problem under <italic>L</italic><sub>1</sub> or <italic>L</italic><sub>2</sub> norms. The penalty parameter <italic>C</italic>, also called the box constraint parameter, is the upper bound of all Lagrange multipliers <italic>&#x003b1;</italic><sub><italic>i</italic></sub> in the corresponding dual problems.</p><p>For example, the original problem is updated as <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\arg \min _{w,b,\xi _{i}} \left (\frac {1}{2}||w||^{2}+C\sum _{i=1}^{m}\xi _{i}\right)$\end{document}</tex-math><mml:math id="M16"><mml:mo>arg</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003be;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003be;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq7.gif"/></alternatives></inline-formula> under the conditions <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$y_{i}(w\cdot x_{i}+b)-1\geqslant \xi _{i}$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>w</mml:mi><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>&#x02265;</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003be;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq8.gif"/></alternatives></inline-formula>, and <italic>&#x003be;</italic><sub><italic>i</italic></sub>&#x02265;0 under the <italic>L</italic><sub>1</sub> norm regularization. Similarly, the original problem is updated as <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\arg \min _{w,b,\xi _{i}}(\frac {1}{2}||w||^{2}+C\sum _{i=1}^{m}{\xi _{i}^{2}})$\end{document}</tex-math><mml:math id="M20"><mml:mo>arg</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>&#x003be;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>w</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mi>&#x003be;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq9.gif"/></alternatives></inline-formula> under the same conditions for the <italic>L</italic><sub>2</sub> norm regularization. The <italic>w</italic>,<italic>b</italic> and corresponding support vectors can be obtained by solving its corresponding dual problems [<xref ref-type="bibr" rid="CR12">12</xref>].</p><p>If the training data do not have a simple hyperplane as an effective separating criterion, they can be mapped to a higher or even infinitely dimensional feature space &#x00433; using a mapping function <italic>&#x003d5;</italic>:<italic>x</italic><sub><italic>i</italic></sub>&#x02192;&#x00433;, and constructing an optimal nonlinear decision boundary in &#x00433; to achieve more separation capabilities. Correspondingly, the decision function for an unknown sample <italic>x</italic><sup>&#x02032;</sup> is formulated as <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(x')=sign\left (\left (\sum _{i=1}^{m}\alpha _{i}(\phi (x_{i})\cdot \phi (x')\right)+b\right).$\end{document}</tex-math><mml:math id="M22"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">sign</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x000b7;</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq10.gif"/></alternatives></inline-formula> Note that the inner product (<italic>&#x003d5;</italic>(<italic>x</italic><sub><italic>i</italic></sub>)&#x000b7;<italic>&#x003d5;</italic>(<italic>x</italic><sub><italic>j</italic></sub>)) in &#x00433; can be evaluated by any kernel (<italic>&#x003d5;</italic>(<italic>x</italic><sub><italic>i</italic></sub>)&#x000b7;<italic>&#x003d5;</italic>(<italic>x</italic><sub><italic>j</italic></sub>))=<italic>k</italic>(<italic>x</italic><sub><italic>i</italic></sub>,<italic>x</italic><sub><italic>j</italic></sub>) implicitly in the input space &#x0211c;<sup><italic>n</italic></sup> if its corresponding kernel matrix is positive definite, that is, <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(x')=sign\left (\left (\sum _{i=1}^{m}\alpha _{i}k(x',x_{i})+b\right.\right).$\end{document}</tex-math><mml:math id="M24"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">sign</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfenced close="" open="(" separators=""><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq11.gif"/></alternatives></inline-formula></p><sec id="Sec3"><title>Kernel selection</title><p>Although there are a class of kernel functions available, we mainly focus on the following kernels: a Gaussian radial basis function (<italic>&#x02018;rbf</italic>&#x02019;) kernel: <italic>k</italic>(<italic>x</italic>,<italic>x</italic><sup>&#x02032;</sup>)= exp(||<italic>x</italic>&#x02212;<italic>x</italic><sup>&#x02032;</sup>||<sup>2</sup>/2<italic>&#x003c3;</italic><sup>2</sup>), quadratic kernel (<italic>&#x02018;quad&#x02019;</italic>) : <italic>k</italic>(<italic>x</italic>,<italic>x</italic><sup>&#x02032;</sup>)=(1+(<italic>x</italic><sub><italic>i</italic></sub>&#x000b7;<italic>x</italic><sup>&#x02032;</sup>))<sup>2</sup>, multilayer perceptron kernel (<italic>&#x02018;mlp&#x02019;</italic>): <italic>k</italic>(<italic>x</italic>,<italic>x</italic><sup>&#x02032;</sup>)= tanh((<italic>x</italic><sub><italic>i</italic></sub>&#x000b7;<italic>x</italic><sup>&#x02032;</sup>)&#x02212;1) kernel, and a widely-used linear kernel: <italic>k</italic>(<italic>x</italic>,<italic>x</italic><sup>&#x02032;</sup>)=(<italic>x</italic><sub><italic>i</italic></sub>&#x000b7;<italic>x</italic><sup>&#x02032;</sup>), in our experiment. In addition, we design an adjusted Gaussian kernel function: <italic>&#x02018;rbf2&#x02019;</italic>, which is obtained by tuning the bandwidth parameter as the total variations of all <italic>m</italic> training samples: <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sigma ^{2}=\frac {1}{(m-1)^{2}}\sum _{i,j}||x_{i}-x_{j}||^{2} $\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mrow><mml:mi>&#x003c3;</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq12.gif"/></alternatives></inline-formula> in the original Gaussian kernel, to demonstrate the impact of parameter tuning in enhancing SVM diagnosis under the Gaussian <italic>&#x02018;rbf&#x02019;</italic> kernel.</p><p>In practice, there are different SVM variants applied in disease diagnosis for its advantages in modeling or implementation. Least-Sequare SVM (LS-SVM) is one of those methods [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. It only employs equality constraints to reformulate the standard SVM (C-SVM). As a result, the normal <italic>w</italic> and bias <italic>b</italic> of the optimal separating hyperplane are calculated by solving linear systems instead of a quadratic programming problem [<xref ref-type="bibr" rid="CR18">18</xref>].</p><p>Previous results have reported that LS-SVM is comparable to the classic SVM in terms of performance and generalization [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. In this work, we employ LS-SVM to substitute the classic SVM in disease diagnosis for its efficiency and simplicity [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. The detailed LS-SVM implementations are chosen from Matlab R2012b bioinformatics Toolbox, which implements the <italic>L</italic><sub>2</sub> soft-margin SVM classifier [<xref ref-type="bibr" rid="CR19">19</xref>].</p></sec><sec id="Sec4"><title>SVM classifier parameterization</title><p>Since we aim at addressing generic diagnostic biases problems in translational bioinformatics through support vector machines, we do not tend to employ an SVM model with too many parameters or seek very special values in parameter setting to prevent the loss of generalization of results. As such, we employ the LS-SVM model for its built-in advantage in simplifying parameter setting than the other models [<xref ref-type="bibr" rid="CR17">17</xref>]. Moreover, we choose to set the default parameters generically in the SVM diagnosis to guarantee the reproducibility and generalization of our results.</p><p>The most important parameter in our context will be the penalty parameter <italic>C</italic>, which affects the training errors and generalization somewhat directly. A large <italic>C</italic> may produce better diagnostic results but risk the loss of the generalization of the classifier; A small <italic>C</italic> may lead to low diagnostic results but enhance the classifier&#x02019;s generalization. In our context, the penalty parameter <italic>C</italic> is chosen as 1.0 uniformly in all diagnoses instead of rescaled values for different groups of samples to guarantee comparable results for different data sets that have skewed or balanced label distributions. In particular, such a parameter choice will contribute to more comparable and easily interpretable Lagrange multipliers <italic>&#x003b1;</italic><sub><italic>i</italic></sub> values that are weights of the support vectors. Although a grid-search way can be employed to seek &#x02018;optimal&#x02019; <italic>C</italic> parameters by trying a geometric sequence such as 2<sup>&#x02212;10</sup>,2<sup>&#x02212;9</sup>,&#x02026;2<sup>0</sup>,&#x02026;2<sup>10</sup> under a specified cross validation for each data set [<xref ref-type="bibr" rid="CR13">13</xref>], such an approach may not contribute to generalizable diagnostic results and possible prohibitive training time demand.</p><p>Furthermore, we choose to automatically scale the training samples to zero mean and unit variance data before training, which is equivalent to corresponding feature scaling [<xref ref-type="bibr" rid="CR13">13</xref>], to optimize the kernel matrix&#x02019;s structure for the sake of learning efficiency and the following diagnostic generalization.</p><sec id="Sec5"><title>Model selection</title><p>We employ widely-used cross-validation methods for model selection that include <italic>k</italic>-fold cross-validation (<italic>k</italic>-fold CV) and independent training and test set approach for the sake of comprehensive diagnostic bias investigation, in addition to leave-one-out cross validation (LOOCV). The <italic>k</italic>-fold CV randomly partitions the training data to form <italic>k</italic> disjoint subsets with approximately equal size, removes the <italic>i</italic><sup><italic>t</italic><italic>h</italic></sup> subset from the training data and employs the the remaining <italic>k</italic> &#x02212;1 subsets to construct the decision function and infer the class types of the samples in the removed subset. Moreover, in the independent training and test set approach, we randomly select 50 % of input omics data for training and another 50 % for test, and repeat such a process 500 times for each data to fully investigate different diagnostic biases and validate the effectiveness of our proposed bias-conquering algorithm.</p></sec><sec id="Sec6"><title>Data selection and preprocessing</title><p>We firstly choose three benchmark omics data sets: <italic>BreastIBC</italic>, <italic>Hepatocellular carcinoma (HCC),</italic> and <italic>Kidney</italic> in our experiment, which are produced by state-of-the-art gene array, protein array and RNA-Seq technologies respectively [<xref ref-type="bibr" rid="CR20">20</xref>&#x02013;<xref ref-type="bibr" rid="CR22">22</xref>]. Table <xref rid="Tab1" ref-type="table">1</xref> illustrates the detailed information of the three data sets in platforms, sample distributions, and feature numbers, where a feature refers a gene (probe), <italic>m/z</italic> ratio, or transcript in our context.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Benchmark data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data</th><th align="left">#Feature</th><th align="left">#Sample</th><th align="left">Technology</th><th align="left">Platform</th></tr></thead><tbody><tr><td align="left">
<italic>BreastIBC</italic>
</td><td align="left">18,995</td><td align="left">13 <italic>inflammatory breast cancer (&#x02018;IBC&#x02019;)</italic>+</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left">34 <italic>non-inflammatory breast cancer (&#x02018;NIBC&#x02019;)</italic>
</td><td align="left">Gene array</td><td align="left">Affymetrix GeneChip</td></tr><tr><td align="left">
<italic>HCC</italic>
</td><td align="left">23,846</td><td align="left">78 <italic>Hepatocellular carcinoma</italic>+ <italic>72 normal</italic>
</td><td align="left">Protein array</td><td align="left">MALDI-TOF</td></tr><tr><td align="left">
<italic>Kidney</italic>
</td><td align="left">20,531</td><td align="left">
<italic>68 normal + 475 kidney renal cell carcinormal tumor</italic>
</td><td align="left">RNA-Seq</td><td align="left">IlluminaGA_RNASeq</td></tr></tbody></table></table-wrap></p><p>It is noted that these data are normalized and processed by different methods. For example, robust multiarray average (RMA) method is applied to normalize the <italic>BreastIBC</italic> data and Reads Per Kilobase per Million mapped reads (RPKM) is used to normalize <italic>Kidney</italic> the data [<xref ref-type="bibr" rid="CR23">23</xref>&#x02013;<xref ref-type="bibr" rid="CR25">25</xref>]. The original raw<italic>BreastIBC</italic> data set has been retrieved from the NCBI Gene Expression Omnibus (GEO) series data with accession number GSE5847, which consists of 13 <italic>inflammatory breast cancer (&#x02018;IBC&#x02019;)</italic> and 34 <italic>non-inflammatory breast cancer (&#x02018;NIBC&#x02019;)</italic> stromal cell samples across 22,283 probes [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR26">26</xref>]. We have further filtered small-variance genes and obtained our <italic>BreastIBC</italic> data set with 18,995 probes. The <italic>Hepatocellular carcinoma (HCC)</italic> data is a mass spectral proteomic data set generated from the MALDI-TOF platform and its detailed normalization process can be found in Ressom <italic>et al.</italic>&#x02019;s work [<xref ref-type="bibr" rid="CR20">20</xref>].</p><p>It is noted that both <italic>BreastIBC</italic> and <italic>HCC data</italic> are subject to normal distributions, and the <italic>Kidney</italic> data are subject to negative binomial (NB) distributions approximately [<xref ref-type="bibr" rid="CR25">25</xref>]. In addition, the sample label distributions of these data are also different. The <italic>HCC data</italic> have an almost balanced distribution: 78 <italic>Hepatocellular carcinoma vs 72 normal</italic>samples. But the <italic>BreastIBC and Kidney</italic> data have obviously skewed label distributions, where the majority count samples are much more than the minority count samples (e.g. 13 <italic>&#x02018;IBC&#x02019; vs</italic> 34 <italic>&#x02018;NIBC&#x02019;</italic> in the <italic>BreastIBC</italic> data; <italic>68 normal vs 475 renal cell carcinormal tumor</italic> samples in the <italic>Kidney</italic> data).</p></sec></sec></sec><sec id="Sec7" sec-type="results"><title>Results</title><p>We introduce the following set of measures for the sake of diagnostic bias investigations: diagnostic accuracy, sensitivity, specificity, positive predictive ratio (PPR), and negative predictive ratio (NPR). The diagnostic accuracy is the ratio of the correctly diagnosed test samples (targets) over total test samples (targets), i.e. <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$accuracy=\frac {TP+TN}{TP+FP+TN+FN},$\end{document}</tex-math><mml:math id="M28"><mml:mtext mathvariant="italic">accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq13.gif"/></alternatives></inline-formula> where TP (TN) is the number of positive (negative) samples correctly diagnosed, and FP (FN) is the number of negative (positive) samples incorrectly diagnosed. The sensitivity, specificity, and positive predictive ratio (PPR) are defined as <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$sensitivity=\frac {TP}{TP+FN},$\end{document}</tex-math><mml:math id="M30"><mml:mtext mathvariant="italic">sensitivity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq14.gif"/></alternatives></inline-formula> and <inline-formula id="IEq15"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$specificity=\frac {TN}{TN+FP},PPR=\frac {TP}{TP+FP},$\end{document}</tex-math><mml:math id="M32"><mml:mtext mathvariant="italic">specificity</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext mathvariant="italic">PPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$NPR=\frac {TN}{TN+FN}$\end{document}</tex-math><mml:math id="M34"><mml:mtext mathvariant="italic">NPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq16.gif"/></alternatives></inline-formula> respectively. It is noted that we use targets and samples interchangeably in this study.</p><p>We conduct SVM diagnosis under a 5-fold cross validation for the three data sets under the following kernels: <italic>&#x02018;linear&#x02019;</italic>, <italic>&#x02018;quad&#x02019;</italic>, <italic>&#x02018;mlp&#x02019;</italic>, <italic>&#x02018;rbf&#x02019;</italic>, and <italic>&#x02018;rbf2&#x02019;</italic>, where the bandwidth parameter <italic>&#x003c3;</italic><sup>2</sup> in the <italic>&#x02018;rbf&#x02019; and &#x02018;rbf2&#x02019;</italic> kernels are selected as 1 and the total variations of all training samples respectively. It is noted that each sample in the training data is scaled as a zero mean sample with variance 1.0 before building the optimal separation plane in SVM diagnostics. Table <xref rid="Tab2" ref-type="table">2</xref> illustrates the SVM diagnoses for the three benchmark data sets with five kernels under the 5-fold cross validation. We have the following interesting findings about diagnostic biases.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>SVM diagnosis for benchmark data under 5-fold cross validation</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left">Accuracy &#x000b1; std (%)</th><th align="left">Sensitivity &#x000b1; std (%)</th><th align="left">Specificity &#x000b1; std (%)</th><th align="left">NPR &#x000b1; std (%)</th><th align="left">PPR &#x000b1; std (%)</th></tr></thead><tbody><tr><td align="left"/><td align="left"/><td align="left">
<italic>BreastIBC data</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">74.56 &#x000b1; 04.52</td><td align="left">97.14 &#x000b1; 06.39</td><td align="left">16.67 &#x000b1; 23.67</td><td align="left">NaN</td><td align="left">75.70 &#x000b1; 06.52</td></tr><tr><td align="left">
<italic>SVM-rbf</italic>
</td><td align="left">72.56 &#x000b1; 03.63</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">00.00 &#x000b1; 00.00</td><td align="left">NaN</td><td align="left">72.56 &#x000b1; 03.63</td></tr><tr><td align="left">
<italic>SVM-quad</italic>
</td><td align="left">74.56 &#x000b1; 04.52</td><td align="left">97.14 &#x000b1; 06.39</td><td align="left">16.67 &#x000b1; 23.67</td><td align="left">NaN</td><td align="left">75.70 &#x000b1; 06.52</td></tr><tr><td align="left">
<italic>SVM-rbf2</italic>
</td><td align="left">72.83 &#x000b1; 10.92</td><td align="left">85.71 &#x000b1; 14.29</td><td align="left">40.00 &#x000b1; 09.13</td><td align="left">63.33 &#x000b1; 34.16</td><td align="left">78.65 &#x000b1; 05.88</td></tr><tr><td align="left">
<italic>SVM-mlp</italic>
</td><td align="left">45.67 &#x000b1; 18.09</td><td align="left">48.10 &#x000b1; 22.99</td><td align="left">40.00 &#x000b1; 09.13</td><td align="left">25.67 &#x000b1; 14.02</td><td align="left">65.33 &#x000b1; 12.16</td></tr><tr><td align="left"/><td align="left"/><td align="left">
<italic>HCC data</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">94.02 &#x000b1; 01.43</td><td align="left">95.81 &#x000b1; 03.83</td><td align="left">92.42 &#x000b1; 05.21</td><td align="left">96.17 &#x000b1; 03.50</td><td align="left">92.39 &#x000b1; 05.00</td></tr><tr><td align="left">
<italic>SVM-rbf</italic>
</td><td align="left">52.00 &#x000b1; 00.75</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">00.00 &#x000b1; 00.00</td><td align="left">NaN</td><td align="left">52.00 &#x000b1; 00.75</td></tr><tr><td align="left">
<italic>SVM-quad</italic>
</td><td align="left">82.05 &#x000b1; 10.66</td><td align="left">77.00 &#x000b1; 10.77</td><td align="left">87.52 &#x000b1; 12.21</td><td align="left">77.87 &#x000b1; 10.32</td><td align="left">87.38 &#x000b1; 11.89</td></tr><tr><td align="left">
<italic>SVM-rbf2</italic>
</td><td align="left">89.90 &#x000b1; 04.32</td><td align="left">92.86 &#x000b1; 08.75</td><td align="left">87.17 &#x000b1; 06.48</td><td align="left">93.60 &#x000b1; 07.25</td><td align="left">87.33 &#x000b1; 05.32</td></tr><tr><td align="left">
<italic>SVM-mlp</italic>
</td><td align="left">51.87 &#x000b1; 10.96</td><td align="left">46.00 &#x000b1; 15.80</td><td align="left">58.29 &#x000b1; 10.06</td><td align="left">50.43 &#x000b1; 08.72</td><td align="left">53.44 &#x000b1; 14.68</td></tr><tr><td align="left"/><td align="left"/><td align="left">
<italic>Kidney data</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">90.23 &#x000b1; 02.35</td><td align="left">96.84 &#x000b1; 03.07</td><td align="left">44.07 &#x000b1; 06.63</td><td align="left">71.46 &#x000b1; 16.90</td><td align="left">92.38 &#x000b1; 00.71</td></tr><tr><td align="left">
<italic>SVM-rbf</italic>
</td><td align="left">87.48 &#x000b1; 00.44</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">00.00 &#x000b1; 00.00</td><td align="left">NaN</td><td align="left">87.48 &#x000b1; 00.44</td></tr><tr><td align="left">
<italic>SVM-quad</italic>
</td><td align="left">87.47 &#x000b1; 01.70</td><td align="left">94.47 &#x000b1; 01.20</td><td align="left">17.47 &#x000b1; 07.89</td><td align="left">50.00 &#x000b1; 21.21</td><td align="left">89.21 &#x000b1; 00.80</td></tr><tr><td align="left">
<italic>SVM-rbf2</italic>
</td><td align="left">87.48 &#x000b1; 00.44</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">00.00 &#x000b1; 00.00</td><td align="left">NaN</td><td align="left">87.48 &#x000b1; 00.44</td></tr><tr><td align="left">
<italic>SVM-mlp</italic>
</td><td align="left">53.39 &#x000b1; 06.79</td><td align="left">54.32 &#x000b1; 07.79</td><td align="left">46.92 &#x000b1; 10.08</td><td align="left">13.02 &#x000b1; 02.95</td><td align="left">87.67 &#x000b1; 02.47</td></tr></tbody></table></table-wrap></p><sec id="Sec8"><title>Three diagnostic biases</title><p>The diagnostic biases would take place in an SVM classifier with any kernels, but it is more likely to occur under nonlinear kernels. In fact, they can happen for almost all SVM classifiers under three different scenarios: <italic>overfitting bias, label skewness bias</italic>, and <italic>underfitting bias</italic>. It is worthwhile to point out that the overfitting bias and label skewness bias may demonstrate similar diagnostic results, whereas they are caused by different reasons.</p><sec id="Sec9"><title>Overfitting biases</title><p>The overfitting bias demonstrates the majority-count phenotype favor mechanism in diagnosis under the nonlinear kernels like <italic>&#x02018;rbf&#x02019;</italic>. That is, the SVM classifier will always diagnose an unknown sample as the type of the samples with the majority-count in the training data (e.g., <italic>&#x02018;NIBC&#x02019;</italic> type for the <italic>BreastIBC</italic> data). Finally, its diagnostic accuracy will equal or approximate the majority-count ratio of the input data. For example, the SVM with the <italic>&#x02018;rbf&#x02019;</italic> kernel <italic>(SVM-rbf)</italic> has the diagnostic accuracies that approximate or totally equal to their corresponding majority-count ratios for the three data sets : <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$72.56\,\%\thickapprox \frac {34}{34+13}=72.34\,\%, 52.00\,\%=\frac {78}{78+72},$\end{document}</tex-math><mml:math id="M36"><mml:mn>72.56</mml:mn><mml:mspace width="0.3em"/><mml:mi>%</mml:mi><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mrow><mml:mn>34</mml:mn></mml:mrow><mml:mrow><mml:mn>34</mml:mn><mml:mo>+</mml:mo><mml:mn>13</mml:mn></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>72.34</mml:mn><mml:mspace width="0.3em"/><mml:mi>%</mml:mi><mml:mo>,</mml:mo><mml:mn>52.00</mml:mn><mml:mspace width="0.3em"/><mml:mi>%</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>78</mml:mn></mml:mrow><mml:mrow><mml:mn>78</mml:mn><mml:mo>+</mml:mo><mml:mn>72</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq17.gif"/></alternatives></inline-formula> and <inline-formula id="IEq18"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$87.48\,\%=\frac {475}{475+68}$\end{document}</tex-math><mml:math id="M38"><mml:mn>87.48</mml:mn><mml:mspace width="0.3em"/><mml:mi>%</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>475</mml:mn></mml:mrow><mml:mrow><mml:mn>475</mml:mn><mml:mo>+</mml:mo><mml:mn>68</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq18.gif"/></alternatives></inline-formula> respectively.</p></sec><sec id="Sec10"><title>Why does NaN appear in diagnostic results?</title><p>The question is why the corresponding <italic>NPR</italic> is NaN in diagnostics (Table <xref rid="Tab2" ref-type="table">2</xref>)? The reason is that the classifier can only recognize the majority-count samples that are specified as the positive type target in our experiment. That is, each trial of diagnoses has a zero count for true negative and false negative, i.e. <italic>T</italic><italic>N</italic>=0 and <italic>F</italic><italic>N</italic>=0, because all negative targets, which are minority-count samples in our experiment, are diagnosed as the positive type. As a result, <inline-formula id="IEq19"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$NPR=\frac {TN}{TN+FN}$\end{document}</tex-math><mml:math id="M40"><mml:mtext mathvariant="italic">NPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq19.gif"/></alternatives></inline-formula> will be NaN. So are the corresponding sensitivity values always 100 % <inline-formula id="IEq20"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left (\frac {TP}{TP+FN}=\frac {TP}{TP}=1.0\right)$\end{document}</tex-math><mml:math id="M42"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq20.gif"/></alternatives></inline-formula> and the specificity values 0 % (<inline-formula id="IEq21"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\frac {TN}{TN+FP}=\frac {0}{FP}=0.0,$\end{document}</tex-math><mml:math id="M44"><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">FP</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.0</mml:mn><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq21.gif"/></alternatives></inline-formula> where <italic>FP</italic> is actually totally number of negative samples that appear as the minority-count samples in our diagnostic experiments).</p><p>Similarly, the SVM with the <italic>&#x02018;rbf2&#x02019;</italic> kernel also demonstrates similar diagnostic results as before, where <italic>&#x02018;rbf2&#x02019;</italic> is obtained by tuning the bandwidth parameter in the original Gaussian kernel. Although they may show some improvements for the protein array data (<italic>HCC</italic> data), they still demonstrate the major-phenotype favor mechanism for the gene array and RNA-Seq data. Alternatively, it indicates that simply tuning the bandwidth parameter may not be a good way to conquer such an diagnostic bias.</p></sec><sec id="Sec11"><title>Label skewness biases</title><p>Unlike the overfitting bias, the label skewness bias demonstrates two different cases. The first is that the SVM classifiers with a linear or nonlinear kernel (e.g., <italic>&#x02018;quad&#x02019;</italic>) demonstrate <italic>an explicit label skewness diagnostic bias</italic> by presenting a diagnostic accuracy close to the majority-count ratio and a pair of unbalanced sensitivity and specificity. For example, Table <xref rid="Tab1" ref-type="table">1</xref> shows that both <italic>SVM-linear</italic> and <italic>SVM-quad</italic> classifiers achieve a 74.56 % accuracy that is close to the majority-count ratio: 72.34 <italic>%</italic> with an imbalanced sensitivity 97.14 % and specificity 16.67 % respectively for the <italic>BreastIBC</italic> data. This indicates such a model can recognize few negative targets in one or more diagnostic trials in addition to diagnosing all positive targets and most of negative targets to the positive target type, which is the majority-count type specified in our implementations.</p><p>The second is that a linear kernel SVM demonstrates <italic>an implicit label skewness diagnostic bias</italic> by presenting a normal diagnostic accuracy but with a pair of imbalanced sensitivity and specificity. For example, the <italic>SVM-linear</italic> classifier achieves 90.23 <italic>%</italic> accuracy with sensitivity 96.84 % and specificity 44.07 %. Such a result indicates there are a large number of false positives than those of false negatives due to the dominance of the positive type in the training data.</p><p>It is noted that not all linear kernels would encounter diagnostic bias. Instead, the <italic>SVM-linear</italic> classifier achieves 94.02 <italic>%</italic> accuracy with 95.81 <italic>%</italic> sensitivity and 94.21 <italic>%</italic> specificity for the <italic>Hepatocellular carcinoma</italic> (<italic>HCC</italic>) data with 78 <italic>HCC</italic> and 72 normal samples that have a more balanced label distribution than those of the <italic>BreastIBC</italic> and <italic>Kidney</italic> data.</p></sec><sec id="Sec12"><title>Underfitting biases</title><p>The <italic>underfitting bias</italic> refers that an SVM classifier with a nonlinear kernel such as <italic>&#x02018;mlp&#x02019;</italic> leads to an underfitting model in diagnostics. The model itself is inappropriate for disease diagnostics because the high-dimensional feature selection space generated from the kernel function may distort the information conveyed by the original data [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. As a result, the SVM classifier will have a quite low diagnostic performance due to the underfitting. For example, the <italic>SVM-mlp</italic> classifier has about 50 % level diagnostic accuracy for all the three data sets. That is, the classifier is equivalent to a random classifier that conducts almost ad-hoc diagnosis because of the underfitting bias.</p><p>Finally, it is clear that the diagnostic biases seem to be irrespective of data distributions. They happen for the gene and protein array data that are subject to normal distributions and RNA-Seq count data that are subject to negative binomial (NB) distributions in our experiment [<xref ref-type="bibr" rid="CR25">25</xref>].</p></sec></sec><sec id="Sec13"><title>Diagnostic biases under other cross validations</title><p>It is worthwhile to point that diagnostic biases can also happen in other cross validations such as independent training and test set approach and leave-one-out cross validation (LOOCV) besides the <italic>k</italic>-fold cross validation. This is because diagnostic biases may occur in each diagnostic trial under a specific kernel due to the built-in characteristics of input data we will mention in the next section. For example, we generate 100 independent training and test sets for the <italic>BreastIBC</italic> data, where each sample has a 50 % likelihood to be selected in the training and test set. The <italic>SVM-rbf</italic> and <italic>SVM-linear</italic> classifiers has the almost same performance as illustrated in Table <xref rid="Tab2" ref-type="table">2</xref>. For example, the former has the average accuracy: 72.70 % &#x000b1; 6.48 % with sensitivity: 100.00 &#x000b1; 0.00 % and specificity: 00.00 &#x000b1; 0.00 %; the latter has the average accuracy: 73.83 % &#x000b1; 7.02 % with sensitivity: 92.87 % &#x000b1; 6.58 % and specificity: 25.45 % &#x000b1; 15.82 %. It is noted that similar results can be also found for this data set under the LOOCV.</p><sec id="Sec14"><title>What are the reasons for diagnostic biases?</title><p>The are different reasons for the three different diagnostic biases, though the overfitting bias and label skewness bias may demonstrate similar diagnostic results.</p><p>The reason for the overfitting bias is rooted in the large or even huge pairwise distances <italic>d</italic><sub><italic>ij</italic></sub>=||<italic>x</italic><sub><italic>i</italic></sub>&#x02212;<italic>x</italic><sub><italic>j</italic></sub>]]<sup>1/2</sup> between omics samples, which implies that the corresponding distances in the feature space under the <italic>&#x02019;rbf&#x02019;</italic> kernel <italic>k</italic>(<italic>x</italic><sub><italic>i</italic></sub>,<italic>x</italic><sub><italic>j</italic></sub>)= exp(&#x02212;||<italic>x</italic><sub><italic>i</italic></sub>&#x02212;<italic>x</italic><sub><italic>j</italic></sub>||<sup>2</sup>/2) will be a zero or tiny value approximate to zero. As a result, it leads to an identity or approximately identity kernel matrix that causes the SVM classifier to recognize the majority-count type samples only.</p><p>Figure <xref rid="Fig1" ref-type="fig">1</xref> illustrates the box-plots of all pairwise sample distance squares <inline-formula id="IEq22"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$d_{\textit {ij}}^{2}, (i\neq j)$\end{document}</tex-math><mml:math id="M46"><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>&#x02260;</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq22.gif"/></alternatives></inline-formula> in each data set in the first row of plots and kernel matrices of the three data sets under the <italic>&#x02018;rbf&#x02019;</italic> kernel in the second row of plots by viewing each data set as the population of training data. It is interesting to see that the the minimum <inline-formula id="IEq23"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$d_{\textit {ij}}^{2}$\end{document}</tex-math><mml:math id="M48"><mml:msubsup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq23.gif"/></alternatives></inline-formula> are greater than 10<sup>2</sup>, which means the distance between any two samples in the feature space will be approximately zero: <italic>k</italic>(<italic>x</italic><sub><italic>i</italic></sub>,<italic>x</italic><sub><italic>j</italic></sub>)&#x02264; exp(&#x02212;10<sup>2</sup>/2)&#x0223c;10<sup>&#x02212;22</sup>. As a result, the corresponding kernel matrix will be an identity matrix as illustrated by the corresponding plot in the second row.
<fig id="Fig1"><label>Fig. 1</label><caption><p>The kernel matrices of the overfitting bias. The first row illustrates the box-plots of all pairwise sample distance squares in each data. The second row lists the kernel matrices of the three data sets under the <italic>&#x02018;rbf&#x02019;</italic> kernel (<italic>&#x003c3;</italic>=1), where each data is viewed as the population of training data, are identity matrices</p></caption><graphic xlink:href="12920_2015_116_Fig1_HTML" id="MO1"/></fig></p><p>It is noted that the large or even huge pairwise sample distances in each omics dataset are actually rooted in the molecular signal amplification mechanism in high-throughput profiling, where gene array, protein array and RNA-Seq technologies all employ real-time PCR or similar approaches to amplify gene and protein expression levels exponentially [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. As a result, the molecular signals greatly increase the sensitivity of disease phenotype and corresponding genotypes in diagnostics [<xref ref-type="bibr" rid="CR28">28</xref>]. On the other hand, the pairwise distances between two samples are large or even huge mathematically, even if each sample is standardized as a zero-mean point with unit standard deviation.</p><p>The label skewness bias is due to the skewness of the label distributions that lead to there are more support vectors from the majority-count type samples and the class type of an unknown sample is more likely to be determined as the majority-count type. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the distributions of <italic>&#x003b1;</italic> values, i.e., the Lagrange multipliers&#x02019; values: <italic>&#x003b1;</italic><sub>1</sub>,<italic>&#x003b1;</italic><sub>2</sub>&#x022ef;<italic>&#x003b1;</italic><sub><italic>m</italic></sub> in the dual problem, in each diagnostic trial in the 5-fold cross validation. As the weights of corresponding support vectors, its values are always positive or zero as we pointed out before. However, the sign of a weight is assigned in our SVM implementation for the convenience of indicating its class property, i.e. a positive (negative) sign means this weight (e.g. <italic>&#x003b1;</italic><sub>1</sub>) is for the support vector belonging to the positive (negative) target group. It is easy to detect that the distributions of <italic>&#x003b1;</italic> values are nearly balanced for the <italic>Hepatocellular carcinoma (HCC)</italic> data that has a relatively balanced sample label distributions, where the number of positive signs are almost equal as that of the negative signs. However, the the distributions of <italic>&#x003b1;</italic> values of the <italic>BreastIBC</italic> and <italic>Kidney</italic> data are obviously skewed to the positive targets, which are the majority-count samples in each data set. In other words, more support vectors can be found for the majority-count type, which will increase the likelihood of an unknown sample to be detected as the majority-count type in the following decision making. For example, since there are 256 and 178 <italic>&#x003b1;</italic> values carrying the positive and negative signs respectively in the 5<sup><italic>th</italic></sup> trial of diagnosis for the <italic>Kidney</italic> data, there will be a more likelihood for a test sample to be detected as a positive target.
<fig id="Fig2"><label>Fig. 2</label><caption><p>The distributions of <italic>&#x003b1;</italic> values. The distributions of <italic>&#x003b1;</italic> values of each diagnostic trial in the 5-fold cross validation for three data sets. The skewness of sample label distribution leads to the skewness of the distributions of <italic>&#x003b1;</italic> values of the diagnoses of the <italic>BreastIBC</italic> and <italic>Kidney</italic> data sets. The signs of the <italic>&#x003b1;</italic> values indicate the group property of corresponding support vectors. As such, more support vectors can be found for the majority-count type, which will increase the likelihood of an unknown sample to be detected as the majority-count type in diagnosis</p></caption><graphic xlink:href="12920_2015_116_Fig2_HTML" id="MO2"/></fig></p><p>On the other hand, the corresponding <italic>b</italic> values, which are the intercepts of the hyperplane that separates the two groups in the normalized data space, are all positive in each trial. For example, the <italic>b</italic> values of the five diagnostic trials for the <italic>Kidney</italic>and <italic>BreastIBC</italic> data are [0.7425, 0.7603, 0.7333, 0.7649, 0.7465] and [0.4594, 0.4210, 0.4594, 0.4594, 0.4359] respectively. As such, given a test sample <italic>x</italic><sup>&#x02032;</sup>, the decision function <inline-formula id="IEq24"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(x')=sign((\sum _{i=1}^{k}\alpha _{i}k(x',x_{i})+b)$\end{document}</tex-math><mml:math id="M50"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">sign</mml:mtext><mml:mo>(</mml:mo><mml:mo>(</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq24.gif"/></alternatives></inline-formula> is more likely to determine it as the positive type, because most support vectors are from the positive type (the majority-count type) and the intercept value <italic>b</italic> is positive.</p><p>The underfitting bias is caused by the inappropriate kernel function such as <italic>&#x02018;mlp&#x02019;</italic> that results in a kernel matrix with all entries are &#x02018;1&#x02019;s that has no any capability to distinguish different samples. To some degree, it corresponds an extreme case for an SVM classifier under the Gaussian kernel with a too large bandwidth parameter that also leads to the kernel matrix with all &#x02018;1&#x02019; entries. It is noted that the underfitting bias is also independent of input data label distributions as the overfitting and label-skewness bias, though it corresponds to a kernel matrix with all &#x02018;1&#x02019; entries instead of an identity kernel matrix as the former or a normal kernel matrix as the latter.</p><p>Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the <italic>&#x02018;mlp&#x02019;</italic> and <italic>&#x02018;linear&#x02019;</italic> kernel matrices of the three data sets, where each data is treated as a training population. It is clear to see that the kernel matrices under the underfitting bias are flat matrices with all &#x02018;1&#x02019; entries, but the kernel matrices under the linear kernel appear to be normal for all three data sets, even if there are <italic>explicit</italic> and <italic>implicit</italic> label skewness biases for the <italic>BreastIBC</italic> and <italic>Kidney</italic> data respectively.
<fig id="Fig3"><label>Fig. 3</label><caption><p>The comparisons of the kernel matrices in the label skewness and underfitting biases. The comparisons of the kernel matrices of the underfitting bias (<italic>&#x02018;mlp&#x02019;</italic> kernels) and those of the linear kernels for the three data sets. The linear kernel matrices appear to be normal ones though the label skewness bias happens to the <italic>BreastIBC</italic> and <italic>Kidney</italic> data</p></caption><graphic xlink:href="12920_2015_116_Fig3_HTML" id="MO3"/></fig></p></sec><sec id="Sec15"><title>Diagnostic bias conquering</title><p>There are no systematic approaches available to conquer diagnostic biases due to the gap between machine learning and translational bioinformatics [<xref ref-type="bibr" rid="CR10">10</xref>]. Although previously related work has been proposed to investigate imbalanced data in SVM classification in data mining, all of these work mainly focus on the <italic>&#x02018;imbalanced data&#x02019;</italic> where the sample label distributions are extremely imbalanced (e.g., 99.5 % positive labels and 0.5 % negative labels) [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>]. Moreover, these imbalanced data are not high-through omics data that do not have <italic>&#x02018;large number of variables but small number of observations&#x02019;</italic> characteristics shared by all high-throughput omics data [<xref ref-type="bibr" rid="CR11">11</xref>]. Thus, a more general but omics data focused algorithm is needed to overcome the diagnostic biases.</p><p>The overfitting and underfitting biases can be &#x02018;conquered&#x02019; by avoiding using the corresponding kernels that lead to the identity, nearly identity, or all &#x02018;1&#x02019; entries kernel matrices. However, it can be challenging to conquer the label skewness bias, especially the implicit diagnostic bias case that has <italic>&#x02018;reasonable&#x02019;</italic> diagnostic accuracy but unbalanced sensitivity and specificity.</p><p>In this work, we propose a derivative component analysis (DCA) based support vector machines (DCA-SVM) to conquer the label skewness bias by extracting true signals by digging latent data characteristics from an input data [<xref ref-type="bibr" rid="CR16">16</xref>]. The true signals share the same dimensionally with the original data but capture essential data characteristics. We introduce DCA briefly as follows and more details about this algorithm can be found in Han&#x02019;s previous work on DCA [<xref ref-type="bibr" rid="CR16">16</xref>].</p></sec></sec><sec id="Sec16"><title>Derivative component analysis (DCA)</title><p><list list-type="order"><list-item><p><bold>Input:</bold><italic>X</italic><sup><italic>t</italic></sup>=[<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>&#x022ef;<italic>x</italic><sub><italic>n</italic></sub>],<italic>x</italic><sub><italic>i</italic></sub>&#x02208;&#x0211c;<sup><italic>p</italic></sup>, DWT level <italic>J</italic>; cutoff <italic>&#x003c4;</italic>; wavelet <italic>&#x003c8;</italic>, variability explanation threshold <italic>&#x003c1;</italic></p></list-item><list-item><p><bold>Output:</bold> true signals: <italic>X</italic><sup>&#x02217;</sup></p></list-item><list-item><p><bold>Step 1:</bold> Conduct<italic>J</italic>-level DWT with wavelet <italic>&#x003c8;</italic> for <italic>X</italic><sup><italic>t</italic></sup> to obtain coefficient detail <italic>c</italic><italic>D</italic><sub><italic>j</italic></sub> and approximation matrix <italic>c</italic><italic>A</italic>:[<italic>c</italic><italic>D</italic><sub>1</sub>,<italic>c</italic><italic>D</italic><sub>2</sub>,&#x022ef;,<italic>c</italic><italic>D</italic><sub><italic>J</italic></sub>;<italic>c</italic><italic>A</italic><sub><italic>J</italic></sub>], where <inline-formula id="IEq25"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${cD}_{j}\in \Re ^{p_{j}\times n}, {cA}_{J}\in \Re ^{p_{J}\times n},p_{j}=\left \lceil {{p}/{2^{j}}}\right \rceil $\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mrow><mml:mtext mathvariant="italic">cD</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211c;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mtext mathvariant="italic">cA</mml:mtext></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211c;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>J</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="&#x02309;" open="&#x02308;" separators=""><mml:mrow><mml:mi>p</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq25.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p><bold>Step 2:</bold> Extract subtle data characteristics, remove system noise and retrieve global data characteristics 
<list list-type="alpha-lower"><list-item><p>Conduct PCA for <italic>c</italic><italic>D</italic><sub><italic>j</italic></sub>,1&#x02264;<italic>j</italic>&#x02264;<italic>&#x003c4;</italic> to obtain its PC matrix <italic>U</italic> and score matrix <italic>S</italic>: <inline-formula id="IEq26"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$U=[u_{1},u_{2},\cdots u_{p_{j}}], u_{i}\in \Re ^{n}$\end{document}</tex-math><mml:math id="M54"><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211c;</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq26.gif"/></alternatives></inline-formula> and score matrix <inline-formula id="IEq27"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$S=[s_{1},s_{2}\cdots s_{p_{j}}], s_{i}\in \Re ^{p_{j}}, i=1,2\cdots p_{j}$\end{document}</tex-math><mml:math id="M56"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211c;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x022ef;</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq27.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p>Identify PCs <italic>u</italic><sub><italic>i</italic></sub>,<italic>u</italic><sub>2</sub>&#x022ef;<italic>u</italic><sub><italic>m</italic></sub>, such that its variability explanation ratio <italic>&#x003c1;</italic><sub><italic>m</italic></sub>&#x02265;<italic>&#x003c1;</italic></p></list-item><list-item><p>Reconstruct <inline-formula id="IEq28"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${cD}_{j}\leftarrow \frac {1}{p_{j}}{cD}_{j}\vec {(1)}\vec {(1)}^{T}+\sum _{i=1}^{m}u_{i}\times {s_{i}^{T}}, \vec {(1)}\in \Re ^{p_{j}}$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow><mml:mtext mathvariant="italic">cD</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02190;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mtext mathvariant="italic">cD</mml:mtext></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#x02192;</mml:mo></mml:mover><mml:mo>&#x02208;</mml:mo><mml:msup><mml:mrow><mml:mi>&#x0211c;</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq28.gif"/></alternatives></inline-formula> with all entries being &#x02018;1&#x02019;s</p></list-item><list-item><p>Reconstruct <italic>c</italic><italic>D</italic><sub><italic>j</italic></sub>,<italic>&#x003c4;</italic>&#x02264;<italic>j</italic>&#x02264;<italic>J</italic> and <italic>c</italic><italic>A</italic><sub><italic>J</italic></sub> under the variability explanation ratio at least 95 %</p></list-item></list></p></list-item><list-item><p><bold>Step 3:</bold> Approximate the original data by the corresponding inverse DWT with the wavelet <italic>X</italic><sup>&#x02217;</sup>&#x02190;<italic>i</italic><italic>n</italic><italic>v</italic><italic>e</italic><italic>r</italic><italic>s</italic><italic>e</italic><italic>D</italic><italic>W</italic><italic>T</italic>([<italic>c</italic><italic>D</italic><sub>1</sub>,<italic>c</italic><italic>D</italic><sub>2</sub>&#x022ef;<italic>c</italic><italic>D</italic><sub><italic>J</italic></sub>;<italic>c</italic><italic>A</italic><sub><italic>J</italic></sub>]).</p></list-item></list></p><p>In our implementation, we uniformly set the transform level <italic>J</italic>=7 for the wavelet <italic>`</italic><italic>d</italic><italic>b</italic>8<sup>&#x02032;</sup>, cutoff <italic>&#x003c4;</italic>=2, and apply the first PC-based detail coefficient matrix reconstruction in DCA for the convenience of implementations [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR31">31</xref>].</p></sec><sec id="Sec17"><title>Derivative component analysis based support vector machines (DCA-SVM)</title><p>Given training data <italic>X</italic>=[<italic>x</italic><sub>1</sub>,<italic>x</italic><sub>2</sub>&#x022ef;<italic>x</italic><sub><italic>p</italic></sub>]<sup><italic>T</italic></sup> and their labels <inline-formula id="IEq29"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\{x_{i},c_{i}\}_{i=1,}^{p}c_{i}\!\in \!\{-1,1\},$\end{document}</tex-math><mml:math id="M60"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mo>&#x02208;</mml:mo><mml:mspace width="0.3em"/><mml:mo>{</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq29.gif"/></alternatives></inline-formula> its corresponding true signals <italic>Y</italic>=[<italic>y</italic><sub>1</sub>,<italic>y</italic><sub>2</sub>&#x022ef;<italic>y</italic><sub><italic>p</italic></sub>]<sup><italic>T</italic></sup> are computed by using DCA, Then, a maximum-margin hyperplane: <italic>O</italic><sub><italic>h</italic></sub>:<italic>w</italic><sup><italic>T</italic></sup><italic>&#x003d5;</italic>(<italic>y</italic>)+<italic>b</italic>=0 in the feature space is constructed to separate the &#x02018;+1&#x02019; (&#x02018;cancer&#x02019;) and &#x02018;-1&#x02019; (&#x02018;control&#x02019;) types of the samples in true signals <italic>Y</italic>, which is equivalent to solving the following optimization problem with a parameter <italic>&#x003bc;</italic>&#x0003e;0, 
<disp-formula id="Equ2"><label>(2)</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{array}{c} \text{\ensuremath{\text{min}_{w,b,e}}}\frac{1}{2}w^{T}w+\frac{1}{2\mu}\sum_{i=1}^{p}\left(c_{i}-w^{T}\phi(y_{i})-b\right)^{2}\\ \text{s.t.}\,\ensuremath{e_{i}=c_{i}-w^{T}\phi(y_{i})-b},i=1,2\cdots p\\ \end{array}  $$ \end{document}</tex-math><mml:math id="M62"><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:mtext/><mml:msub><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mtext/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>&#x003d5;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>s.t.</mml:mtext><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msup><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mi>&#x003d5;</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&#x02212;</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x022ef;</mml:mo><mml:mi>p</mml:mi></mml:mtd></mml:mtr><mml:mtr/></mml:mtable></mml:math><graphic xlink:href="12920_2015_116_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p>The dual problem of this constrained minimization problem can be formulated as follows, where <italic>k</italic>(<italic>y</italic><sub><italic>i</italic></sub>,<italic>y</italic><sub><italic>j</italic></sub>)=(<italic>&#x003d5;</italic>(<italic>y</italic><sub><italic>i</italic></sub>)&#x000b7;<italic>&#x003d5;</italic>(<italic>y</italic><sub><italic>j</italic></sub>)) 
<disp-formula id="Equ3"><label>(3)</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{array}{c} \sum_{i=1}^{p}\alpha_{i}k(y_{i},y_{j})+b+\mu=c_{i},\text{}\text{}i=1,2\cdots p\\ s.t. \sum_{i=1}^{p}\alpha_{i}=0\\ \end{array}  $$ \end{document}</tex-math><mml:math id="M64"><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>&#x003bc;</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext/><mml:mtext/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>&#x022ef;</mml:mo><mml:mi>p</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>s.t.</mml:mi><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr/></mml:mtable></mml:math><graphic xlink:href="12920_2015_116_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p><p>The <italic>b</italic> and <italic>&#x003b1;</italic><sub><italic>i</italic></sub>,<italic>i</italic>=1,2&#x022ef;<italic>p</italic> can be obtained by solving the corresponding linear system of the dual problem. The decision rule <inline-formula id="IEq30"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(x')=sign\left (\sum _{i=1}^{p}\alpha _{i}k(y_{i},y')+b\right)$\end{document}</tex-math><mml:math id="M66"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext mathvariant="italic">sign</mml:mtext><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munderover><mml:mrow><mml:mo>&#x02211;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>&#x003b1;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>k</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>&#x02032;</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq30.gif"/></alternatives></inline-formula> is used to determine the class type of a testing sample <italic>x</italic><sup>&#x02032;</sup>, where <italic>y</italic><sup>&#x02032;</sup> is its corresponding vector computed from DCA. The function <italic>k</italic>(<italic>y</italic><sub><italic>i</italic></sub>,<italic>y</italic><sup>&#x02032;</sup>) is a kernel function mapping <italic>y</italic><sub><italic>i</italic></sub> and <italic>y</italic><sup>&#x02032;</sup> into a same-dimensional or high-dimensional feature space, which is chosen as the linear kernel <italic>k</italic>(<italic>y</italic><sub><italic>i</italic></sub>,<italic>y</italic><sup>&#x02032;</sup>)=(<italic>y</italic><sub><italic>i</italic></sub>&#x000b7;<italic>y</italic><sup>&#x02032;</sup>) in our experiment.</p></sec><sec id="Sec18"><title>Random undersampling Boost (<italic>RUBoost</italic>)</title><p>To demonstrate the effectiveness of the proposed algorithm, we include an ensemble learning method: random undersampling Boost (<italic>RUBoost</italic>) as well as the original SVM as comparison algorithms [<xref ref-type="bibr" rid="CR29">29</xref>]. The reason we choose the ensemble learning method is because it is believed to perform well for imbalanced data [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]. We employ an ensemble of 1000 deep trees that have minimal leaf size of 5 with a learning rate 0.1 in <italic>RUBoost</italic> learning to attain a high ensemble accuracy.</p><p>Table <xref rid="Tab3" ref-type="table">3</xref> compares the performance of the proposed DCA-SVM with those of SVM and <italic>RUBoost</italic> under the 5-fold cross validation. It is interesting to see that our algorithm not only fully conquer the label skewness biases for the <italic>BreastIBC</italic> and <italic>Kidney</italic> data, but also achieve exceptional diagnostic results for all three data sets for its latent data characteristics extraction that forces a data characteristics driven diagnosis. It is noted that the extracted latent data characteristics contribute to the structure optimization of the kernel matrices that enhance the classifier&#x02019;s detectability [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>].
<table-wrap id="Tab3"><label>Table 3</label><caption><p>The three diagnostics under 5-fold cross validation</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left">Accuracy &#x000b1; std (%)</th><th align="left">Sensitivity &#x000b1; std (%)</th><th align="left">Specificity &#x000b1; std (%)</th><th align="left">NPR &#x000b1; std (%)</th><th align="left">PPR &#x000b1; std (%)</th></tr></thead><tbody><tr><td align="left"/><td align="left"/><td align="left">
<italic>BreastIBC data</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>DCA-SVM</italic>
</td><td align="left">97.78 &#x000b1; 04.97</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">90.00 &#x000b1; 22.36</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">97.50 &#x000b1; 05.59</td></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">74.56 &#x000b1; 04.52</td><td align="left">97.14 &#x000b1; 06.39</td><td align="left">16.67 &#x000b1; 23.67</td><td align="left">NaN</td><td align="left">75.70 &#x000b1; 06.52</td></tr><tr><td align="left">
<italic>RUBoost</italic>
</td><td align="left">73.33 &#x000b1; 00.00</td><td align="left">53.33 &#x000b1; 44.72</td><td align="left">82.86 &#x000b1; 18.63</td><td align="left">83.17 &#x000b1; 15.86</td><td align="left">54.67 &#x000b1; 44.07</td></tr><tr><td align="left"/><td align="left"/><td align="left">
<italic>HCC data</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>DCA-SVM</italic>
</td><td align="left">99.33 &#x000b1; 01.49</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">98.57 &#x000b1; 03.19</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">98.82 &#x000b1; 02.63</td></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">94.02 &#x000b1; 01.43</td><td align="left">95.81 &#x000b1; 03.83</td><td align="left">92.42 &#x000b1; 05.21</td><td align="left">96.17 &#x000b1; 03.50</td><td align="left">92.39 &#x000b1; 05.00</td></tr><tr><td align="left">
<italic>RUBoost</italic>
</td><td align="left">85.23 &#x000b1; 00.00</td><td align="left">82.08 &#x000b1; 11.98</td><td align="left">88.76 &#x000b1; 06.30</td><td align="left">82.56 &#x000b1; 10.14</td><td align="left">88.64 &#x000b1; 06.54</td></tr><tr><td align="left"/><td align="left"/><td align="left">
<italic>Kidney data</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>DCA-SVM</italic>
</td><td align="left">99.81 &#x000b1; 00.41</td><td align="left">99.79 &#x000b1; 00.47</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">98.57 &#x000b1; 03.19</td><td align="left">100.0 &#x000b1; 00.00</td></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">90.23 &#x000b1; 02.35</td><td align="left">96.84 &#x000b1; 03.07</td><td align="left">44.07 &#x000b1; 06.63</td><td align="left">71.46 &#x000b1; 16.90</td><td align="left">92.38 &#x000b1; 00.71</td></tr><tr><td align="left">
<italic>RUBoost</italic>
</td><td align="left">87.47 &#x000b1; 00.00</td><td align="left">90.95 &#x000b1; 03.54</td><td align="left">63.08 &#x000b1; 11.17</td><td align="left">51.31 &#x000b1; 12.17</td><td align="left">94.55 &#x000b1; 01.42</td></tr></tbody></table></table-wrap></p><p>For example, the explicit label skewness diagnostic bias illustrated in the <italic>BreastIBC</italic> data is overcome by achieving 97.78 % diagnostic accuracy with 100 % sensitivity and 90 % specificity. Unlike all negative targets are recognized as the positive targets in some diagnostic trial, the total negative prediction rate (NPR) is 100 % and the positive prediction rate (PPR) is 97 %. Moreover, the implicit label skewness diagnostic bias illustrated in the <italic>Kidney</italic> data is overcome by achieving 99.81 % diagnostic accuracy with 99.79 % sensitivity and 100 % specificity, compared to the original 90.23 % diagnostic accuracy with 96.84 % sensitivity and 44.07 % specificity.</p><p>Furthermore, DCA-SVM achieves the exceptional diagnostics on the <italic>HCC</italic>data by attaining 99.33 % diagnostic accuracy with 100 % sensitivity and 98.57 % specificity compared to the original 94.02 % accuracy with 95.81 % sensitivity and 92.42 % specificity. Alternatively, the <italic>RUBoost</italic> diagnosis has some improvements in balancing the sensitivity and specificity, whereas it has relatively low diagnostic accuracy, especially for balanced <italic>HCC</italic> data, and needs a long learning time.</p><p>Figure <xref rid="Fig4" ref-type="fig">4</xref> compares the ROC plots of DCA-SVM, SVM, PCA-SVM, ICA-SVM diagnoses under the 5-fold cross validation for the <italic>BreastIBC</italic> and <italic>Kidney</italic> data [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR33">33</xref>]. It is easy to see that the proposed DCA-SVM diagnosis conquers the label skewness bias by achieving the best performance, which prepares itself as a good candidate in personalized diagnostics in the coming personalized medicine for its unbiased exceptional diagnostic performance for different omics data. It is worthwhile to point out that such a rivaling clinical-level diagnosis is mainly because the true signals extraction in DCA that forces the SVM hyperplane construction to rely on both subtle and global data characteristics of the whole profile in a de-noised feature space, which seems to contribute to a robust and consistent high-accuracy diagnosis greatly. In fact, since such a consistent performance applies to different data sets rather than work only on an individual data set, it almost prevents from any overfitting possibility. Moreover, the following two subsections further demonstrate such an exceptional performance is impossible from overfitting because our proposed algorithm works well consistently for different data sets with different training and test data selection methods. Especially, the phenotype separation results in Fig. <xref rid="Fig5" ref-type="fig">5</xref> strongly validate the effectiveness from a biomarker discovery and visualization standing point.
<fig id="Fig4"><label>Fig. 4</label><caption><p>ROC plots. The ROC plots of DCA-SVM, SVM, PCA-SVM, ICA-SVM diagnoses under the 5-fold cross validation for the <italic>BreastIBC</italic> and <italic>Kidney</italic> data</p></caption><graphic xlink:href="12920_2015_116_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>The phenotype separation. The phenotype separation for four different data sets: <italic>GliomaRNASeq</italic> (LGG RNA-Seq), <italic>GliomaMiRNASeq</italic> (LGG MiRNA-Seq), <italic>Kidney</italic> (Kidney (KIRC) RNA-Seq), and <italic>HCC</italic> (HCC MALDI-TOF) by using the top three biomarkers</p></caption><graphic xlink:href="12920_2015_116_Fig5_HTML" id="MO5"/></fig></p><sec id="Sec19"><title>Independent data sets: brain low grade glioma (LGG) TCGA data</title><p>To further demonstrate the effectiveness of our proposed algorithm, we have retrieved level-3 TCGA data for brain low grade gliomas (LGG) from the TCGA portal that include gene expression, protein expression, RNA-Seq and miRNA-Seq data [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR35">35</xref>]. The LGG refers to the grade I and grade II glioma tumors that are usually considered as benign brain tumors compared with those grade II and IV glioma tumors. Since the gene and protein expression data only contain grade-I glioma samples that prevent us doing diagnostics from a translational bioinformatics viewpoint, we include the RNA-Seq and miRNA-Seq data as the independent data sets: <italic>GliomaRNASeq</italic> and <italic>GliomaMiRNASeq</italic> for our algorithm testing. The detailed information about the two data sets can be found in the Table <xref rid="Tab4" ref-type="table">4</xref>, where each feature refers to a gene or microRNA.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Brain Low grade Glioma (LGG) TCGA data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data</th><th align="left">#Feature</th><th align="left">#Sample</th><th align="left">Technology</th><th align="left">Platform</th></tr></thead><tbody><tr><td align="left">
<italic>GliomaRNASeq</italic>
</td><td align="left">20,531</td><td align="left">18 <italic>grade-I Glioma tumors</italic> +</td><td align="left" colspan="2"/></tr><tr><td align="left"/><td align="left"/><td align="left">
<italic>516 grade-II Glioma tumors</italic>
</td><td align="left">RNA-Seq</td><td align="left">IlluminaHiSeq_RNASeqV2</td></tr><tr><td align="left">
<italic>GliomaMiRNASeq</italic>
</td><td align="left">1046</td><td align="left">18 <italic>grade-I Glioma tumors</italic> +</td><td align="left" colspan="2"/></tr><tr><td align="left"/><td align="left"/><td align="left">
<italic>512 grade-II Glioma tumors</italic>
</td><td align="left">miRNA-Seq</td><td align="left">IlluminaHiSeq_miRNASeq</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec20"><title>Normalization</title><p>It is noted that both are <italic>&#x02018;imbalanced data&#x02019;</italic>, where 96.63 % and 95.88 % samples are grade-II tumors respectively, and follow the negative binomial (NB) distribution approximately. The raw <italic>GliomaRNASeq</italic> data, a big data that asks 14.5 Gigebytes storage, is normalized by dividing each sample with a scale factor <italic>s</italic>=<italic>Q</italic><sub>3</sub>/1000, where <italic>Q</italic><sub>3</sub> is the 75-percentile of each sample. The raw data is normalized by the <italic>count-per-million</italic> method, in which all counts in a sample are adjusted to reads per million to facilitate comparison between samples [<xref ref-type="bibr" rid="CR36">36</xref>].</p></sec><sec id="Sec21"><title>Monte Carlo simulation oriented training and test data selection</title><p>Different from the previous <italic>k</italic>-fold cross-validation, we randomly select 50 % of Glioma RNA-Seq (miRNA-Seq) samples for training and another 50 % for test, and repeat such a process 500 times in our diagnostic experiments. It is noted that such a Monte Carlo simulation oriented independent training and test data choice will have an advantage to evaluate the effectiveness of the proposed algorithm than the previous <italic>k</italic>-fold cross-validation. This is because it reduces the dependence between training and test data by fully leveraging the two omics data sets with a large number of observations.</p><p>Table <xref rid="Tab5" ref-type="table">5</xref> compares the diagnostic results of DCA-SVM, with SVM under four different kernels: <italic>&#x02018;linear&#x02019;</italic>, <italic>&#x02018;rbf&#x02019;</italic>, <italic>&#x02018;quad&#x02019;</italic> and <italic>&#x02018;mlp&#x02019;</italic>for the two data sets. It is not a surprise that the SVM-<italic>mlp</italic>classifier encounters the underfitting bias for both <italic>LGG</italic> data sets by demonstrating quite low diagnostic accuracy values. Similarly, the SVM-<italic>rbf</italic> classifier still suffers from the overfitting bias by only recognizing the majority count phenotypes. That is, its average diagnostic accuracy closely approximates the majority count ratios of the <italic>GliomaRNASeq</italic> and <italic>GliomaMiRNASeq</italic> data sets <inline-formula id="IEq31"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$96.68\,\%\approx \frac {516}{516+18}$\end{document}</tex-math><mml:math id="M68"><mml:mn>96.68</mml:mn><mml:mspace width="0.3em"/><mml:mi>%</mml:mi><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mrow><mml:mn>516</mml:mn></mml:mrow><mml:mrow><mml:mn>516</mml:mn><mml:mo>+</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq31.gif"/></alternatives></inline-formula> and <inline-formula id="IEq32"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$96.63\,\%\approx \frac {512}{512+18}$\end{document}</tex-math><mml:math id="M70"><mml:mn>96.63</mml:mn><mml:mspace width="0.3em"/><mml:mi>%</mml:mi><mml:mo>&#x02248;</mml:mo><mml:mfrac><mml:mrow><mml:mn>512</mml:mn></mml:mrow><mml:mrow><mml:mn>512</mml:mn><mml:mo>+</mml:mo><mml:mn>18</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq32.gif"/></alternatives></inline-formula> respectively. For the same reason, its average positive prediction rate will just be its diagnostic accuracy because the SVM-<italic>rbf</italic> classifier diagnoses all samples into the positive samples. Alternatively, the corresponding negative prediction ratio <inline-formula id="IEq33"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$NPR=\frac {TN}{TN+FN}$\end{document}</tex-math><mml:math id="M72"><mml:mtext mathvariant="italic">NPR</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq33.gif"/></alternatives></inline-formula> is NaN because of <italic>T</italic><italic>N</italic>=<italic>F</italic><italic>N</italic>=0 in each diagnostic case, and the sensitivity and specificity are 100 % and 0 % respectively.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>The diagnostic results with independent training and test sets for LGG data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left">Accuracy &#x000b1; std (%)</th><th align="left">Sensitivity &#x000b1; std (%)</th><th align="left">Specificity &#x000b1; std (%)</th><th align="left">NPR &#x000b1; std (%)</th><th align="left">PPR &#x000b1; std (%)</th></tr></thead><tbody><tr><td align="left"/><td align="left"/><td align="left">
<italic>GliomaRNASeq</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>DCA-SVM</italic>
</td><td align="left">99.52 &#x000b1; 00.58</td><td align="left">99.64 &#x000b1; 00.53</td><td align="left">97.00 &#x000b1; 08.08</td><td align="left">91.18 &#x000b1; 11.05</td><td align="left">99.87 &#x000b1; 00.36</td></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">95.87 &#x000b1; 00.84</td><td align="left">98.77 &#x000b1; 00.00</td><td align="left">12.10 &#x000b1; 11.46</td><td align="left">NaN</td><td align="left">97.02 &#x000b1; 00.85</td></tr><tr><td align="left">
<italic>SVM-rbf</italic>
</td><td align="left">96.68 &#x000b1; 00.78</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">00.00 &#x000b1; 00.00</td><td align="left">NaN</td><td align="left">96.68 &#x000b1; 00.78</td></tr><tr><td align="left">
<italic>SVM-quad</italic>
</td><td align="left">96.53 &#x000b1; 00.75</td><td align="left">99.60 &#x000b1; 00.40</td><td align="left">07.40 &#x000b1; 08.67</td><td align="left">NaN</td><td align="left">96.91 &#x000b1; 00.79</td></tr><tr><td align="left">
<italic>SVM-mlp</italic>
</td><td align="left">56.28 &#x000b1; 05.61</td><td align="left">56.73 &#x000b1; 05.95</td><td align="left">43.77 &#x000b1; 18.61</td><td align="left">03.34 &#x000b1; 01.53</td><td align="left">96.70 &#x000b1; 01.31</td></tr><tr><td align="left"/><td align="left"/><td align="left">
<italic>GliomaMiRNASeq</italic>
</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">
<italic>DCA-SVM</italic>
</td><td align="left">99.63 &#x000b1; 00.52</td><td align="left">99.73 &#x000b1; 00.39</td><td align="left">97.52 &#x000b1; 08.21</td><td align="left">93.13 &#x000b1; 09.40</td><td align="left">99.89 &#x000b1; 00.39</td></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">93.78 &#x000b1; 01.27</td><td align="left">96.68 &#x000b1; 01.58</td><td align="left">10.93 &#x000b1; 10.04</td><td align="left">10.89 &#x000b1; 11.08</td><td align="left">96.89 &#x000b1; 00.84</td></tr><tr><td align="left">
<italic>SVM-rbf</italic>
</td><td align="left">96.63 &#x000b1; 00.81</td><td align="left">100.0 &#x000b1; 00.00</td><td align="left">00.00 &#x000b1; 00.00</td><td align="left">NaN</td><td align="left">96.63 &#x000b1; 00.81</td></tr><tr><td align="left">
<italic>SVM-quad</italic>
</td><td align="left">95.65 &#x000b1; 00.97</td><td align="left">98.76 &#x000b1; 00.87</td><td align="left">06.14 &#x000b1; 07.39</td><td align="left">NaN</td><td align="left">96.80 &#x000b1; 00.79</td></tr><tr><td align="left">
<italic>SVM-mlp</italic>
</td><td align="left">56.62 &#x000b1; 06.31</td><td align="left">58.16 &#x000b1; 06.69</td><td align="left">42.51 &#x000b1; 18.98</td><td align="left">03.45 &#x000b1; 01.74</td><td align="left">96.68 &#x000b1; 01.30</td></tr></tbody></table></table-wrap></p><p>Also like the previous cases, the SVM-<italic>linear</italic> and SVM-<italic>quad</italic> classifiers both encounter the explicit label skewness bias because both data sets are imbalanced where the <italic>GliomaRNASeq</italic> data has 18 grade I and 516 grade II gliomas and the <italic>GliomaMiRNASeq</italic>data has 18 grade I and 512 grade II gliomas respectively.</p><p>The explicit label skewness bias demonstrates a deceptive diagnostic accuracy that is close to the majority-count ratio for each data. For example, the SVM-<italic>linear</italic> classifier achieves an average accuracy 95.87 % and 93.78 % for the two data sets respectively, both of which are close to the majority-count ratios 96.68 <italic>%</italic> and 96.63 <italic>%</italic>. However, both diagnostic results are characterized by imbalanced sensitivity &#x00026; specificity, and positive &#x00026; negative prediction rates. For example, the SVM-<italic>linear</italic> classifier achieves 98.77 % sensitivity and 12.10 % specificity.</p><p>Although its average negative predication ratio (NPR) appears to be NaN, such an exception is caused by the fact that both <italic>TN</italic> and <italic>FN</italic> are zero counts in some trials of diagnosis, due to the major-count phenotype favor mechanism. In fact, it is easy to estimate that its average NPR should be a small percentage, because the corresponding average PPR is 97.02 <italic>%</italic>, i.e. very few negative targets or even none are correctly diagnosed in each diagnosis. As such, the <italic>&#x02018;high&#x02019;</italic> diagnostic accuracy does not mean the classifiers have high detection capabilities. Instead, the high&#x02019; diagnostic accuracy is from the high majority-count ratio.</p><p>However, the proposed DCA-SVM algorithm successfully overcomes the diagnostic biases and achieves rivaling-clinical diagnostic accuracy and balanced sensitivity and specificity for the two data sets. In particular, we still employ the transform level <italic>J</italic>=7 and cutoff <italic>&#x003c4;</italic>=2, in addition to keeping the first PC-based detail coefficient matrix reconstruction in DCA for the sake of consistence.</p><p>Such a result is consistent with the previous results from gene/protein expression and RNA-Seq data with <italic>k</italic>-fold cross validation. For example, our DCA-SVM classifier achieves 99.52 % (sensitivity: 99.64 %, specificity: 97.00 %, NPR: 91.98 %, PPR: 99.87 %) and 99.63 % (sensitivity: 99.73 %, specificity: 97.52 %, NPR: 93.13 %, PPR: 99.89 %) average diagnostic accuracy for the <italic>GliomaRNASeq</italic>and <italic>GliomaMiRNASeq</italic>data. Considering different types of omics data and different training and test data selections, such a result strongly suggests the effectiveness of our proposed method in conquering the diagnostic biases.</p></sec><sec id="Sec22"><title>Diagnostic index</title><p>We create a diagnostic index <inline-formula id="IEq34"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\beta =-\log _{2}a-\log _{2}\frac {s+p}{2},$\end{document}</tex-math><mml:math id="M74"><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mrow><mml:mo>log</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mi>a</mml:mi><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mrow><mml:mo>log</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq34.gif"/></alternatives></inline-formula> where <italic>a</italic>,<italic>s</italic>, and <italic>p</italic> represent accuracy, sensitivity and specificity to evaluate if a classifier is subject to any diagnostic biases. A small diagnostic index value (e.g., <italic>&#x003b2;</italic>=0.01) means the classifier achieves a good accuracy with a light degree diagnostic bias. The smallest diagnostic index refers to the perfect diagnosis for a classifier: <italic>a</italic>=<italic>s</italic>=<italic>p</italic>=100 <italic>%</italic>. Alternatively, a large <italic>&#x003b2;</italic> (e.g., 2.0) means classifier achieves a poor diagnostic accuracy or a high degree diagnostic bias. Table <xref rid="Tab6" ref-type="table">6</xref> compares the diagnostic index values of the proposed DCA-SVM with those of the other classifiers. It is interesting to see that its <italic>&#x003b2;</italic> values are the lowest among all diagnostic index values, which validate again the effectiveness of the proposed algorithm in conquering the label skewness bias and achieving rivaling clinical diagnostic results.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>The diagnostic index</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Diagnostic index</th><th align="left"/></tr></thead><tbody><tr><td align="left">Algorithm</td><td align="left">
<italic>GliomaRNASeq</italic>
</td><td align="left">
<italic>GliomaMiRNASeq</italic>
</td></tr><tr><td align="left">
<italic>DCA-SVM</italic>
</td><td align="left">0.0314</td><td align="left">0.0235</td></tr><tr><td align="left">
<italic>SVM-linear</italic>
</td><td align="left">0.9123</td><td align="left">0.9868</td></tr><tr><td align="left">
<italic>SVM-rbf</italic>
</td><td align="left">1.0487</td><td align="left">1.0495</td></tr><tr><td align="left">
<italic>SVM-quad</italic>
</td><td align="left">0.9533</td><td align="left">0.9951</td></tr><tr><td align="left">
<italic>SVM-mlp</italic>
</td><td align="left">1.8221</td><td align="left">1.7857</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec23"><title>Derivative component analysis based phenotype separation</title><p>We create a diagnostic index <inline-formula id="IEq35"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\beta =-\log _{2}a-\log _{2}\frac {s+p}{2}$\end{document}</tex-math><mml:math id="M76"><mml:mi>&#x003b2;</mml:mi><mml:mo>=</mml:mo><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mrow><mml:mo>log</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mi>a</mml:mi><mml:mo>&#x02212;</mml:mo><mml:munder><mml:mrow><mml:mo>log</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12920_2015_116_Article_IEq35.gif"/></alternatives></inline-formula>, where <italic>a</italic>,<italic>s</italic>, and <italic>p</italic> represent accuracy, sensitivity and specificity to evaluate if a classifier is subject to any diagnostic biases. A small diagnostic index value (e.g., <italic>&#x003b2;</italic>=0.01) means the classifier achieves a good accuracy with a light degree diagnostic bias. The smallest diagnostic index refers to the perfect diagnosis for a classifier: <italic>a</italic>=<italic>s</italic>=<italic>p</italic>=100 <italic>%</italic>. Alternatively, a large <italic>&#x003b2;</italic> (e.g., 2.0) means classifier achieves a poor diagnostic accuracy or a high degree diagnostic bias. Table <xref rid="Tab6" ref-type="table">6</xref> compares the diagnostic index values of the proposed DCA-SVM with those of the other classifiers. It is interesting to see that its <italic>&#x003b2;</italic> values are the lowest among all diagnostic index values, which validate again the effectiveness of the proposed algorithm in conquering the label skewness bias and achieving rivaling clinical diagnostic results.</p></sec><sec id="Sec24"><title>Derivative component analysis based phenotype separation</title><p>The diagnostic results from the proposed DCA-SVM classifier indicates that the high-dimensional omics data in our experiment are linear separable after derivative component analysis. In other words, it means that support vectors can be found to separate the two groups of samples geometrically according to the definition of linear separability [<xref ref-type="bibr" rid="CR12">12</xref>]. On the other hand, it suggests that disease biomarkers can be identified from the omics data to discriminate different phenotypes in such a translational bioinformatics based disease diagnostics. As such, we demonstrate the following biomarker discovery method that captures disease biomarkers and a visualization technique that show the possible support vectors in phenotype separation, that is to further &#x02018;prove&#x02019; and validate the effectiveness of our proposed algorithm.</p><p>Our biomarker discovery method assumes the normal distribution of input data. If an input data is not normally distributed, we conduct a transform <bold>Y</bold><bold>=</bold><bold>E</bold><bold>(</bold><bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold><bold>)</bold><bold>/</bold><bold>v</bold><bold>a</bold><bold>r</bold><bold>(</bold><bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold><bold>)</bold> to covert it to a corresponding normally distributed data approximately. It is noted that <bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold> is obtained by element-wisely applying the log transform to <bold>X</bold><bold>+</bold><bold>1</bold><bold>,</bold> which adds each entry in input data <bold>X</bold> by 1. Similarly, <bold>E</bold><bold>(</bold><bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold><bold>)</bold> updates <bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold> by adjusting its column with its corresponding mean, and <bold>v</bold><bold>a</bold><bold>r</bold><bold>(</bold><bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold><bold>)</bold> is the matrix, each column of which is a vector consisting of the variance of <bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold> at the column, and Y is obtained by the element-wise division between <bold>E</bold><bold>(</bold><bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold><bold>)</bold> and <bold>v</bold><bold>a</bold><bold>r</bold><bold>(</bold><bold>log</bold><bold>(</bold><bold>X</bold><bold>+</bold><bold>1</bold><bold>)</bold><bold>)</bold>.</p><p>Then, derivative component analysis (DCA) is applied to the normally distributed omics data to retrieve its true signals by using the same parameter setting in the previous experiments. Finally, the classic two-sample <italic>t-test</italic> is employed to identify the differentially expressed features (e.g. genes) with the smallest <italic>p</italic>-values from the extracted true signals as potential biomarkers. It is worthwhile to point out that a large amount of tiny <italic>p</italic>-values will come from the <italic>t</italic>-test due to the de-noising process in DCA. Although we can get a set of well-supported biomarkers from the statistical test applied to the true signals, we prefer to employ the top three biomarkers to conduct phenotype separation and corresponding support vector finding for the convenience of visualization.</p><p>Figure <xref rid="Fig5" ref-type="fig">5</xref> shows the corresponding phenotype separations for four data sets from different high-throughput technologies and platforms: <italic>GliomaRNASeq</italic> (LGG RNA-Seq), <italic>GliomaMiRNASeq</italic> (LGG MiRNA-Seq), <italic>Kidney</italic> (Kidney (KIRC) RNA-Seq), and <italic>HCC</italic> (HCC MALDI-TOF), by using its top three biomarkers. Each yellow/red dot in the visualization represents a corresponding sample. For example, the 18 yellow dots represent 18 grade I glioma samples in the NW plot for LGG RNA-Seq data. It is interesting to see that the three biomarkers discovered from each data set demonstrate the linear-separability very well and corresponding support vectors can be easily found from each phenotype separation.</p><p>Such results strongly suggest the effectiveness of our proposed algorithm and provides a visualization support for DCA-SVM&#x02019;s rivaling clinical diagnostic performance. Furthermore, it provides more insights to elucidate the latent structures of the omics data, which can contribute to deciphering the different pathological sub-states of tumors. For example, the NE sub-figure discloses that 512 grade II tumors of the <italic>GliomaMiRNASeq</italic> data span three different clusters, which may indicate that grade II tumors may have different pathological sub-states due to different genetic alternations [<xref ref-type="bibr" rid="CR35">35</xref>]. It is also noted that such results also apply to the <italic>BreastIBC data</italic> though it is not included in Fig. <xref rid="Fig5" ref-type="fig">5</xref>.</p></sec></sec></sec><sec id="Sec25" sec-type="discussion"><title>Discussion</title><p>In this work, we comprehensively investigate diagnostic bias in translational bioinformatics by using support vector machines (SVM). It is worthwhile to point that the overfitting bias and underfitting bias can be viewed as special diagnostic biases associated with the kernel-based learning, though they still happen in the other classifier-based diagnosis. However, the label skewness bias can be found widely found in the other classifiers, because the SVM classifiers with different kernels can be viewed as the &#x02018;simulations&#x02019; of different classifiers [<xref ref-type="bibr" rid="CR12">12</xref>]. For example, an <italic>SVM-linear</italic> classifier can be viewed as a simulation of linear discriminant analysis (LDA), because they usually have a similar or same level performance [<xref ref-type="bibr" rid="CR37">37</xref>]. In fact, LDA does demonstrate label skewness diagnostic bias on the <italic>BreastIBC</italic> data under the same cross validation by achieving 71.83 % accuracy with 94.17 % sensitivity and 15 % specificity.</p><p>We also have employed a multi-layer perceptron (MLP) classifier to the five data sets used to investigate the occurrence of diagnostic biases for its comparable performance with respect to SVM and other classifiers such as decision trees [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. We still use the 5-fold cross validation is still for the convenience of comparisons. The MLP classifier has 10 neurons in its input layer, two hidden layers, each of which has 5 neurons, and two neurons in its output layer. The Levenberg-Marquardt optimization is employed to train the network, in which the maximum number of epochs and minimum performance gradient in training are set as 10<sup>3</sup> and 10<sup>&#x02212;9</sup> respectively [<xref ref-type="bibr" rid="CR40">40</xref>]. We are interesting to find that it encounters different diagnostic biases on almost all data sets under the 5-fold cross validation except the <italic>Hepatocellular carcinoma (HCC)</italic> data, where it has an accuracy 85.91 % with sensitivity 90.29 % and specificity 81.92 %. For example, it achieves 92.18 % accuracy (sensitivity 95.40 %, specificity: 0.0 %) for the <italic>GliomaRNASeq</italic>data, and 96.07 % accuracy (sensitivity 99.40 %, specificity: 1.08 %) for the <italic>GliomaMiRNASeq</italic> data respectively. Obviously, it encounters overfitting diagnosis by diagnosing all test samples as the majority count samples with an approximately zero specificity. In addition, it demonstrates the explicit label skewness biases for the <italic>Kidney</italic> and <italic>BreastIBC</italic> data with low diagnostic accuracy: 79.73 % (sensitivity: 14.45 %, specificity: 89.09 %) and 65.78 % (sensitivity: 85.71 %, specificity: 13.33 %) respectively. All these results strongly demonstrate the generalization of our proposed diagnostic biases.</p><p>Unlike other ad-hoc diagnostic bias conquering by tuning parameters, the proposed DCA-SVM demonstrates rivaling-clinical level diagnostic results by overcoming both explicit and implicit label skewness biases. Although some statistical test-based feature selection can conquer some diagnostic bias well for some data, it may not be generalized to other data with different distributions. For example, the <italic>SVM-linear</italic> classifier can achieve a quite excellent diagnostic performance on the <italic>BreastIBC</italic> data with an average diagnostic accuracy 98.00 % (sensitivity: 100 %, specificity: 93.33 %) under the <italic>5</italic>-fold cross validation, if we only pick the top-ranked 200 genes (features) from this data by using Bayesian <italic>t-</italic>test [<xref ref-type="bibr" rid="CR41">41</xref>]. However, if we apply the same feature selection approach to the <italic>Hepatocellular carcinoma (HCC)</italic> data, the classifier only attains a mediocre performance with an average diagnostic accuracy 88.03 % (sensitivity: 84.76 %, specificity: 91.08 %), which is far from the more than 94 %-level diagnostic accuracy achieved by the same classifier without using any feature selection. On the other hand, such a normal distribution assumed feature selection method can not apply to the RNA-Seq and MiRNA-Seq data directly, because these data are not normally distributed. Thus, such a feature filtering approach can not be a good choice for overcoming diagnostic biases. Alternatively, our derivative component analysis (DCA) is a generic feature extraction algorithm that does not have special data distribution requirements but retrieve true signals from each omics data by capturing essential data behaviors. As such, the proposed DCA-SVM diagnosis can be viewed as a generic solution for the diagnostic bias problem in translational bioinformatics.</p><p>Although we assume training and testing samples are picked from a normalized population in our context, our method can still work well provide the testing samples are not normalized or normalized with a different approach as the training ones. The renormalization process will be required but it can be different for different types of omics data. For example, the renormalization for microarray data is usually done by normalizing all the training and testing samples before retraining the classifier in diagnostics [<xref ref-type="bibr" rid="CR42">42</xref>, <xref ref-type="bibr" rid="CR43">43</xref>]. This is mainly because microarray data generally has strong background-signals that make the comparisons of expression levels between genes within a single sample impossible [<xref ref-type="bibr" rid="CR44">44</xref>, <xref ref-type="bibr" rid="CR45">45</xref>]. Due to its fundamentally different data generation mechanism as microarray data, RNA-Seq or MiRNA-Seq data can compare different genes&#x02019; expression levels within a single sample [<xref ref-type="bibr" rid="CR44">44</xref>]. As such, the renormalization for such type of data can be done by only conducting normalization for each testing sample by using corresponding normalization methods (e.g. DESeq-normalization) before the proposed diagnosis [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR46">46</xref>].</p></sec><sec id="Sec26" sec-type="conclusion"><title>Conclusions</title><p>Our studies comprehensively investigate the diagnostic bias problem in translational bioinformatics by analyzing benchmark gene array, protein array, RNA-Seq and miRNA-Seq data. We identify three types of diagnostic biases: overfitting bias, label skewness bias, and underfitting bias in SVM diagnosis, and disclose the reasons for its occurrence through rigorous analysis. As we pointed out before, the diagnostic biases, which happen at almost all kernels and data with different distributions, are actually caused by three major factors, that is, kernel selection, special signal amplification mechanism in the high throughput profiling, and training data label distribution.</p><p>Interestingly, the overfitting bias and label skewness bias both demonstrate a majority-count phenotype favor mechanism in diagnosis, which means that only majority-count samples can be recognized in diagnosis. However, the former is rooted in the molecular signal amplification mechanism in high-throughput profiling that leads to the large or even huge pairwise distances in the training data. The latter is caused by the unbalanced label distributions in the training data.</p><p>Unlike other diagnostic biases, the label skewness bias is hard to detect and conquer, especially the implicit label skewness bias that usually demonstrate quite normal or even some good diagnostic accuracy but with imbalanced sensitivity and specificity. Our studies propose a DCA-SVM that not only conquer the bias but also achieve rivaling clinical diagnostic results by leverage the powerful feature extraction capabilities of derivative component analysis. Our work is not only significant in translational bioinformatics by identifying and solving an important problem, but also has a positive impact on machine learning for adding new results to kernel-based learning for omics data.</p><p>In our further studies, we plan to investigate the label skewness bias for the multi-class diagnostics, which can be more complicate and applied in medical informatics than the current binary type diagnostics [<xref ref-type="bibr" rid="CR47">47</xref>]. Moreover, we are interested in investigating diagnostic biases in deep learning methods for its importance in big omics data oriented diagnostics [<xref ref-type="bibr" rid="CR48">48</xref>, <xref ref-type="bibr" rid="CR49">49</xref>], in addition to integrating different types of omics data sets to conduct differential expression analysis [<xref ref-type="bibr" rid="CR50">50</xref>].</p></sec><sec id="Sec27"><title>Availability of supporting data</title><p>All data sets used in this paper are publicly available from <ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/tbdiagnosticbiases/">https://sites.google.com/site/tbdiagnosticbiases/</ext-link>.</p></sec></body><back><fn-group><fn><p><bold>Competing interests</bold></p><p>The author declares that he has no competing interests.</p></fn><fn><p><bold>Authors&#x02019; contributions</bold></p><p>Han does all the work for this study.</p></fn></fn-group><ack><title>Acknowledgements</title><p>This work was partially supported by the start-up funding package provided to Han by the Fordham University.</p></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>B</given-names></name><name><surname>Peng</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><article-title>Computational solutions for omics data</article-title><source>Nat Rev Genet</source><year>2013</year><volume>14</volume><issue>5</issue><fpage>333</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1038/nrg3433</pub-id><?supplied-pmid 23594911?><pub-id pub-id-type="pmid">23594911</pub-id></element-citation></ref><ref id="CR2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>XL</given-names></name><name><surname>Ng</surname><given-names>SK</given-names></name><name><surname>Ji</surname><given-names>Z</given-names></name></person-group><article-title>Multi-resolution-test for consistent phenotype discrimination and biomarker discovery in translational bioinformatics</article-title><source>J Bioinformatics Comput Biol</source><year>2013</year><volume>11</volume><issue>06</issue><fpage>1343010</fpage><pub-id pub-id-type="doi">10.1142/S0219720013430105</pub-id></element-citation></ref><ref id="CR3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nepomuceno-Chamorro</surname><given-names>I</given-names></name><name><surname>Azuaje</surname><given-names>F</given-names></name><name><surname>Devaux</surname><given-names>Y</given-names></name><name><surname>Nazarov</surname><given-names>PV</given-names></name><name><surname>Muller</surname><given-names>A</given-names></name><name><surname>Aguilar-Ruiz</surname><given-names>JS</given-names></name><etal/></person-group><article-title>Prognostic transcriptional association networks: a new supervised approach based on regression trees</article-title><source>Bioinformatics</source><year>2011</year><volume>27</volume><issue>2</issue><fpage>252</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btq645</pub-id><?supplied-pmid 21098433?><pub-id pub-id-type="pmid">21098433</pub-id></element-citation></ref><ref id="CR4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nepomuceno-Chamorro</surname><given-names>I</given-names></name><name><surname>Aguilar-Ruiz</surname><given-names>JS</given-names></name><name><surname>Riquelme</surname><given-names>JC</given-names></name></person-group><article-title>Inferring gene regression networks with model trees</article-title><source>BMC Bioinformatics</source><year>2010</year><volume>11</volume><fpage>517</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-11-517</pub-id><?supplied-pmid 20950452?><pub-id pub-id-type="pmid">20950452</pub-id></element-citation></ref><ref id="CR5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shah</surname><given-names>NH</given-names></name><name><surname>Tenenbaum</surname><given-names>JD</given-names></name></person-group><article-title>The coming age of data-driven medicine: translational bioinformatics&#x02019; next frontier</article-title><source>J Am Med Inform Assoc</source><year>2012</year><volume>19</volume><fpage>e2</fpage><lpage>e4</lpage><pub-id pub-id-type="doi">10.1136/amiajnl-2012-000969</pub-id><?supplied-pmid 22718035?><pub-id pub-id-type="pmid">22718035</pub-id></element-citation></ref><ref id="CR6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canuel</surname><given-names>V</given-names></name><name><surname>Rance</surname><given-names>B</given-names></name><name><surname>Avillach</surname><given-names>P</given-names></name><name><surname>Degoulet</surname><given-names>P</given-names></name><name><surname>Burgun</surname><given-names>A</given-names></name></person-group><article-title>Translational research platforms integrating clinical and omics data: a review of publicly available solutions</article-title><source>Brief Bioinform</source><year>2015</year><volume>16</volume><issue>2</issue><fpage>280</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1093/bib/bbu006</pub-id><?supplied-pmid 24608524?><pub-id pub-id-type="pmid">24608524</pub-id></element-citation></ref><ref id="CR7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Nayak</surname><given-names>TK</given-names></name><name><surname>Modarres</surname><given-names>R</given-names></name><name><surname>Lee</surname><given-names>NH</given-names></name><name><surname>McCaffrey</surname><given-names>TA</given-names></name></person-group><article-title>Concordant integrative gene set enrichment analysis of multiple large-scale two-sample expression data sets</article-title><source>BMC Genomics</source><year>2014</year><volume>15</volume><issue>Suppl 1</issue><fpage>S6</fpage><pub-id pub-id-type="doi">10.1186/1471-2164-15-S1-S6</pub-id><?supplied-pmid 24564564?><pub-id pub-id-type="pmid">24564564</pub-id></element-citation></ref><ref id="CR8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>R</given-names></name><name><surname>Mias</surname><given-names>GI</given-names></name><name><surname>Li-Pook-Than</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>L</given-names></name><name><surname>Lam</surname><given-names>HY</given-names></name><name><surname>Chen</surname><given-names>R</given-names></name><etal/></person-group><article-title>Personal omics profiling reveals dynamic molecular and medical phenotypes</article-title><source>Cell</source><year>2012</year><volume>148</volume><issue>6</issue><fpage>1293</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2012.02.009</pub-id><?supplied-pmid 22424236?><pub-id pub-id-type="pmid">22424236</pub-id></element-citation></ref><ref id="CR9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chien</surname><given-names>S</given-names></name><name><surname>Bashir</surname><given-names>R</given-names></name><name><surname>Nerem</surname><given-names>RM</given-names></name><name><surname>Pettigrew</surname><given-names>R</given-names></name></person-group><article-title>Engineering as a new frontier for translational medicine</article-title><source>Sci Transl Med</source><year>2015</year><volume>7</volume><issue>281</issue><fpage>281fs13</fpage><pub-id pub-id-type="doi">10.1126/scitranslmed.aaa4325</pub-id><?supplied-pmid 25834106?><pub-id pub-id-type="pmid">25834106</pub-id></element-citation></ref><ref id="CR10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>H</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name></person-group><article-title>Overcome support vector machine diagnosis overfitting</article-title><source>Cancer Inform</source><year>2014</year><volume>Sl</volume><fpage>1145</fpage><lpage>158</lpage></element-citation></ref><ref id="CR11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>Multi-resolution independent component analysis for high-performance tumor classification and biomarker discovery</article-title><source>BMC Bioinformatics</source><year>2011</year><volume>12</volume><issue>S1</issue><fpage>S7</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-12-S1-S7</pub-id></element-citation></ref><ref id="CR12"><label>12</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shawe-Taylor</surname><given-names>J</given-names></name><name><surname>Cristianini</surname><given-names>N</given-names></name></person-group><source>Support Vector Machines and other kernel-based learning methods</source><year>2000</year><publisher-loc>New York NY</publisher-loc><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="CR13"><label>13</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>J</given-names></name></person-group><source>The Elements of statistical learning</source><year>2008</year><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="CR14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blomquist</surname><given-names>TM</given-names></name><name><surname>Crawford</surname><given-names>EL</given-names></name><name><surname>Lovett</surname><given-names>JL</given-names></name><name><surname>Yeo</surname><given-names>J</given-names></name><name><surname>Stanoszek</surname><given-names>LM</given-names></name><collab>Levin A, et al.</collab></person-group><article-title>Targeted RNA-sequencing with competitive multiplex-PCR amplicon libraries</article-title><source>PLoS ONE</source><year>2013</year><volume>8</volume><issue>11</issue><fpage>e79120</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0079120</pub-id><?supplied-pmid 24236095?><pub-id pub-id-type="pmid">24236095</pub-id></element-citation></ref><ref id="CR15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagy</surname><given-names>ZB</given-names></name><name><surname>Kelemen</surname><given-names>JZ</given-names></name><name><surname>Feh&#x000e9;r</surname><given-names>LZ</given-names></name><name><surname>Zvara</surname><given-names>A</given-names></name><name><surname>Juh&#x000e1;sz</surname><given-names>K</given-names></name><name><surname>Pus&#x000e1;s</surname><given-names>LG</given-names></name></person-group><article-title>Real-time polymerase chain reaction-based exponential sample amplification for microarray gene expression profiling</article-title><source>Anal Biochem</source><year>2005</year><volume>337</volume><issue>1</issue><fpage>76</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.ab.2004.09.044</pub-id><?supplied-pmid 15649378?><pub-id pub-id-type="pmid">15649378</pub-id></element-citation></ref><ref id="CR16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>H</given-names></name></person-group><article-title>Derivative component analysis for mass spectral serum proteomic profiles</article-title><source>BMC Med Genomics</source><year>2014</year><volume>7</volume><fpage>S1</fpage><pub-id pub-id-type="doi">10.1186/1755-8794-7-S1-S5</pub-id></element-citation></ref><ref id="CR17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suykens</surname><given-names>JAK</given-names></name><name><surname>Vandewalle</surname><given-names>J</given-names></name></person-group><article-title>Least squares support vector machine classifiers</article-title><source>Neural Process Lett</source><year>1999</year><volume>9</volume><issue>3</issue><fpage>293</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1023/A:1018628609742</pub-id></element-citation></ref><ref id="CR18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van</surname><given-names>GT</given-names></name><name><surname>Suykens</surname><given-names>JAK</given-names></name><name><surname>Baesens</surname><given-names>B</given-names></name><name><surname>Viaene</surname><given-names>S</given-names></name><name><surname>Vanthienen</surname><given-names>J</given-names></name><name><surname>Dedene</surname><given-names>G</given-names></name><etal/></person-group><article-title>Benchmarking least squares support vector machine classifiers</article-title><source>Mach Learn</source><year>2004</year><volume>54</volume><issue>1</issue><fpage>5</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1023/B:MACH.0000008082.80494.e0</pub-id></element-citation></ref><ref id="CR19"><label>19</label><mixed-citation publication-type="other">Bioinformatics Toolbox. <ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com/products/bioinfo/">http://www.mathworks.com/products/bioinfo/</ext-link>.</mixed-citation></ref><ref id="CR20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ressom</surname><given-names>H</given-names></name><name><surname>Varghese</surname><given-names>R</given-names></name><name><surname>Drake</surname><given-names>S</given-names></name><name><surname>Hortin</surname><given-names>G</given-names></name><name><surname>Abdel-Hamid</surname><given-names>M</given-names></name><name><surname>Loffredo</surname><given-names>C</given-names></name><etal/></person-group><article-title>Peak selection from MALDI-TOF mass spectra using ant colony optimization</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><issue>5</issue><fpage>619</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btl678</pub-id><?supplied-pmid 17237065?><pub-id pub-id-type="pmid">17237065</pub-id></element-citation></ref><ref id="CR21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boersma</surname><given-names>BJ</given-names></name><name><surname>Reimers</surname><given-names>M</given-names></name><name><surname>Yi</surname><given-names>M</given-names></name><name><surname>Ludwig</surname><given-names>JA</given-names></name><name><surname>Luke</surname><given-names>BT</given-names></name><name><surname>Stephens</surname><given-names>RM</given-names></name><etal/></person-group><article-title>A stromal gene signature associated with inflammatory breast cancer</article-title><source>Int J Cancer</source><year>2008</year><volume>122</volume><issue>6</issue><fpage>1324</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1002/ijc.23237</pub-id><?supplied-pmid 17999412?><pub-id pub-id-type="pmid">17999412</pub-id></element-citation></ref><ref id="CR22"><label>22</label><mixed-citation publication-type="other">TCGA portal. <ext-link ext-link-type="uri" xlink:href="https://tcga-data.nci.nih.gov/tcga/">https://tcga-data.nci.nih.gov/tcga/</ext-link>.</mixed-citation></ref><ref id="CR23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Irizarry</surname><given-names>R</given-names></name><name><surname>Hobbs</surname><given-names>B</given-names></name><name><surname>Collin</surname><given-names>F</given-names></name><name><surname>Beazer-Barclay</surname><given-names>Y</given-names></name><name><surname>Antonellis</surname><given-names>K</given-names></name><collab>Scherf U, et al</collab></person-group><article-title>Exploration, normalization, and summaries of high density oligonucleotide array probe level data</article-title><source>Biostatistics</source><year>2003</year><volume>4</volume><fpage>249</fpage><pub-id pub-id-type="doi">10.1093/biostatistics/4.2.249</pub-id><?supplied-pmid 12925520?><pub-id pub-id-type="pmid">12925520</pub-id></element-citation></ref><ref id="CR24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dillies</surname><given-names>MA1</given-names></name><name><surname>Rau</surname><given-names>A</given-names></name><name><surname>Aubert</surname><given-names>J</given-names></name><name><surname>Hennequet-Antier</surname><given-names>C</given-names></name><name><surname>Jeanmougin</surname><given-names>M</given-names></name><name><surname>Servant</surname><given-names>N</given-names></name><etal/></person-group><article-title>A comprehensive evaluation of normalization methods for Illumina high-throughput RNA sequencing data analysis</article-title><source>Brief Bioinform</source><year>2013</year><volume>14</volume><issue>6</issue><fpage>671</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1093/bib/bbs046</pub-id><?supplied-pmid 22988256?><pub-id pub-id-type="pmid">22988256</pub-id></element-citation></ref><ref id="CR25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marioni</surname><given-names>JC</given-names></name><name><surname>Mason</surname><given-names>CE</given-names></name><name><surname>Mane</surname><given-names>SM</given-names></name><name><surname>Stephens</surname><given-names>M</given-names></name><name><surname>Gilad</surname><given-names>Y</given-names></name></person-group><article-title>RNA-seq an assessment of technical reproducibility and comparison with gene expression arrays</article-title><source>Genome Res</source><year>2008</year><volume>18</volume><issue>9</issue><fpage>1509</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1101/gr.079558.108</pub-id><?supplied-pmid 18550803?><pub-id pub-id-type="pmid">18550803</pub-id></element-citation></ref><ref id="CR26"><label>26</label><mixed-citation publication-type="other">The NCBI Gene Expression Omnibus (GEO). <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/geo/">http://www.ncbi.nlm.nih.gov/geo/</ext-link>.</mixed-citation></ref><ref id="CR27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haasdonk</surname><given-names>B</given-names></name></person-group><article-title>Feature space interpretation of svms with indefinite kernels</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2005</year><volume>27</volume><issue>4</issue><fpage>482</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2005.78</pub-id><?supplied-pmid 15794155?><pub-id pub-id-type="pmid">15794155</pub-id></element-citation></ref><ref id="CR28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rallapalli</surname><given-names>G</given-names></name><name><surname>Kemen</surname><given-names>EM</given-names></name><name><surname>Robert-Seilaniantz</surname><given-names>A</given-names></name><name><surname>Segonzac</surname><given-names>C</given-names></name><name><surname>Etherington</surname><given-names>G</given-names></name><name><surname>Sohn</surname><given-names>KH</given-names></name><etal/></person-group><article-title>EXPRSS: an Illumina based high-throughput expression-profiling method to reveal transcriptional dynamics</article-title><source>BMC Genomics</source><year>2014</year><volume>15</volume><fpage>341</fpage><pub-id pub-id-type="doi">10.1186/1471-2164-15-341</pub-id><?supplied-pmid 24884414?><pub-id pub-id-type="pmid">24884414</pub-id></element-citation></ref><ref id="CR29"><label>29</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seiffert</surname><given-names>C</given-names></name><name><surname>Khoshgoftaar</surname><given-names>TM</given-names></name><name><surname>Van Hulse</surname><given-names>J</given-names></name><name><surname>Napolitano</surname><given-names>A</given-names></name></person-group><article-title>RUSBoost: Improving clasification performance when training data is skewed</article-title><source>19th International Conference on Pattern Recognition (ICPR)</source><year>2008</year><publisher-loc>Tampa, FL</publisher-loc><publisher-name>IEEE</publisher-name></element-citation></ref><ref id="CR30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Wong</surname><given-names>AC</given-names></name><name><surname>Kamel</surname><given-names>M</given-names></name></person-group><article-title>Classification of imbalanced data, a review</article-title><source>Int J Patt Recogn Artif Intell</source><year>2009</year><volume>23</volume><fpage>687</fpage><pub-id pub-id-type="doi">10.1142/S0218001409007326</pub-id></element-citation></ref><ref id="CR31"><label>31</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jolliffe</surname><given-names>I</given-names></name></person-group><source>Principal component analysis</source><year>2002</year><publisher-loc>New York</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="CR32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>MS</given-names></name><name><surname>Zhang</surname><given-names>BT</given-names></name></person-group><article-title>Ensemble learning with active example selection for imbalanced biomedical data classification</article-title><source>IEEE/ACM Trans Comput Biol Bioinform</source><year>2011</year><volume>8</volume><issue>2</issue><fpage>316</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1109/TCBB.2010.96</pub-id><?supplied-pmid 20876935?><pub-id pub-id-type="pmid">20876935</pub-id></element-citation></ref><ref id="CR33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>X</given-names></name></person-group><article-title>Nonnegative principal component analysis for cancer molecular pattern discovery</article-title><source>IEEE/ACM Trans Comput Biol Bioinformatics</source><year>2010</year><volume>7</volume><issue>3</issue><fpage>537</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1109/TCBB.2009.36</pub-id></element-citation></ref><ref id="CR34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>X</given-names></name></person-group><article-title>Improving gene expression cancer molecular pattern discovery using nonnegative principal component analysis</article-title><source>Genome Informat</source><year>2008</year><volume>21</volume><fpage>200</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1142/9781848163324_0017</pub-id></element-citation></ref><ref id="CR35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Wu</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>CP</given-names></name><name><surname>Tatevossian</surname><given-names>RG</given-names></name><name><surname>Dalton</surname><given-names>JD</given-names></name><name><surname>Tang</surname><given-names>B</given-names></name><etal/></person-group><article-title>Whole-genome sequencing identifies genetic alterations in pediatric low-grade gliomas</article-title><source>Nat Genet</source><year>2013</year><volume>45</volume><issue>6</issue><fpage>602</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/ng.2611</pub-id><?supplied-pmid 23583981?><pub-id pub-id-type="pmid">23583981</pub-id></element-citation></ref><ref id="CR36"><label>36</label><mixed-citation publication-type="other">Tam S, Tsao MS, McPherson JD. Optimization of miRNA-seq data preprocessing. Brief Bioinform. 2015;:1&#x02013;14. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bib/bbv019">10.1093/bib/bbv019</ext-link>.</mixed-citation></ref><ref id="CR37"><label>37</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McLachlan</surname><given-names>G</given-names></name></person-group><source>Discriminant Analysis and Statistical Pattern Recognition</source><year>2005</year><publisher-loc>Hoboken, NJ USA</publisher-loc><publisher-name>Wiley Interscience</publisher-name></element-citation></ref><ref id="CR38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nazarov</surname><given-names>PV</given-names></name><name><surname>Apanasovich</surname><given-names>VV</given-names></name><name><surname>Lutkovski</surname><given-names>VM</given-names></name><name><surname>Yatskou</surname><given-names>MM</given-names></name><name><surname>Koehorst</surname><given-names>RBM</given-names></name><name><surname>Hemminga</surname><given-names>MA</given-names></name></person-group><article-title>Artificial neural network modification of simulation-based fitting: application to a protein-lipid system</article-title><source>J Chem Inf Comput Sci</source><year>2004</year><volume>44</volume><issue>2</issue><fpage>568</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1021/ci034149g</pub-id><?supplied-pmid 15032537?><pub-id pub-id-type="pmid">15032537</pub-id></element-citation></ref><ref id="CR39"><label>39</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>Ling</surname><given-names>CX</given-names></name></person-group><article-title>Comparing naive bayes, decision trees, and SVM with AUC and accuracy</article-title><source>Third IEEE International Conference on Data Mining</source><year>2003</year><publisher-loc>Melbourne, Florida</publisher-loc><publisher-name>IEEE</publisher-name></element-citation></ref><ref id="CR40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jing</surname><given-names>X</given-names></name></person-group><article-title>Robust adaptive learning of feedforward neural networks via LMI optimizations</article-title><source>IEEE Trans Neural Netw</source><year>2012</year><volume>31</volume><fpage>33</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2012.03.003</pub-id></element-citation></ref><ref id="CR41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>RJ</given-names></name><name><surname>Dimmic</surname><given-names>MW</given-names></name></person-group><article-title>A two-sample Bayesian t-test for microarray data</article-title><source>BMC Bioinformatics</source><year>2006</year><volume>10</volume><issue>7</issue><fpage>126</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-7-126</pub-id><pub-id pub-id-type="pmid">16529652</pub-id></element-citation></ref><ref id="CR42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCall</surname><given-names>MN</given-names></name><name><surname>Bolstad</surname><given-names>BM</given-names></name><name><surname>Irizarry</surname><given-names>RA</given-names></name></person-group><article-title>Frozen robust multiarray analysis</article-title><source>Biostatistics</source><year>2010</year><volume>11</volume><issue>2</issue><fpage>242</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1093/biostatistics/kxp059</pub-id><?supplied-pmid 20097884?><pub-id pub-id-type="pmid">20097884</pub-id></element-citation></ref><ref id="CR43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>X</given-names></name></person-group><article-title>Inferring species phylogenies: a microarray approach</article-title><source>Comput Intell Bioinformatics Lecture Notes Comput Sci</source><year>2006</year><volume>4115</volume><fpage>485</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1007/11816102_52</pub-id></element-citation></ref><ref id="CR44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>MD</given-names></name><name><surname>Oshlack</surname><given-names>A</given-names></name></person-group><article-title>A scaling normalization method for differential expression analysis of RNA-seq data</article-title><source>Genome Biol</source><year>2010</year><volume>11</volume><fpage>R25</fpage><pub-id pub-id-type="doi">10.1186/gb-2010-11-3-r25</pub-id><?supplied-pmid 20196867?><pub-id pub-id-type="pmid">20196867</pub-id></element-citation></ref><ref id="CR45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Gerstein</surname><given-names>M</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name></person-group><article-title>RNA-Seq: a revolutionary tool for transcriptomics</article-title><source>Nat Rev Genet</source><year>2009</year><volume>10</volume><fpage>57</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1038/nrg2484</pub-id><?supplied-pmid 19015660?><pub-id pub-id-type="pmid">19015660</pub-id></element-citation></ref><ref id="CR46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anders</surname><given-names>S</given-names></name><name><surname>Huber</surname><given-names>W</given-names></name></person-group><article-title>Differential expression analysis for sequence count data</article-title><source>Genome Biol</source><year>2010</year><volume>11</volume><fpage>R106</fpage><pub-id pub-id-type="doi">10.1186/gb-2010-11-10-r106</pub-id><?supplied-pmid 20979621?><pub-id pub-id-type="pmid">20979621</pub-id></element-citation></ref><ref id="CR47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tapia</surname><given-names>E</given-names></name><name><surname>Ornella</surname><given-names>L</given-names></name><name><surname>Bulacio</surname><given-names>P</given-names></name><name><surname>Angelone</surname><given-names>L</given-names></name></person-group><article-title>Multiclass classification of microarray data samples with a reduced number of genes</article-title><source>BMC Bioinformatics</source><year>2011</year><volume>12</volume><fpage>59</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-12-59</pub-id><?supplied-pmid 21342522?><pub-id pub-id-type="pmid">21342522</pub-id></element-citation></ref><ref id="CR48"><label>48</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fakoor</surname><given-names>R</given-names></name><name><surname>Ladhak</surname><given-names>F</given-names></name><name><surname>Nazi</surname><given-names>A</given-names></name><name><surname>Huber</surname><given-names>M</given-names></name></person-group><article-title>Using deep learning to enhance cancer diagnosis and classification</article-title><source>Proceedings of the ICML Workshop on the Role of Machine Learning in Transforming Healthcare</source><year>2013</year><publisher-loc>Atlanta, Georgia</publisher-loc><publisher-name>JMLR: W&#x00026;CP</publisher-name></element-citation></ref><ref id="CR49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quang</surname><given-names>D</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Xie</surname><given-names>X</given-names></name></person-group><article-title>DANN: a deep learning approach for annotating the pathogenicity of genetic variants</article-title><source>Bioinformatics</source><year>2015</year><volume>31</volume><issue>5</issue><fpage>761</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btu703</pub-id><?supplied-pmid 25338716?><pub-id pub-id-type="pmid">25338716</pub-id></element-citation></ref><ref id="CR50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lai</surname><given-names>Y</given-names></name><name><surname>Eckenrode</surname><given-names>SE</given-names></name><name><surname>She</surname><given-names>JX</given-names></name></person-group><article-title>A statistical framework for integrating two microarray data sets in differential expression analysis</article-title><source>BMC Bioinformatics</source><year>2009</year><volume>10</volume><issue>Suppl 1</issue><fpage>S23</fpage><pub-id pub-id-type="doi">10.1186/1471-2105-10-S1-S23</pub-id><?supplied-pmid 19208123?><pub-id pub-id-type="pmid">19208123</pub-id></element-citation></ref></ref-list></back></article>