<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd"> 
<article article-type="research-article"><?DTDIdentifier.IdentifierValue -//NPG//DTD XML Article//EN?><?DTDIdentifier.IdentifierType public?><?SourceDTD.DTDName NPG_XML_Article.dtd?><?SourceDTD.Version 2.7.10?><?ConverterInfo.XSLTName nature2nlmx2.xsl?><?ConverterInfo.Version 2?><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">4145288</article-id><article-id pub-id-type="pii">srep06207</article-id><article-id pub-id-type="doi">10.1038/srep06207</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Critical limitations of consensus clustering in class discovery</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>&#x00218;enbabao&#x0011f;lu</surname><given-names>Yasin</given-names></name><xref ref-type="corresp" rid="c1">a</xref><xref ref-type="aff" rid="a1">1</xref><xref ref-type="aff" rid="a4">4</xref></contrib><contrib contrib-type="author"><name><surname>Michailidis</surname><given-names>George</given-names></name><xref ref-type="aff" rid="a2">2</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Jun Z.</given-names></name><xref ref-type="corresp" rid="c2">b</xref><xref ref-type="aff" rid="a3">3</xref></contrib><aff id="a1"><label>1</label><institution>Department of Computational Medicine &#x00026; Bioinformatics, University of Michigan</institution>, Ann Arbor, MI, <country>USA</country></aff><aff id="a2"><label>2</label><institution>Department of Statistics and EECS, University of Michigan</institution>, Ann Arbor, MI, <country>USA</country></aff><aff id="a3"><label>3</label><institution>Department of Human Genetics, University of Michigan</institution>, Ann Arbor, MI, <country>USA</country></aff><aff id="a4"><label>4</label>Current address: Computational Biology Center, Memorial Sloan Kettering Cancer Center, New York, NY, USA</aff></contrib-group><author-notes><corresp id="c1"><label>a</label><email>yasin@cbio.mskcc.org</email></corresp><corresp id="c2"><label>b</label><email>junzli@med.umich.edu</email></corresp></author-notes><pub-date pub-type="epub"><day>27</day><month>08</month><year>2014</year></pub-date><pub-date pub-type="collection"><year>2014</year></pub-date><volume>4</volume><elocation-id>6207</elocation-id><history><date date-type="received"><day>19</day><month>05</month><year>2014</year></date><date date-type="accepted"><day>08</day><month>08</month><year>2014</year></date></history><permissions><copyright-statement>Copyright &#x000a9; 2014, Macmillan Publishers Limited. All rights reserved</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Macmillan Publishers Limited. All rights reserved</copyright-holder><license xmlns:xlink="http://www.w3.org/1999/xlink" license-type="open-access" xlink:href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><!--author-paid--><license-p>This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in the credit line; if the material is not included under the Creative Commons license, users will need to obtain permission from the license holder in order to reproduce the material. To view a copy of this license, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc-sa/4.0/">http://creativecommons.org/licenses/by-nc-sa/4.0/</ext-link></license-p></license></permissions><abstract><p>Consensus clustering (CC) has been adopted for unsupervised class discovery in many genomic studies. It calculates how frequently two samples are grouped together in repeated clustering runs, and uses the resulting pairwise "consensus rates" for visual demonstration that clusters exist, for comparing cluster stability, and for estimating the optimal cluster number (K). However, the sensitivity and specificity of CC have not been systemically assessed. Through simulations we find that CC is able to divide randomly generated unimodal data into apparently stable clusters for a range of K, essentially reporting chance partitions of cluster-less data. For data with known structure, the common implementations of CC perform poorly in identifying the true K. These results suggest that CC should be applied and interpreted with caution. We found that a new metric based on CC, the proportion of ambiguously clustered pairs (PAC), infers K equally or more reliably than similar methods in simulated data with known K. Our overall approach involves the use of realistic null distributions based on the observed gene-gene correlation structure in a given study, and the implementation of PAC to more accurately estimate K. We discuss the strength of our approach in the context of other ensemble-based methods.</p></abstract></article-meta></front><body><p>Cluster analysis is a basic tool for subtype discovery and sample classification using high-dimensional data. In a dataset of n samples and p features, when the class/subtype labels are known for the samples, the typical task is to define an optimized predictor in this training set, and apply it in class prediction for new samples with unknown labels. Here the performance is assessed by "external" validation measures, usually the agreement between the prediction and the known labels. In contrast, when class labels are not known, the task is to perform <italic>ab initio</italic> class discovery. Since 1996, cluster analysis of microarray-derived gene expression profiles has led to the discovery of molecular subtypes of many cancers<xref ref-type="bibr" rid="b1">1</xref><xref ref-type="bibr" rid="b2">2</xref><xref ref-type="bibr" rid="b3">3</xref><xref ref-type="bibr" rid="b4">4</xref><xref ref-type="bibr" rid="b5">5</xref><xref ref-type="bibr" rid="b6">6</xref>. However, it has always been difficult to compare clustering results between methods or between studies. Thus, a clustering-based study often leaves behind questions such as: what is the chance of reporting clusters when none truly exists? Is it possible for a method to overstate clustering strength? What is the confidence of the inferred optimal number of clusters (denoted K from now on)? And how can one validate the optimal K in an unbiased fashion? Inattention to these questions in the initial, subtype-discovery phase can hinder the downstream, integrative analyses. For example, the number of subtypes for a certain cancer could differ between two studies simply because neither study had strong evidence to formally support K over (K&#x02212;1) or (K+1). Similarly, within the same cancer cohort, the reported optimal K may vary among DNA, mRNA, and methylation data if different methods were applied to different data types, and if these methods have different sensitivity/specificity in detecting clusters. The end result of such confusion is that we don't know if the discrepancy between studies or between data types within a study could reflect a real biological distinction, or could be explained by methodological differences or the mere absence of a strong cluster signal.</p><p>Despite its critical importance, the task of evaluating cluster strength is difficult to be formulated in a hypothesis-testing framework<xref ref-type="bibr" rid="b7">7</xref>. This is because each real dataset could have its own unique covariance structure, making it challenging to calculate false-positive and false-negative rates of cluster results. In gene expression analyses, for example, shared regulatory pathways or mixture of multiple cell types inevitably produce strong gene-gene correlations. Thus a multivariate Gaussian distribution without gene-gene correlation does not represent a valid null distribution in cluster analysis. This difficulty has motivated the development of non-parametric, resampling-based methods, where multiple subsamples of the original dataset are clustered, and the results are compared against null datasets to assess cluster strength in terms of cluster stability.</p><p>Many cluster ensembles methods have emerged (reviewed in Ref. <xref ref-type="bibr" rid="b8">8</xref>, also see below "<bold>Comparison with other ensemble-based methods</bold>"). One such method, consensus clustering (CC)<xref ref-type="bibr" rid="b9">9</xref>, has recently gained widespread use in genomic studies (e.g.<xref ref-type="bibr" rid="b10">10</xref><xref ref-type="bibr" rid="b11">11</xref><xref ref-type="bibr" rid="b12">12</xref>,). CC calculates a &#x0201c;consensus rate&#x0201d; between all pairs of samples, defined as the frequency with which a given pair is grouped together in multiple clustering runs, each with a certain degree of permutation either by random initialization or by random sample- or gene-subsampling. The resulting sample-sample similarity matrix is routinely used both as a visualization tool for putative clusters and as an inference tool: the differences between within-group and between-group consensus rates are used to assess cluster stability and to infer the optimal K. The main assumption of CC is that if the samples under study were drawn from K distinct sub-populations that truly exist, different subsamples would show the greatest level of stability at the true K. This assumption is easily satisfied in cases of well-separated clusters. However, whether CC can also find apparently robust clusters in data with weak or no clusters has not been evaluated. Although this limitation is acknowledged in literature (for example<xref ref-type="bibr" rid="b9">9</xref>,), many studies using CC still rely on the consensus rate heatmap to visually demonstrate the existence of clusters, with few going further to reporting their robustness.</p><p>An early example that motivated this reassessment is the analysis of Glioblastoma Multiforme (GBM) by The Cancer Genome Atlas (TCGA) Research Network<xref ref-type="bibr" rid="b13">13</xref>, which reported four molecular subtypes according to gene expression clusters discovered by CC<xref ref-type="bibr" rid="b14">14</xref>. We found that, while the CC heatmaps show four crisp clusters (<xref ref-type="fig" rid="f1">Fig. 1a&#x02013;b</xref>, <xref ref-type="supplementary-material" rid="s1">Supplementary Note 1</xref>), the appearance of clusters in the Pearson's correlation coefficient matrix (<xref ref-type="fig" rid="f1">Fig. 1c</xref>) is much weaker, and principal component analysis (PCA) does not show discernible gaps among reported clusters (<xref ref-type="fig" rid="f1">Fig. 1d</xref>). Further, the number of clusters, K = 4, does not always appear better than alternative hypotheses such as K = 2 or 3 (<xref ref-type="supplementary-material" rid="s1">Supplementary Note 1</xref>, <xref ref-type="supplementary-material" rid="s1">Supplementary Fig. 1 and 3c</xref>). These observations led us to ask the following questions: (1) How can a researcher realize if he/she is merely partitioning data from a unimodal distribution into multiple groups? (2) How should the optimal K be determined? (3) How to verify the existence of clusters and how to validate K? In this study we address these questions by systematically assessing the sensitivity and specificity of CC using simulated datasets with known absence of clusters, or datasets with known number of clusters. We also discuss CC in the context of similar methods.</p><sec disp-level="1" sec-type="results"><title>Results</title><sec disp-level="2"><title>CC is capable of finding clusters in simulated datasets of <italic>unimodal</italic> distribution</title><p>We generated two simulated datasets with no clusters: (1) <italic>Square1</italic>, consisting of 100 samples, each with measurements in 1,000 genes, that form a regularly spaced square-shaped grid in the PC1-PC2 space, and (2) <italic>Circle1</italic>, with ~300 samples forming a similar but circle-shaped grid (see <bold>Methods</bold> for details of the simulation). We tested CC on <italic>Circle1</italic> for K = 2&#x02013;5, using k-means as the base method. In <xref ref-type="fig" rid="f2">Figure 2a&#x02013;d</xref>, the upper panels show the group partition in a single typical k-means run; and the lower panels show the CC matrix heatmaps for 250 runs with 80% sample-subsampling. While there is no inherent structure in <italic>Circle1</italic>, CC can nonetheless partition the samples into K = 2&#x02013;5 subgroups, which are spatially segregated. Importantly, CC is able to show a high level of apparent cluster stability, especially at K = 2&#x02013;4 (<xref ref-type="fig" rid="f2">Fig. 2a&#x02013;c</xref>). Moreover, the stability is further improved for larger K (such as 7 or 8) (<xref ref-type="supplementary-material" rid="s1">Supplementary Fig. 2</xref>), making it tempting to conclude that the original data contain 7 or 8 clusters. The apparent stability in <italic>Circle1</italic> is potentially caused by the presence of outliers or "corners" of the sample distribution that arise as a random byproduct of the simulation procedure. To investigate this, we performed CC on <italic>Square1</italic> for K = 2&#x02013;5 and found clear partitions and strong stability, especially for K = 4 (<xref ref-type="fig" rid="f2">Fig. 2e&#x02013;h</xref>). Clusters for K = 2&#x02013;3 were not as &#x02018;clean&#x02019;, suggesting that the four corners of the grid helped to anchor the K = 4 partitions and enhance their stability.</p><p>Together, these two examples illustrate how CC is capable of demonstrating apparent stability of chance partitioning of null datasets, suggesting that its exquisite sensitivity could lead to over-interpretation of cluster strength in a real study. Further, visual evidence from CC can be misleading, and this is particularly relevant in practice, as many published studies using CC relied on visualization of the CC matrix to support cluster claims. To systematically evaluate the sensitivity of CC, one needs to compare clustering results for a test dataset with those from an ensemble of negative datasets, which form a null distribution.</p></sec><sec disp-level="2"><title>CC shows stable clusters for null models harboring empirical gene-gene correlations</title><p>One option to generate the null distribution is to populate an ensemble of n-p matrices&#x02014;for n samples and p genes&#x02014;using random values from a univariate <bold>uniform</bold> or <bold>unimodal</bold> distribution<xref ref-type="bibr" rid="b15">15</xref>. However, the gene-gene correlation structure also needs to be considered when constructing null distributions as it is a key parameter in unsupervised class discovery. The influence of the gene covariance structure on sample discovery is caused by the interdependence between the gene-gene and sample-sample correlations. This can be understood in two ways: (1) If the samples fall into two clusters, the genes that differentiate the two clusters will be correlated, leading to a corresponding structure in gene-gene correlation. (2) Conversely, if a group of genes are co-regulated, they will limit the "shape" of sample projections in the p-dimensional space. For example, if gene-1 (g1) and gene-2 (g2) are strongly correlated, samples will tend to occupy an elongated ellipsoid in the g1-g2 dimension rather than a sphere, making it easier to identify sample clusters occupying opposite ends of the ellipsoid.</p><p>We created null cluster-less datasets with the same gene-gene correlation from a real dataset by (1) constructing an n-m score matrix representing the top m principal component scores for n samples by randomly sampling a univariate Gaussian distribution, and (2) multiplying this score matrix with the top m eigenvectors from TCGA's data for the first GBM cohort (GBM1<xref ref-type="bibr" rid="b14">14</xref>) (<bold>Methods</bold>). By repeating this procedure we generated 50 null datasets called the <bold>pcNormal</bold> datasets. When needing to run one-to-one comparisons with GBM1, we chose a representative dataset from pcNormal, <bold>Sim25,</bold> for which the silhouette width (<bold>Methods</bold>) is ranked 25th among the 50.</p><p>Although the pcNormal datasets have a known lack of substructure, CC shows stable clusters with K = 2, 3, 4. As an example, Sim25 (<xref ref-type="fig" rid="f3">Fig. 3</xref>) showed stable clusters in the K = 4 heatmap; and these are as crisp as those for the original GBM1 data (compare <xref ref-type="fig" rid="f3">Fig. 3b&#x02013;c</xref> with <xref ref-type="fig" rid="f1">Fig. 1a&#x02013;b</xref>). K = 2 and 3 also showed crisp clusters. Although this comparison does not establish that GBM1 has no valid clusters, it shows that simulated data with no known local density or outlier groups are fully capable of producing visually convincing clusters with the use of CC. In contrast with these observations with CC, other quantitative measures such as CLEST and average silhouette width did show that GBM1 had more structural features than the null datasets (<xref ref-type="supplementary-material" rid="s1">Supplementary Note 2</xref>). This underlines the fact that different clustering methods emphasize different features of a given heterogeneous dataset. Silhouette widths<xref ref-type="bibr" rid="b16">16</xref>, for example, are strongly influenced by the existence of one or more highly compact &#x0201c;local&#x0201d; clusters.</p></sec><sec disp-level="2"><title>Difficulties in finding the true K</title><p>To generate datasets of known K and known cluster separation, we obtained K clusters in Sim25 using a k-means run, then incrementally "pulled apart" the samples in each cluster, in PC space, from the global center of all samples (<bold>Methods</bold>). We generated such "positive datasets" for K = 2&#x02013;6, and with pull-apart degree "<italic>a</italic>" in the range [0, 0.8], where 1 represents pulling the sample PC scores away from the global mean by the original distance between the cluster mean and the global mean.</p><p>The original implementation of CC involves two measures for finding K: the cumulative distribution function (CDF) and the proportional change in the area under the CDF curve upon an increase of K (&#x00394;(K)) (<bold>Methods</bold>). In addition to CDF and &#x00394;(K), we also tested two other methods for finding K: GAP-PC<xref ref-type="bibr" rid="b17">17</xref> and CLEST<xref ref-type="bibr" rid="b15">15</xref>. The results for the four methods are shown in separate rows in <xref ref-type="fig" rid="f4">Figure 4</xref>. Within each row, from left to right are results from four positive datasets: no-pull-apart (<italic>a</italic> = 0), 2-way pull-apart at <italic>a</italic> = 0.08, 3-way pull-apart at <italic>a</italic> = 0.12, and 4-way pull-apart at <italic>a</italic> = 0.12, respectively. These <italic>a</italic> values were chosen as the smallest values in the range [0, 0.8] where the CDF plot exhibits a flat middle portion for the true K value (<xref ref-type="fig" rid="f4">Fig. 4a</xref>).</p><p>CDF is able to reveal the correct K, as the CDF curve is flat only for the true K (<xref ref-type="fig" rid="f4">Fig. 4a</xref>), reflecting a perfectly or near-perfectly stable partitioning of the samples at the correct K. As expected, the no-pull-apart dataset does not have such a flat curve because true K = 1. In contrast, &#x00394;(K) curves (<xref ref-type="fig" rid="f4">Fig. 4b</xref>) are alike in that they all exhibit an "elbow" at K = 4, i.e., K = 4 had a smaller improvement than K = 3; and that K = 4 would be called optimal even when the true structure has K = 1, 2, or 3.</p><p>The GAP method provides an estimate of K by comparing the change in within-cluster dispersion with that expected under a reference null distribution. According to the original decision rule of GAP<xref ref-type="bibr" rid="b17">17</xref> (<bold>Methods</bold>), all four datasets (<xref ref-type="fig" rid="f4">Fig. 4c</xref>) conclude an optimal K of 3, even when the true K is 1, 2, or 4. The CLEST method is based on the <italic>d<sub>k</sub></italic> statistic (see<xref ref-type="bibr" rid="b15">15</xref> and also <bold>Methods</bold>). In <xref ref-type="fig" rid="f4">Figure 4d</xref>, the optimal K for the first dataset is 1 because the minimum required difference was not achieved by any K (<italic>d<sub>k</sub></italic> &#x0003c; <italic>d<sub>min</sub></italic> = 0.05). For K = 2, 3, and 4, CLEST concludes an optimal K of 2, 3, and 5 respectively, as given by the K with the maximum <italic>d<sub>k</sub></italic>. In total, CLEST was able to make correct inferences in three out of four cases tested.</p><p>We also analyzed two real datasets that have well-separated clusters: a lymphoma dataset by Alizadeh et al.<xref ref-type="bibr" rid="b1">1</xref> and a dataset of twelve cancer types ("Pan-Cancer")<xref ref-type="bibr" rid="b18">18</xref>. The former has been used as a benchmark in multiple method comparison studies. It was originally reported to have an optimal K = 3 based on 4,026 genes<xref ref-type="bibr" rid="b1">1</xref>, and was corroborated by Smolkin &#x00026; Ghosh<xref ref-type="bibr" rid="b19">19</xref>. However, de Souto et al.<xref ref-type="bibr" rid="b20">20</xref> found that K could be either 3 or 4 with a subset of 2,093 genes (which we used in our test). The Multi-K method<xref ref-type="bibr" rid="b21">21</xref> found K = 3 using the 300 most variable genes. Bertoni &#x00026; Valentini<xref ref-type="bibr" rid="b22">22</xref> and Lange et al.<xref ref-type="bibr" rid="b23">23</xref> independently found K = 2 by using the 200 most variable genes. Lange et al. also reported that if K = 3 is forced, the 3 groups would not correspond to FL, CLL and DLBCL, but would split DLBCL into two groups. The correlation and CC heatmaps, shown in <xref ref-type="fig" rid="f5">Figure 5a&#x02013;d</xref>, suggest that K = 2 and K = 3 are both plausible. The CDF plot (<xref ref-type="fig" rid="f5">Fig. 5e</xref>) show a flat curve for K = 2, but an increase of the area under the curve for K = 3, resulting in a maximal &#x00394;(K) at K = 3 (<xref ref-type="fig" rid="f5">Fig. 5f</xref>). These observations show that even the real datasets with well separated clusters can have an uncertain true K, making it difficult to use them as benchmarks for comparing class discovery methods. A similar situation is seen with the Pan-Cancer dataset (<xref ref-type="supplementary-material" rid="s1">Supplementary Fig. 4a&#x02013;d</xref>): it contains 12 clinically defined cancer types, but K = 16 was found in a previous report<xref ref-type="bibr" rid="b18">18</xref>. Our analysis show that any K above 8 is plausible (<xref ref-type="supplementary-material" rid="s1">Supplementary Fig. 4e&#x02013;f</xref>).</p><p>For these reasons, simulated datasets where the data structure is controlled are more reliable for comparing methods that aim to find the true K. In our simulated positive datasets, when clusters are sufficiently separated, the CDF curves exhibit a flat middle segment only for the true K, and this can be used to infer the optimal K (see below). In contrast, &#x00394;(K) is uninformative even in the presence of genuine structure (<xref ref-type="fig" rid="f4">Fig. 4b</xref>). The original GAP decision criterion also performs poorly (<xref ref-type="fig" rid="f4">Fig. 4c</xref>). CLEST, on the other hand, may have similar sensitivity compared with CDF curves (<xref ref-type="fig" rid="f4">Fig. 4d</xref>). In a next section we will expand our comparison to a wider range of (K, <italic>a</italic>) combinations.</p></sec><sec disp-level="2"><title>Proportion of ambiguous clustering (PAC) and its performance</title><p>In the CDF curve of a consensus matrix, the lower left portion represents sample pairs rarely clustered together, the upper right portion represents those almost always clustered together, whereas the middle portion represents those with occasional co-assignments in different clustering runs. As shown in <xref ref-type="fig" rid="f4">Figure 4a</xref>, the CDF curves show a flat middle segment only for the true K, suggesting that very few sample pairs are ambiguous when K is correctly inferred. To capture this feature of the CDF curve we propose a new index: the "proportion of ambiguous clustering" (<bold>PAC</bold>), defined as the fraction of sample pairs with consensus index values falling in the intermediate sub-interval (x<sub>1</sub>, x<sub>2</sub>) &#x02208; [0, 1] (<bold>Methods</bold>). A low value of PAC indicates a flat middle segment, allowing inference of the optimal K by the lowest PAC.</p><p>We used the aforementioned positive datasets to compare PAC with six other methods: &#x00394;(K), CLEST, GAP-PC with the original decision rule, GAP-PC with a modified decision rule (explained in <bold>Methods</bold>), the silhouette width, and LCE<xref ref-type="bibr" rid="b24">24</xref>. While <xref ref-type="fig" rid="f4">Figure 4</xref> showed results for four specific combinations of K and <italic>a</italic>, here we sought to compare methods across a wider range of (K, <italic>a</italic>) values. The results are shown in a new plot, with five panels of stacked bar plots for each method (<xref ref-type="fig" rid="f6">Fig. 6</xref>). Those for LCE are shown in <xref ref-type="supplementary-material" rid="s1">Supplementary Figure 5</xref>. For each method, the five panels correspond to, from bottom to top, K = [2,...,6]. Within each panel, from left to right are segmented bar plots for increasing <italic>a</italic> in the range [0, 0.8]. Within each bar plot, the length of the vertical segments shows the fraction of inferred K across 50 simulated positive datasets for the given (K, <italic>a</italic>) combination (For LCE we only tested 10 datasets for each parameter combination, see below and <bold>Methods</bold>). The segments were color-coded to facilitate direct visualization of how well the inferred Ks agree with the true K, as shown on the far right. Such plots allow systematic performance comparisons for different methods under different (K, <italic>a</italic>). For a given (K, <italic>a</italic>), the same 50 datasets were used in testing the first six methods.</p><p>PAC detects the correct K across most of tested (K, <italic>a</italic>) pairs (<xref ref-type="fig" rid="f6">Fig. 6a</xref>). In comparison, &#x00394;(K) detects the correct K for K = 2 and 3, but calls K = 3 even when the true K = 4&#x02013;6 (<xref ref-type="fig" rid="f6">Fig. 6b</xref>), i.e., it consistently under-calls when K &#x0003e;3, consistent with <xref ref-type="fig" rid="f4">Figure 4b</xref>. For CLEST, the inferred K is correct for most datasets with true K = 2, 3, 6 and with <italic>a</italic> &#x0003e; 0.2 (<xref ref-type="fig" rid="f6">Fig. 6c</xref>). When the true K is 4 or 5, CLEST has a tendency to overcall. On the whole, the parameter space of correct calls in CLEST is smaller than in PAC, but comparable with modified GAP-PC and LCE.</p><p>The original GAP-PC method performs well for K = 2&#x02013;3, and improves with larger <italic>a</italic>, but it severely under-calls for K = 4&#x02013;6 (<xref ref-type="fig" rid="f6">Fig. 6d</xref>). In contrast, the modified GAP-PC performs well for K = 3&#x02013;6, although it over-calls when true K = 2 (<xref ref-type="fig" rid="f6">Fig. 6e</xref>). On the whole, the modified GAP-PC is improved over the original GAP-PC. The silhouette width severely under-calls in most situations (<xref ref-type="fig" rid="f6">Fig. 6f</xref>). Lastly, LCE showed variable performance according to the algorithm for creating the ensemble (fixed-k or variable-k), the method to partition the consensus matrix (average linkage, single linkage, or complete linkage), and the internal validation index (Davies-Bouldin and Dunn index). The best of these 12 combinations is FixedK_CTS-CL_DB (<xref ref-type="supplementary-material" rid="s1">Supplementary Fig. 5</xref>): it performed comparably with GAP-PC and CLEST, but worse than PAC (<xref ref-type="fig" rid="f7">Fig. 7</xref>). In sum, using simulated data we show that PAC outperforms several commonly used methods for estimating K.</p></sec><sec disp-level="2"><title>Gene-gene correlation among most discriminant genes makes it easy to &#x0201c;validate&#x0201d; any K</title><p>After an optimal K is determined for a dataset, the next task is to validate K. This can be difficult when there is no external information (e.g., known class labels) with which to calculate classification error rates. An alternative solution is to replicate the claim of K clusters in independent datasets. Ideally, the replication in the second dataset should not "borrow" any information from the first, discovery dataset. However, a method that has become highly popular involves (1) determining the most discriminant genes from the original dataset for its optimal K-way clustering, and (2) using these genes to classify samples in an independent dataset. In a typical implementation<xref ref-type="bibr" rid="b2">2</xref><xref ref-type="bibr" rid="b25">25</xref>, after the best classifier genes for each of K clusters are chosen from the learning set, a heatmap of all learning samples with only these genes is constructed, with the samples and the genes both grouped in K clusters. Next, another heatmap is made using the same genes for the replication samples. Observing the same number of discrete gene and sample clusters in the latter heatmap is considered a validation of K. We show below that, due to the persistent gene-gene correlation structure in genomic datasets, this approach can easily &#x0201c;validate&#x0201d; a K value even for data with no true clusters.</p><p>For this analysis, we start from Sim25, the representative dataset from the pcNormal simulations, using it as the &#x0201c;discovery&#x0201d; dataset from which the clusters and discriminating genes were to be learned. Following the procedure in<xref ref-type="bibr" rid="b14">14</xref>, we first run k-means on Sim25 with K = 4 and obtain four clusters for the 202 samples. We then find the 210 most discriminating genes for each cluster based on the t-scores for each cluster against the three other clusters. The four gene sets are combined to form a list of 551 unique genes, and are used in both Sim25 and a series of replication datasets chosen from pcNormal. The heatmap of Sim25 (<xref ref-type="fig" rid="f8">Fig. 8a</xref>) shows discrete placement of four gene sets and four sample classes. However, for nine null datasets from pcNormal, selected to represent the entire spectrum of silhouette width averages, similar clustering signatures are observed in all nine cases (<xref ref-type="fig" rid="f8">Fig. 8b</xref>). This can be explained by noting that the most discriminant genes contain many that are strongly correlated with each other. Such correlations could arise from co-regulation by common upstream regulators, or from inherent differences in different cell types, and can easily recur in an independent dataset even when the clustering pattern is different or absent in the latter. Results in <xref ref-type="fig" rid="f8">Figure 8</xref> show that the blocks of genes with co-expression in subsets of samples could persist even when the independent dataset is simulated from a unimodal distribution, thus apparently validating K.</p></sec><sec disp-level="2"><title>Comparison with other ensemble-based methods</title><p>CC is a method for class discovery, and must rely on "internal" validation measures such as the stability of the partition in an ensemble of diverse cluster solutions (other internal measures include compactness, connectedness, separation, etc.<xref ref-type="bibr" rid="b8">8</xref>). It belongs to the sub-class of methods known as stability-based <italic>cluster ensembles</italic> methods. One way to categorize these methods is by how the ensemble is generated. 1. Some methods perform gene-subsampling, essentially generating the ensemble by repeated clustering runs in feature sub-space<xref ref-type="bibr" rid="b9">9</xref><xref ref-type="bibr" rid="b19">19</xref><xref ref-type="bibr" rid="b26">26</xref>. 2. Others perform sample-subsampling, with the hypothesis that samples drawn from the same source should consistently exhibit the structure of the source population<xref ref-type="bibr" rid="b9">9</xref><xref ref-type="bibr" rid="b15">15</xref><xref ref-type="bibr" rid="b27">27</xref>. 3. Others apply a multitude of "base" clustering algorithms (e.g., k-means and hierarchical clustering)<xref ref-type="bibr" rid="b28">28</xref>, or, 4. incorporate a diverse set of parameter choices for each method, such as varying the initial cluster centers or the number of clusters (k) in k-means clustering<xref ref-type="bibr" rid="b21">21</xref><xref ref-type="bibr" rid="b26">26</xref><xref ref-type="bibr" rid="b29">29</xref>. 5. Some methods inject random noise in the original dataset to produce the ensemble of perturbed solutions<xref ref-type="bibr" rid="b30">30</xref>. Here we focus on methods for identifying sample clusters among n samples using p genes, where p &#x0226b; n. Those that aim to identify gene clustering (e.g., Figure-of-Merit<xref ref-type="bibr" rid="b31">31</xref>) have a different dimensionality problem and are not considered here. In typical implementations CC uses either gene- or sample-subsampling. Its base clustering algorithm is k-means in this study, yet was chosen as hierarchical clustering or self-organization maps in other comparisons<xref ref-type="bibr" rid="b24">24</xref><xref ref-type="bibr" rid="b26">26</xref><xref ref-type="bibr" rid="b27">27</xref>. We have found (not shown) that hierarchical clustering with average linkage is unreliable as a base method, because cutting the dendrogram at level K often assigns outlier samples into small or singleton clusters. This drawback of HC has been observed in other studies<xref ref-type="bibr" rid="b20">20</xref><xref ref-type="bibr" rid="b21">21</xref>.</p><p>While many cluster ensemble methods were developed after CC was proposed, and most of them performed well in their original evaluations, they emphasize different aspects of data structure and were tested in specific settings. As a result, no method is regarded universally as "the best". While we do not intend to provide a direct method comparison in this study, in the following we highlight some key distinctions of our approach. Many methods do not consider gene-gene correlations in generating the null datasets<xref ref-type="bibr" rid="b21">21</xref><xref ref-type="bibr" rid="b26">26</xref><xref ref-type="bibr" rid="b27">27</xref>, therefore the "data manifold" in the real data are not recapitulated in simulation. In this study we advocate the routine use of gene-gene correlation in simulations. Some methods evaluate performance based on the ability to identify complex geometric shapes in the sample distribution such as donuts, spirals, horseshoes, concentric rings<xref ref-type="bibr" rid="b21">21</xref><xref ref-type="bibr" rid="b29">29</xref>. We do not consider such complex shapes to be highly relevant for biomedical data. At least one method focused on evaluating cluster-specific robustness, not finding the optimal K<xref ref-type="bibr" rid="b30">30</xref>. Only a fraction of the methods, such as GAP<xref ref-type="bibr" rid="b17">17</xref>, MULTI-K<xref ref-type="bibr" rid="b21">21</xref>, and Model Explorer<xref ref-type="bibr" rid="b27">27</xref>, consider the global null scenario with K = 1, with the others only inferring K = 2 or above. Bertoni &#x00026; Valentini<xref ref-type="bibr" rid="b22">22</xref> expressed the importance of K = 1 but did not formally test it. Our implementation of PAC is similar to Model Explorer<xref ref-type="bibr" rid="b27">27</xref> and Bertoni &#x00026; Valentini<xref ref-type="bibr" rid="b22">22</xref> in using the cumulative distribution pattern of a stability measure across a range of K to find the optimal K. Our observation in <xref ref-type="fig" rid="f2">Figure 2</xref> that CC merely creates partitions of unimodal data has been noted by Ben-David et al.<xref ref-type="bibr" rid="b32">32</xref>, who pointed out that such partitions can be increasingly stable as sample size increases. Several innovations have appeared among the new methods. For example, Kim et al. applied entropy plots to de-emphasize new clusters formed by one or a small number of samples<xref ref-type="bibr" rid="b21">21</xref>. Bertoni &#x00026; Valentini<xref ref-type="bibr" rid="b22">22</xref> perform sample re-sampling in a "bounded" space around the original data. The method we tested, LCE<xref ref-type="bibr" rid="b24">24</xref>, incorporates a link-based similarity measure.</p><p>The results of LCE tested on positive datasets show that it underperformed PAC (<xref ref-type="fig" rid="f7">Figure 7</xref>, <xref ref-type="supplementary-material" rid="s1">Supplementary Fig. 5</xref>). Since it does not consider the null situation of no clusters (K = 1), it cannot be evaluated on our negative datasets and cannot inform whether the structure is absent. Its performance on real datasets varies (<xref ref-type="supplementary-material" rid="s1">Supplementary Table 1</xref>). Over the 12 parameter combinations it infers K = 2, 3, 4, 5, 8 for GBM1, K = 2, 3, 7 for Alizadeh et al., and K = 2 or 6 for Pan-Cancer. Using the best-performing combination, FixedK_CTS-CL_DB, it finds K = 4 for GBM1 and K = 2 for Alizadeh et al., both reasonable solutions. But it finds K = 2 for Pan-Cancer, severely under-calling the true K of 8&#x02013;16.</p></sec></sec><sec disp-level="1" sec-type="discussion"><title>Discussion</title><p>Our assessment using simulated <italic>Circle1</italic> and <italic>Square1</italic> has shown that CC is exquisitely sensitive: declaring structure where there is no significant separation or local compactness. This led us to systemically assess CC's sensitivity by comparing the real data with suitably formed null datasets. We also assessed the specificity of finding true K by comparing different methods across positive datasets of known K, with known degrees of separation. To limit the scope of our analysis we had to make some specific assumptions: (1) Samples in cluster boundaries are assigned to a single cluster; no partial memberships are used, (2) clusters are viewed as disjoint but co-equal, without being nested in each other, and (3) clusters are simulated with similar sizes, with no outliers added to represent very small groups (i.e., uneven divisions). These complicating factors need to be explored in future studies.</p><p>The choice of null distribution depends on the two distinct tasks of class discovery: first, to determine if there <italic>is</italic> evidence for clusters; second, when it is shown that clusters do exist, to determine the optimal number of clusters. For the first task, a <bold><italic>global null</italic></bold> should be constructed to test the "structure vs. no-structure" hypotheses, and needs to account for the gene-gene correlation in the original dataset as it affects the shape of the sample distribution in the high-dimensional space, potentially driving the baseline cluster stability. Here we refrain from using the terms &#x0201c;random&#x0201d; and &#x0201c;homogeneous&#x0201d; to describe this type of global null, because the gene-gene correlations can be considered as a form of innate data structure (i.e., non-random). For the second task, a set of <bold><italic>study-specific null</italic></bold> datasets for alternative K's should be used, because K cannot be reported as optimal unless the null hypotheses of K&#x02212;1 and K+1 are both rejected<xref ref-type="bibr" rid="b22">22</xref>.</p><p>In summary, while CC can be a powerful tool for identifying clusters, it needs to be applied with caution as it is highly sensitive and prone to over-interpretation. If clusters are not well separated, CC could lead one to conclude apparent structure when there is none, or declare cluster stability when it is weak. To reduce false positives in the exploratory phases of a new study, we recommend the following: 1) Do not rely solely on the consensus matrix heatmap to declare the existence of clusters, or to estimate optimal K. 2) Do a formal test of cluster strength using simulated unimodal data with the same gene-gene correlation as in the empirical data. 3) Apply the proportion of ambiguous clustering (PAC) as a simple yet powerful method to infer optimal K. 4) do not use the most discriminant genes for K clusters in the test dataset to validate K in a new dataset. Lastly, we strongly recommend that CC is applied in conjunction with other cluster ensembles methods.</p></sec><sec disp-level="1" sec-type="methods"><title>Methods</title><sec disp-level="2"><title>Datasets</title><p>This study used gene expression data from three cohorts of GBM samples. GBM1 is the cohort analyzed by the TCGA pilot study<xref ref-type="bibr" rid="b13">13</xref><xref ref-type="bibr" rid="b14">14</xref>. Gene expression data were downloaded in March 2010 from <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://tcga-data.nci.nih.gov/docs/publications/gbm_exp/">http://tcga-data.nci.nih.gov/docs/publications/gbm_exp/</ext-link>. Most of our analyses were based on "unifiedScaledFiltered.txt", which contains processed data for 1,740 most variable genes for 202 GBM1 samples. A second cohort was subsequently analyzed by TCGA and was called GBM2 here. Gene expression data for GBM2 were downloaded in September 2010 from the TCGA Data Matrix webpage (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://tcga-data.nci.nih.gov/tcga/dataAccessMatrix.htm">https://tcga-data.nci.nih.gov/tcga/dataAccessMatrix.htm</ext-link>). This dataset contains 175 samples, and we focused on the same 1,740 genes as in GBM1. The third cohort was the validation dataset used in<xref ref-type="bibr" rid="b14">14</xref> and is a collection of samples from four previous studies<xref ref-type="bibr" rid="b25">25</xref><xref ref-type="bibr" rid="b33">33</xref><xref ref-type="bibr" rid="b34">34</xref><xref ref-type="bibr" rid="b35">35</xref>. This dataset, called "validation" in this work, contains 260 samples, and the number of genes in common between GBM1 and <bold>validation</bold> is 1,676. This dataset was also downloaded in September 2010 from <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://tcga-data.nci.nih.gov/docs/publications/gbm_exp/">http://tcga-data.nci.nih.gov/docs/publications/gbm_exp/</ext-link>.</p><p>The Alizadeh et al. dataset was downloaded on June 23, 2014 from <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://bioinformatics.rutgers.edu/Static/Supplements/CompCancer/CDNA/alizadeh-2000-v3/">http://bioinformatics.rutgers.edu/Static/Supplements/CompCancer/CDNA/alizadeh-2000-v3/</ext-link>. It contains gene expression patterns of the three most prevalent adult lymphoid malignancies: Diffuse large B-cell lymphoma (DLBCL, n = 49), follicular lymphoma (FL, n = 9) and chronic lymphocytic leukemia (CLL, n = 11). Alizadeh et al. further identified two molecularly distinct groups of DLBCL, DLBCL1 and DLBCL2 (n = 21 and 21, respectively). We have adopted the gene filtering scheme in de Souto et al.<xref ref-type="bibr" rid="b20">20</xref> to obtain 2,093 genes. The parameters used in this filter are shown in <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://bioinformatics.rutgers.edu/Static/Supplements/CompCancer/CDNA/alizadeh-2000-v3/alizadeh_description.htm/">http://bioinformatics.rutgers.edu/Static/Supplements/CompCancer/CDNA/alizadeh-2000-v3/alizadeh_description.htm/</ext-link>.</p><p>The Pan-Cancer dataset (version number 2014-06-03) was downloaded on June 23, 2014 from the UCSC Cancer Genomics Browser <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://genome-cancer.ucsc.edu/proj/site/hgHeatmap/">https://genome-cancer.ucsc.edu/proj/site/hgHeatmap/</ext-link>. It contains RNAseq (Illumina HiSeq) gene expression profiles across 12 TCGA cohorts in the PANCAN12 study. This dataset for 3,468 samples was originally downloaded on June 23, 2014 from <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn1695373">https://www.synapse.org/#!Synapse:syn1695373</ext-link> and log transformed by using log2(x+1). Genes with both mean and variance greater than 2.5 were kept in order to select for the highly expressed and highly variable genes, resulting in 1,338 genes. The filtered data were centered and scaled across samples to have mean 0 and standard deviation 1. To reduce computational burden we removed every other sample in the filtered dataset to arrive at a reduced version with 1,734 samples, which have a similar representation of each tumor type as in the full version. The number of samples from each tumor type is: Acute Myeloid Leukemia 173 (89, the number in parenthesis is the reduced dataset), Bladder Cancer 96 (49), Breast Cancer 822 (396), Colon Cancer 192 (104), Endometrioid Cancer 333 (166), Glioblastoma Multiforme 167 (82), Head and Neck Cancer 303 (149), Kidney Clear Cell Carcinoma 470 (244), Lung Adenocarcinoma 355 (172), Lung Squamous Cell Carcinoma 220 (108), Ovarian Cancer 266 (139), Rectal Cancer 71 (36).</p></sec><sec disp-level="2"><title><italic>Square1 and Circle1</italic> simulations</title><p>We drew two 1000-element random vectors from Normal(0,1) that served as fixed PC1 and PC2 eigenvectors. Next, for <italic>Square1</italic>, we generated 100 pairs of [PC1, PC2] coefficients that would place 100 samples onto a 10-by-10 grid in the PC1-PC2 space. In this formation, samples had regularly increasing PC1 scores from left to right in the PC1-PC2 plot, and regularly increasing PC2 scores from bottom to top. The [PC1, PC2] scores were slightly "wiggled" from the grid points by adding random Normal(0,1) noise. The final 100 &#x000d7; 1000 data matrix is formed by linear combinations of the two fixed PC eigenvectors with the 100 [PC1, PC2] coefficient pairs. Similarly, for <italic>Circle1</italic>, we repeated the procedure above but changed the number of samples from 100 to 400, forming a 20-by-20 grid plus the same level of random wiggle. We then trimmed the square grid to keep only the samples with a distance to the center smaller than a radius of ~9.62 grid units, leaving ~300 samples that form a circle. Strictly speaking, both <italic>Square1</italic> and <italic>Circle1</italic> have higher gene-gene correlations than a matrix filled with Normal(0,1) data, because all 100 (or 300) objects are derived from the same PC1 and PC2 vectors. However, they are still cluster-less (or unimodal) in the sense that the sample placements lack local compactness or separation. Thus they can serve as the null dataset where no cluster is known to exist, and from which no robust cluster should be found.</p></sec><sec disp-level="2"><title>Generating null distributions based on empirical gene-gene correlations in GBM1</title><p>In settings naturally encountered in genomic studies, n &#x0226a; p, and gene-gene correlation information is often reliably represented by the top eigenvectors, i.e., the top principle component <italic>loadings</italic> that quantify the contribution of each of the p genes to the most salient data structure. By using PCA we decomposed the GBM1 data consisting of 202 samples and 1,740 genes into (1) the 202 &#x000d7; 202 principal component score matrix and (2) the 202 &#x000d7; 1740 eigenvector matrix. When simulating null datasets, in order to preserve the same relative magnitude of the PC scores for different PCs in GBM1, we constructed 202 &#x000d7; 202 random PC score matrices by populating each column with random draws from a univariate Gaussian distribution with mean = 0 and standard deviation equal to that of the corresponding column in the original GBM1's score matrix. Multiplying this random score matrix with the 202 &#x000d7; 1740 eigenvector matrix yields a null 202 &#x000d7; 1740 dataset, in which it is known that no cluster exists. We repeated this procedure 50 times to generate a null collection of <italic>pcNormal</italic> datasets.</p><p>The specific steps for this procedure are as follows:<list id="l1" list-type="order"><list-item><p>Using principal component analysis, we obtain the orthogonal matrix <italic>A</italic> of GBM1 eigenvectors. <inline-formula id="m1"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e736" xlink:href="srep06207-m1.jpg"/></inline-formula><italic>Y</italic> is the PC score matrix for GBM1. A is the PC vector matrix. </p></list-item><list-item><p>Next, we simulate a random score matrix <italic>Y<sup>N</sup></italic> where column <italic>i</italic> is populated with random values in a normal distribution with zero mean and standard deviation equal to that of column <italic>i</italic> in Y. <inline-formula id="m2"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e758" xlink:href="srep06207-m2.jpg"/></inline-formula>where <italic>s<sub>i</sub></italic> is the standard deviation of <italic>Y<sub>.i</sub></italic> and <italic>i</italic> = {1, &#x02026;, 202}. </p></list-item><list-item><p>Multiplying <italic>Y<sup>N</sup></italic> with the transpose of <italic>A</italic> yields <italic>Q<sup>N</sup></italic>, one of the <italic>pcNormal</italic> simulations. <inline-formula id="m3"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e798" xlink:href="srep06207-m3.jpg"/></inline-formula></p></list-item><list-item><p>We repeat steps 2 and 3 50 times to obtain a collection of 50 <italic>pcNormal</italic> simulations. <inline-formula id="m4"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e810" xlink:href="srep06207-m4.jpg"/></inline-formula>
</p></list-item></list></p></sec><sec disp-level="2"><title>Choosing a representative null dataset from pcNormal</title><p>A representative dataset, called <bold>Sim25,</bold> is chosen from <italic>pcNormal</italic> as having clustering signals closest to the median of the 50 datasets, as measured by the average of positive silhouette widths and the fraction of negative silhouette widths. Let <inline-formula id="m5"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e828" xlink:href="srep06207-m5.jpg"/></inline-formula>where [<italic>fN<sub>i</sub></italic>, <italic>aP<sub>i</sub></italic>] is the silhouette width statistics for simulation <italic>i</italic> &#x02208; {1, &#x02026;, 50} and <italic>d</italic> is the Euclidean distance function in the [<italic>fN</italic>, <italic>aP</italic>] space.</p></sec><sec disp-level="2"><title>Choosing nine pcNormal simulations for validation by most discriminant genes</title><p>The Euclidean distance of the (aP,fN) pair to the median of these quantities in the <bold>pcNormal</bold> cohort was computed for each of the 50 simulations and ranked from lowest to highest. Every 5th dataset was selected among the ranked simulations, such that the [6, 11, 16, 21, 26, 31, 36, 41, 46] ranked datasets were chosen. This ensures that nine datasets cover the entire range of clustering strength in <italic>pcNormal</italic>.</p></sec><sec disp-level="2"><title>Generating positive datasets for comparing the ability to find true K</title><p>To generate a positive dataset with K clusters, we first ran k-means clustering on Sim25 with the designated K in the range of 2&#x02013;6. Next, we computed the centroids of the PC scores for each of the K clusters, and added a known fraction of the centroid coordinates (i.e. the pull-apart degree, denoted as a positive scalar, "a") to the original PC scores of each sample in the corresponding cluster. Next, we multiplied the resulting PC scores from all clusters by the original principal component vectors of Sim25 so that the pull-apart datasets preserve the initial gene-gene correlation structure (with the caveat that increasing a values would gradually increase the gene-gene correlation).</p><p>Algorithmically, we execute the following steps for this procedure:<list id="l4" list-type="order"><list-item><p>Use principal component analysis to obtain the eigenvector matrix <italic>A</italic> as before. <inline-formula id="m6"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e882" xlink:href="srep06207-m6.jpg"/></inline-formula>
</p></list-item><list-item><p>Use k-means to find K clusters in Sim25, assign each sample <italic>s<sub>i</sub></italic> (<italic>i</italic> = 1, &#x02026;, 202) into one of K classes. The set of samples in class <italic>k</italic>(<italic>k</italic> = 1, &#x02026;, <italic>K</italic>) is denoted as <italic>E<sub>k</sub></italic>
</p></list-item><list-item><p>For each set <italic>E<sub>k</sub></italic>, compute the centroid <italic>C<sub>k</sub></italic> of PC scores <inline-formula id="m18"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e924" xlink:href="srep06207-m18.jpg"/></inline-formula>
</p></list-item><list-item><p>For each set <italic>E<sub>k</sub></italic> and for a given pull-apart degree <italic>a</italic>, compute pulled-apart score matrix <inline-formula id="m19"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e937" xlink:href="srep06207-m19.jpg"/></inline-formula>
<inline-formula id="m7"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e942" xlink:href="srep06207-m7.jpg"/></inline-formula>
</p></list-item><list-item><p>Multiply <italic>Y<sup>p</sup></italic> with <italic>A<sup>T</sup></italic> to obtain the pulled-apart dataset <italic>X<sup>p</sup></italic>. <inline-formula id="m8"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e967" xlink:href="srep06207-m8.jpg"/></inline-formula>
</p></list-item></list></p></sec><sec disp-level="2"><title>Base method for consensus clustering: K-means</title><p>Given a set of observations (<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, &#x02026;, <italic>x<sub>n</sub></italic>) where each observation is a d-dimensional real vector, k-means clustering aims to partition the <italic>n</italic> observations into <italic>k</italic> sets (<italic>k</italic> &#x02264; <italic>n</italic>), <bold><italic>S</italic></bold> = {<italic>S</italic><sub>1</sub>, <italic>S</italic><sub>2</sub>, &#x02026;, <italic>S<sub>k</sub></italic>} so as to minimize the within-cluster dispersion: <inline-formula id="m9"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1028" xlink:href="srep06207-m9.jpg"/></inline-formula>where <italic>&#x003bc;<sub>i</sub></italic> is the mean of points in <italic>S<sub>i</sub></italic><xref ref-type="bibr" rid="b36">36</xref>.</p><p>The method starts with k arbitrary cluster centers. Each search step consists of assigning each observation to its nearest cluster center, and updating the centers of the clusters according to the observations assigned to each cluster. The procedure is repeated until the cluster assignment no longer changes.</p></sec><sec disp-level="2"><title>Five ways to measure clustering signals and determine K</title><sec disp-level="3"><title>Empirical CDF</title><p>For a given consensus matrix <italic>M</italic>, the corresponding empirical cumulative distribution (CDF) is defined over the range [0, 1] as follows: <inline-formula id="m10"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1060" xlink:href="srep06207-m10.jpg"/></inline-formula>where 1{&#x02026;}denotes the indicator function, <italic>M(i, j)</italic> denotes entry (i, j) of the consensus matrix <italic>M</italic>, <italic>N</italic> is the number of rows (and columns) of <italic>M</italic>, and <italic>c</italic> is the consensus index value<xref ref-type="bibr" rid="b9">9</xref>.</p></sec><sec disp-level="3"><title>Proportional area change under CDF (<italic>&#x00394;</italic>(K))</title><p>The changes of CDF as K increases provide evidence for finding the optimal number of clusters. A CDF curve that closely describes a three-phase step function is indicative of a higher cluster stability. A method for using this information is to select the largest K that induces a large enough increase in the area under the CDF<xref ref-type="bibr" rid="b9">9</xref>, which is defined as: <inline-formula id="m11"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1095" xlink:href="srep06207-m11.jpg"/></inline-formula>The progression, in turn, can be visualized by plotting the proportion increase &#x00394;(<italic>K</italic>) in the CDF area as K increases. &#x00394;(<italic>K</italic>) is computed as follows: <inline-formula id="m12"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1109" xlink:href="srep06207-m12.jpg"/></inline-formula>The optimal K according to &#x00394;(<italic>K</italic>) is the K where the &#x02018;elbow&#x02019; occurs in the &#x00394;(<italic>K</italic>) vs. K plot. This is a subjective criterion, and in cases where the elbow occurs at a &#x00394;(<italic>K</italic>) value very close to zero, the optimal K can also be considered to be the K before the elbow occurs or the K where &#x00394;(<italic>K</italic>) reaches its maximum. In <xref ref-type="fig" rid="f6">Figure 6</xref>, we adopted this last decision rule due to the elbow rule not being amenable to automatization.</p></sec></sec><sec disp-level="2"><title>Silhouette width</title><p>The silhouette widths of a clustering result<xref ref-type="bibr" rid="b16">16</xref> have been applied to report clustering strength and to find the optimal number of clusters K. For an object i in the dataset, let A denote the cluster to which it is assigned, and define</p><p><italic>a(i)</italic> = average dissimilarity of i to all other objects of A</p><p>For each of the clusters <italic>C</italic> &#x02260; <italic>A</italic>, calculate</p><p><italic>d(i,C)</italic> = average dissimilarity of i to all objects of C</p><p>Then select the smallest of d. <inline-formula id="m13"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1157" xlink:href="srep06207-m13.jpg"/></inline-formula>The silhouette width of object i is defined as: <inline-formula id="m14"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1164" xlink:href="srep06207-m14.jpg"/></inline-formula>It can be seen that S(i) lies between &#x02212;1 and +1.</p><p>In <xref ref-type="supplementary-material" rid="s1">Supplementary Figure 3b</xref> we compare GBM1 with the null simulations according to two summary statistics derived from silhouette widths. One is the "<italic>fraction of samples with negative silhouette widths</italic>". A negative silhouette width indicates that the sample is likely to have been assigned to the wrong cluster. The second statistic is the "<italic>average of positive silhouette widths</italic>". Higher values of this statistic indicate stronger cluster separation.</p></sec><sec disp-level="2"><title>GAP-statistic</title><p>The GAP-statistic provides an estimate for the number of clusters in a dataset by comparing the within-cluster dispersion <italic>W<sub>k</sub></italic> with that expected under an appropriate reference null distribution (<inline-formula id="m20"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1192" xlink:href="srep06207-m20.jpg"/></inline-formula>where <italic>b</italic> &#x02208; {1,2, &#x02026;, <italic>B</italic>})<xref ref-type="bibr" rid="b17">17</xref>. We first computed <italic>W<sub>k</sub></italic> for each <italic>K</italic> &#x02265; 2. We have not included K = 1 to ensure comparability across all methods tested here; methods such as CDF and silhouette width do not allow an inference of K = 1.</p><p>For the reference distribution, there are two alternative algorithms: <italic>GAP-unif</italic> and <italic>GAP-PC</italic>. For the former, the null datasets are generated from a uniform distribution over the range of each observed feature; and for the latter, they are generated from a uniform distribution over a box aligned with the principal components of the centered design matrix. The first approach has the advantage of simplicity, but the second approach can factor in the shape of the data distribution<xref ref-type="bibr" rid="b17">17</xref>. We chose the latter as it can take into account the empirical gene-gene correlation.</p><p>We generated <italic>B</italic> = 40 reference datasets using <italic>GAP-PC</italic>. Next, we computed the within-cluster sum of squares <inline-formula id="m21"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1230" xlink:href="srep06207-m21.jpg"/></inline-formula> for each reference dataset and estimated the <italic>gap<sub>k</sub></italic> statistic with the formula: <disp-formula id="m15"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1238" xlink:href="srep06207-m15.jpg"/></disp-formula>The standard error for this quantity, <italic>s<sub>k</sub></italic>, was then computed as <inline-formula id="m22"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1247" xlink:href="srep06207-m22.jpg"/></inline-formula> where <italic>sd<sub>k</sub></italic> is the uncorrected sample standard deviation of the <inline-formula id="m23"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1255" xlink:href="srep06207-m23.jpg"/></inline-formula> quantities with <italic>b</italic> &#x02208; {1,2, &#x02026;, <italic>B</italic>}.</p><p>The optimal K is the smallest K for which the GAP score is larger than the lower bound for K+1; where the lower bound is defined as the GAP score minus the standard error for that particular K value:</p><p><italic>optimal K</italic> = smallest K such that <italic>gap<sub>k</sub></italic> &#x02265; <italic>gap<sub>k</sub></italic><sub>+1</sub>&#x02212;<italic>s<sub>k</sub></italic><sub>+1</sub><xref ref-type="bibr" rid="b17">17</xref></p></sec><sec disp-level="2"><title>Modified GAP-PC:</title><p>The original GAP-PC decision rule for the optimal K is to choose the smallest K where the <italic>gap<sub>k</sub></italic> score is larger than the lower bound for K+1. Our modified, more intuitive decision rule is to declare the K value with the highest <italic>gap<sub>k</sub></italic> score as the optimal K.</p></sec><sec disp-level="2"><title>CLEST</title><p>CLEST<xref ref-type="bibr" rid="b15">15</xref> is a resampling-based method that randomly partitions the original dataset into a learning set and a test set. The former is used to build a K-cluster classifier, which is applied to partition the latter (the test set) in supervised assignment (such as DLDA<xref ref-type="bibr" rid="b37">37</xref>). The test set is also partitioned independently with the same unsupervised clustering algorithm as applied to the learning set. The concordance between the supervised and unsupervised partitions is summarized by measures such as the Fowlkes-Mallows (FM) index, for which a higher value indicates a stronger agreement of clustering results.</p><p>The observed concordance for each K is denoted as <italic>t<sub>k</sub></italic>. Its estimated expected value under the null hypothesis of K = 1, namely <inline-formula id="m24"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1321" xlink:href="srep06207-m24.jpg"/></inline-formula>, is then subtracted from <italic>t<sub>k</sub></italic> to obtain the <italic>d<sub>k</sub></italic> statistic. Among the K values that satisfy a pre-specified <italic>d<sub>min</sub></italic> criterion (here <italic>d<sub>min</sub></italic> = 0.05), the optimal K is the one with maximum <italic>d<sub>k</sub></italic>. If none of the tested K values satisfy the pre-specified criteria, the optimal K is concluded to be 1.</p></sec><sec disp-level="2"><title>A new way to infer optimal K using CC: Proportion of Ambiguous Clustering (PAC)</title><p>The empirical CDF plot has consensus index values on the x-axis and CDF values on the y-axis. PAC is defined as the fraction of sample pairs with consensus index values falling in the intermediate sub-interval (x<sub>1</sub>, x<sub>2</sub>) &#x02208; [0, 1]. x<sub>1</sub> and x<sub>2</sub> are data-dependent thresholds, but will generally be chosen near 0 and 1 respectively. In our implementation, x<sub>1</sub> = 0.1 and x<sub>2</sub> = 0.9. Since CDF(c) corresponds to the fraction of sample pairs with consensus index values less than or equal to c as explained in the &#x0201c;Empirical CDF&#x0201d; section above, PAC is given by CDF(x<sub>2</sub>) - CDF(x<sub>1</sub>). A low value of PAC indicates a flat middle segment, allowing inference of the optimal K by the lowest PAC. <disp-formula id="m16"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1380" xlink:href="srep06207-m16.jpg"/></disp-formula><disp-formula id="m17"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1382" xlink:href="srep06207-m17.jpg"/></disp-formula></p></sec><sec disp-level="2"><title>LCE Pseudo-code</title><p>LCE was implemented using functions from the LinkCluE package<xref ref-type="bibr" rid="b38">38</xref>.</p><p>We set N = 202 (Number of samples) and <inline-formula id="m25"><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" id="d33e1393" xlink:href="srep06207-m25.jpg"/></inline-formula>.</p><p>Run 1: Fixed K. Execute step 1&#x02013;5.</p><p>&#x02003;&#x02003;&#x02003;&#x02003;Step 1: Generate cluster ensemble from k-means runs with 10 different random seeds and fixed K where K = K_max.</p><p>&#x02003;&#x02003;&#x02003;&#x02003;Step 2: For ensemble from Step 1, create link-based similarity matrix using CTS (referred to as WCT in Iam-on et al.<xref ref-type="bibr" rid="b24">24</xref>) and decay factor 0.8.</p><p>&#x02003;&#x02003;&#x02003;&#x02003;Step 3: Partition the similarity matrix from Step 2 with a consensus function to assign samples into K_final clusters.</p><p>&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02010; Consensus function alternatives are average-, single-, and complete-linkage HC</p><p>&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02010; Vary K_final in 2:6</p><p>&#x02003;&#x02003;&#x02003;&#x02003;Step 4: Evaluate quality of clusters using internal validity measures Davies-Bouldin (DB) and Dunn index. The DB and Dunn indices were calculated by comparing the partition from Step-3 and the k-means partition of the input data.</p><p>&#x02003;&#x02003;&#x02003;&#x02003;Step 5: Optimal K is the K_final with the best internal validity measure (highest Dunn index or lowest Davies-Bouldin index)</p><p>Run 2: Random K</p><p>&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02003;&#x02010; In Step 1, use random K in 2:K_max instead of fixed K. Then, execute step 2&#x02013;5 as above.</p></sec></sec><sec disp-level="1"><title>Author Contributions</title><p>Y.S. and J.Z.L. conceived the study; Y.S. performed the simulation and data analysis; J.Z.L. and G.M. provided guidance to the project; Y.S. and J.Z.L. wrote the manuscript; all authors reviewed the manuscript.</p></sec><sec sec-type="supplementary-material" id="s1"><title>Supplementary Material</title><supplementary-material id="d33e54" content-type="local-data"><caption><title>Supplementary Information</title><p>Supplementary materials</p></caption><media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-s1.pdf"/></supplementary-material></sec></body><back><ref-list><ref id="b1"><mixed-citation publication-type="journal"><name><surname>Alizadeh</surname><given-names>A. A.</given-names></name>
<italic>et al.</italic>
<article-title>Distinct types of diffuse large B-cell lymphoma identified by gene expression profiling</article-title>. <source>Nature</source>
<volume>403</volume>, <fpage>503</fpage>&#x02013;<lpage>511</lpage> (<year>2000</year>).<pub-id pub-id-type="pmid">10676951</pub-id></mixed-citation></ref><ref id="b2"><mixed-citation publication-type="journal"><name><surname>Bertucci</surname><given-names>F.</given-names></name>
<italic>et al.</italic>
<article-title>Gene expression profiling identifies molecular subtypes of inflammatory breast cancer</article-title>. <source>Cancer Res</source>
<volume>65</volume>, <fpage>2170</fpage>&#x02013;<lpage>2178</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15781628</pub-id></mixed-citation></ref><ref id="b3"><mixed-citation publication-type="journal"><name><surname>Hayes</surname><given-names>D. N.</given-names></name>
<italic>et al.</italic>
<article-title>Gene expression profiling reveals reproducible human lung adenocarcinoma subtypes in multiple independent patient cohorts</article-title>. <source>J Clin Oncol</source>
<volume>24</volume>, <fpage>5079</fpage>&#x02013;<lpage>5090</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">17075127</pub-id></mixed-citation></ref><ref id="b4"><mixed-citation publication-type="journal"><name><surname>Lapointe</surname><given-names>J.</given-names></name>
<italic>et al.</italic>
<article-title>Gene expression profiling identifies clinically relevant subtypes of prostate cancer</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>101</volume>, <fpage>811</fpage>&#x02013;<lpage>816</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">14711987</pub-id></mixed-citation></ref><ref id="b5"><mixed-citation publication-type="journal"><name><surname>Monti</surname><given-names>S.</given-names></name>
<italic>et al.</italic>
<article-title>Molecular profiling of diffuse large B-cell lymphoma identifies robust subtypes including one characterized by host inflammatory response</article-title>. <source>Blood</source>
<volume>105</volume>, <fpage>1851</fpage>&#x02013;<lpage>1861</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15550490</pub-id></mixed-citation></ref><ref id="b6"><mixed-citation publication-type="journal"><name><surname>Wilkerson</surname><given-names>M. D.</given-names></name>
<italic>et al.</italic>
<article-title>Lung squamous cell carcinoma mRNA expression subtypes are reproducible, clinically important, and correspond to normal cell types</article-title>. <source>Clin Cancer Res</source>
<volume>16</volume>, <fpage>4864</fpage>&#x02013;<lpage>4875</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20643781</pub-id></mixed-citation></ref><ref id="b7"><mixed-citation publication-type="journal"><name><surname>Kleinberg</surname><given-names>J.</given-names></name>
<article-title>An Impossibility Theorem for Clustering</article-title>. <source>Adv Neural Inf Process Syst</source> (2002)&#x0003c;<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://papers.nips.cc/paper/2340-an-impossibility-theorem-for-clustering">http://papers.nips.cc/paper/2340-an-impossibility-theorem-for-clustering</ext-link>&#x0003e; (Accessed on 08/07/2014).</mixed-citation></ref><ref id="b8"><mixed-citation publication-type="journal"><name><surname>Handl</surname><given-names>J.</given-names></name>, <name><surname>Knowles</surname><given-names>J.</given-names></name> &#x00026; <name><surname>Kell</surname><given-names>D. B.</given-names></name>
<article-title>Computational cluster validation in post-genomic data analysis</article-title>. <source>Bioinformatics</source>
<volume>21</volume>, <fpage>3201</fpage>&#x02013;<lpage>3212</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15914541</pub-id></mixed-citation></ref><ref id="b9"><mixed-citation publication-type="journal"><name><surname>Monti</surname><given-names>S.</given-names></name>, <name><surname>Tamayo</surname><given-names>P.</given-names></name>, <name><surname>Mesirov</surname><given-names>J.</given-names></name> &#x00026; <name><surname>Golub</surname><given-names>T.</given-names></name>
<article-title>Consensus clustering: A resampling-based method for class discovery and visualization of gene expression microarray data</article-title>. <source>Machine Learning</source>
<volume>52</volume>, <fpage>91</fpage>&#x02013;<lpage>118</lpage> (<year>2003</year>).</mixed-citation></ref><ref id="b10"><mixed-citation publication-type="journal">Cancer Genome Atlas Research Network. <article-title>Integrated genomic analyses of ovarian carcinoma</article-title>. <source>Nature</source>
<volume>474</volume>, <fpage>609</fpage>&#x02013;<lpage>615</lpage> (<year>2011</year>).<pub-id pub-id-type="pmid">21720365</pub-id></mixed-citation></ref><ref id="b11"><mixed-citation publication-type="journal">Cancer Genome Atlas Research Network. <article-title>Comprehensive molecular portraits of human breast tumours</article-title>. <source>Nature</source>
<volume>490</volume>, <fpage>61</fpage>&#x02013;<lpage>70</lpage> (<year>2012</year>).<pub-id pub-id-type="pmid">23000897</pub-id></mixed-citation></ref><ref id="b12"><mixed-citation publication-type="journal">Cancer Genome Atlas Research Network. <italic>et al.</italic>
<article-title>Integrated genomic characterization of endometrial carcinoma</article-title>. <source>Nature</source>
<volume>497</volume>, <fpage>67</fpage>&#x02013;<lpage>73</lpage> (<year>2013</year>).<pub-id pub-id-type="pmid">23636398</pub-id></mixed-citation></ref><ref id="b13"><mixed-citation publication-type="journal">Cancer Genome Atlas Research Network. <article-title>Comprehensive genomic characterization defines human glioblastoma genes and core pathways</article-title>. <source>Nature</source>
<volume>455</volume>, <fpage>1061</fpage>&#x02013;<lpage>1068</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">18772890</pub-id></mixed-citation></ref><ref id="b14"><mixed-citation publication-type="journal"><name><surname>Verhaak</surname><given-names>R. G.</given-names></name>
<italic>et al.</italic>
<article-title>Integrated genomic analysis identifies clinically relevant subtypes of glioblastoma characterized by abnormalities in PDGFRA, IDH1, EGFR, and NF1</article-title>. <source>Cancer Cell</source>
<volume>17</volume>, <fpage>98</fpage>&#x02013;<lpage>110</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20129251</pub-id></mixed-citation></ref><ref id="b15"><mixed-citation publication-type="journal"><name><surname>Dudoit</surname><given-names>S.</given-names></name> &#x00026; <name><surname>Fridlyand</surname><given-names>J.</given-names></name>
<article-title>A prediction-based resampling method for estimating the number of clusters in a dataset</article-title>. <source>Genome Biology</source>
<volume>3</volume>, <fpage>research/0036.0031</fpage>-<lpage>0021</lpage> (<year>2002</year>).</mixed-citation></ref><ref id="b16"><mixed-citation publication-type="journal"><name><surname>Rousseeuw</surname><given-names>P. J.</given-names></name>
<article-title>Silhouettes - a Graphical Aid to the Interpretation and Validation of Cluster-Analysis</article-title>. <source>J Comput Appl Math</source>
<volume>20</volume>, <fpage>53</fpage>&#x02013;<lpage>65</lpage> (<year>1987</year>).</mixed-citation></ref><ref id="b17"><mixed-citation publication-type="journal"><name><surname>Tibshirani</surname><given-names>R.</given-names></name>, <name><surname>Walther</surname><given-names>G.</given-names></name> &#x00026; <name><surname>Hastie</surname><given-names>T.</given-names></name>
<article-title>Estimating the number of clusters in a data set via the gap statistic</article-title>. <source>J R Stat Soc Series B Stat Methodol</source>
<volume>63</volume>, <fpage>411</fpage>&#x02013;<lpage>423</lpage> (<year>2001</year>).</mixed-citation></ref><ref id="b18"><mixed-citation publication-type="journal"><name><surname>Cline</surname><given-names>M. S.</given-names></name>
<italic>et al.</italic>
<article-title>Exploring TCGA Pan-Cancer data at the UCSC Cancer Genomics Browser</article-title>. <source>Sci Rep</source>
<volume>3</volume>, <fpage>2652</fpage> (<year>2013</year>).<pub-id pub-id-type="pmid">24084870</pub-id></mixed-citation></ref><ref id="b19"><mixed-citation publication-type="journal"><name><surname>Smolkin</surname><given-names>M.</given-names></name> &#x00026; <name><surname>Ghosh</surname><given-names>D.</given-names></name>
<article-title>Cluster stability scores for microarray data in cancer studies</article-title>. <source>BMC Bioinformatics</source>
<volume>4</volume>, <fpage>36</fpage> (<year>2003</year>).<pub-id pub-id-type="pmid">12959646</pub-id></mixed-citation></ref><ref id="b20"><mixed-citation publication-type="journal"><name><surname>de Souto</surname><given-names>M. C.</given-names></name>, <name><surname>Costa</surname><given-names>I. G.</given-names></name>, <name><surname>de Araujo</surname><given-names>D. S.</given-names></name>, <name><surname>Ludermir</surname><given-names>T. B.</given-names></name> &#x00026; <name><surname>Schliep</surname><given-names>A.</given-names></name>
<article-title>Clustering cancer gene expression data: a comparative study</article-title>. <source>BMC Bioinformatics</source>
<volume>9</volume>, <fpage>497</fpage> (<year>2008</year>).<pub-id pub-id-type="pmid">19038021</pub-id></mixed-citation></ref><ref id="b21"><mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>E. Y.</given-names></name>, <name><surname>Kim</surname><given-names>S. Y.</given-names></name>, <name><surname>Ashlock</surname><given-names>D.</given-names></name> &#x00026; <name><surname>Nam</surname><given-names>D.</given-names></name>
<article-title>MULTI-K: accurate classification of microarray subtypes using ensemble k-means clustering</article-title>. <source>BMC Bioinformatics</source>
<volume>10</volume>, <fpage>260</fpage> (<year>2009</year>).<pub-id pub-id-type="pmid">19698124</pub-id></mixed-citation></ref><ref id="b22"><mixed-citation publication-type="journal"><name><surname>Bertoni</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Valentini</surname><given-names>G.</given-names></name>
<article-title>Model order selection for bio-molecular data clustering</article-title>. <source>BMC Bioinformatics</source>
<volume>8 Suppl 2</volume>, <fpage>S7</fpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17493256</pub-id></mixed-citation></ref><ref id="b23"><mixed-citation publication-type="journal"><name><surname>Lange</surname><given-names>T.</given-names></name>, <name><surname>Roth</surname><given-names>V.</given-names></name>, <name><surname>Braun</surname><given-names>M. L.</given-names></name> &#x00026; <name><surname>Buhmann</surname><given-names>J. M.</given-names></name>
<article-title>Stability-based validation of clustering solutions</article-title>. <source>Neural Comput</source>
<volume>16</volume>, <fpage>1299</fpage>&#x02013;<lpage>1323</lpage> (<year>2004</year>).<pub-id pub-id-type="pmid">15130251</pub-id></mixed-citation></ref><ref id="b24"><mixed-citation publication-type="journal"><name><surname>Iam-on</surname><given-names>N.</given-names></name>, <name><surname>Boongoen</surname><given-names>T.</given-names></name> &#x00026; <name><surname>Garrett</surname><given-names>S.</given-names></name>
<article-title>LCE: a link-based cluster ensemble method for improved gene expression data analysis</article-title>. <source>Bioinformatics</source>
<volume>26</volume>, <fpage>1513</fpage>&#x02013;<lpage>1519</lpage> (<year>2010</year>).<pub-id pub-id-type="pmid">20444838</pub-id></mixed-citation></ref><ref id="b25"><mixed-citation publication-type="journal"><name><surname>Phillips</surname><given-names>H. S.</given-names></name>
<italic>et al.</italic>
<article-title>Molecular subclasses of high-grade glioma predict prognosis, delineate a pattern of disease progression, and resemble stages in neurogenesis</article-title>. <source>Cancer Cell</source>
<volume>9</volume>, <fpage>157</fpage>&#x02013;<lpage>173</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16530701</pub-id></mixed-citation></ref><ref id="b26"><mixed-citation publication-type="journal"><name><surname>Yu</surname><given-names>Z.</given-names></name>, <name><surname>Wong</surname><given-names>H. S.</given-names></name> &#x00026; <name><surname>Wang</surname><given-names>H.</given-names></name>
<article-title>Graph-based consensus clustering for class discovery from gene expression data</article-title>. <source>Bioinformatics</source>
<volume>23</volume>, <fpage>2888</fpage>&#x02013;<lpage>2896</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">17872912</pub-id></mixed-citation></ref><ref id="b27"><mixed-citation publication-type="journal"><name><surname>Ben-Hur</surname><given-names>A.</given-names></name>, <name><surname>Elisseeff</surname><given-names>A.</given-names></name> &#x00026; <name><surname>Guyon</surname><given-names>I.</given-names></name>
<article-title>A stability based method for discovering structure in clustered data</article-title>. <source>Pac Symp Biocomput</source>
<volume>7</volume>, <fpage>6</fpage>&#x02013;<lpage>17</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">11928511</pub-id></mixed-citation></ref><ref id="b28"><mixed-citation publication-type="journal"><name><surname>Swift</surname><given-names>S.</given-names></name>
<italic>et al.</italic>
<article-title>Consensus clustering and functional interpretation of gene-expression data</article-title>. <source>Genome Biol</source>
<volume>5</volume>, <fpage>R94</fpage> (<year>2004</year>).<pub-id pub-id-type="pmid">15535870</pub-id></mixed-citation></ref><ref id="b29"><mixed-citation publication-type="journal"><name><surname>Fred</surname><given-names>A. L.</given-names></name> &#x00026; <name><surname>Jain</surname><given-names>A. K.</given-names></name>
<article-title>Combining multiple clusterings using evidence accumulation</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>
<volume>27</volume>, <fpage>835</fpage>&#x02013;<lpage>850</lpage> (<year>2005</year>).<pub-id pub-id-type="pmid">15943417</pub-id></mixed-citation></ref><ref id="b30"><mixed-citation publication-type="journal"><name><surname>McShane</surname><given-names>L. M.</given-names></name>
<italic>et al.</italic>
<article-title>Methods for assessing reproducibility of clustering patterns observed in analyses of microarray data</article-title>. <source>Bioinformatics</source>
<volume>18</volume>, <fpage>1462</fpage>&#x02013;<lpage>1469</lpage> (<year>2002</year>).<pub-id pub-id-type="pmid">12424117</pub-id></mixed-citation></ref><ref id="b31"><mixed-citation publication-type="journal"><name><surname>Yeung</surname><given-names>K. Y.</given-names></name>, <name><surname>Haynor</surname><given-names>D. R.</given-names></name> &#x00026; <name><surname>Ruzzo</surname><given-names>W. L.</given-names></name>
<article-title>Validating clustering for gene expression data</article-title>. <source>Bioinformatics</source>
<volume>17</volume>, <fpage>309</fpage>&#x02013;<lpage>318</lpage> (<year>2001</year>).<pub-id pub-id-type="pmid">11301299</pub-id></mixed-citation></ref><ref id="b32"><mixed-citation publication-type="journal"><name><surname>Ben-David</surname><given-names>S.</given-names></name>, <name><surname>von Luxburg</surname><given-names>U.</given-names></name> &#x00026; <name><surname>Pal</surname><given-names>D.</given-names></name>
<source>in Learning Theory: Lecture Notes in Computer Science</source>
<volume>Vol. 4005</volume>, <fpage>5</fpage>&#x02013;<lpage>19</lpage> (Springer, <year>2006</year>).</mixed-citation></ref><ref id="b33"><mixed-citation publication-type="journal"><name><surname>Beroukhim</surname><given-names>R.</given-names></name>
<italic>et al.</italic>
<article-title>Assessing the significance of chromosomal aberrations in cancer: methodology and application to glioma</article-title>. <source>Proc Natl Acad Sci U S A</source>
<volume>104</volume>, <fpage>20007</fpage>&#x02013;<lpage>20012</lpage> (<year>2007</year>).<pub-id pub-id-type="pmid">18077431</pub-id></mixed-citation></ref><ref id="b34"><mixed-citation publication-type="journal"><name><surname>Murat</surname><given-names>A.</given-names></name>
<italic>et al.</italic>
<article-title>Stem cell-related "self-renewal" signature and high epidermal growth factor receptor expression associated with resistance to concomitant chemoradiotherapy in glioblastoma</article-title>. <source>J Clin Oncol</source>
<volume>26</volume>, <fpage>3015</fpage>&#x02013;<lpage>3024</lpage> (<year>2008</year>).<pub-id pub-id-type="pmid">18565887</pub-id></mixed-citation></ref><ref id="b35"><mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>L.</given-names></name>
<italic>et al.</italic>
<article-title>Neuronal and glioma-derived stem cell factor induces angiogenesis within the brain</article-title>. <source>Cancer Cell</source>
<volume>9</volume>, <fpage>287</fpage>&#x02013;<lpage>300</lpage> (<year>2006</year>).<pub-id pub-id-type="pmid">16616334</pub-id></mixed-citation></ref><ref id="b36"><mixed-citation publication-type="journal"><name><surname>MacQueen</surname><given-names>J.</given-names></name>
<article-title>in <italic>Proc</italic></article-title>. <source>Fifth Berkeley Symp. on Math. Statist. and Prob.</source>
<volume>Vol. 1</volume>, <fpage>281</fpage>&#x02013;<lpage>297</lpage> (Univ. of Calif. Press, <year>1967</year>).</mixed-citation></ref><ref id="b37"><mixed-citation publication-type="journal"><name><surname>Venables</surname><given-names>W. N.</given-names></name> &#x00026; <name><surname>Ripley</surname><given-names>B. D.</given-names></name>
<source>Modern Applied Statistics with S.</source> Fourth edn, (Springer, New York, <year>2002</year>).</mixed-citation></ref><ref id="b38"><mixed-citation publication-type="journal"><name><surname>Iam-on</surname><given-names>N.</given-names></name> &#x00026; <name><surname>Garrett</surname><given-names>S.</given-names></name>
<article-title>LinkCluE: A MATLAB Package for Link-Based Cluster Ensembles</article-title>. <source>J Stat Softw</source>
<volume>36</volume>, (<year>2010</year>).&#x0003c;<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="http://www.jstatsoft.org/v36/i09">http://www.jstatsoft.org/v36/i09</ext-link> &#x0003e; (Accessed on 06/07/2014).</mixed-citation></ref></ref-list></back><floats-group><fig id="f1"><label>Figure 1</label><caption><title>Different ways to visualize clustering strength in GBM1.</title><p>(a) gene-subsampling consensus heatmap with K = 4, (b) sample-subsampling consensus heatmap with K = 4, (c) sample-sample Pearson's correlation coefficient heatmap in the same order as in a, showing less crisp clustering patterns, (d) four clusters found by k-means clustering with k = 4, visualized by PC1, PC2, and PC3 (along the x-, y-, and z-axis, respectively). The variances explained by PC1-PC2-PC3 are 21.6%, 9.9%, and 7.9%, respectively. The color scale on the heatmaps ranges from 0 (blue) to 1 (red) and is the same throughout the paper unless otherwise stated.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f1"/></fig><fig id="f2"><label>Figure 2</label><caption><title>Consensus heatmaps show apparent clusters even for samples in unimodal distributions.</title><p>The top panels show <italic>Circle1</italic> sample clusters with k-means partitioning for K = 2&#x02013;5 (in a&#x02013;d), displayed with PC1 (17.7%) on the x-axis and PC2 (15.1%) on the y-axis. The bottom panels show consensus heatmaps for K = 2&#x02013;5 with 80% sample subsampling and k-means as the base method. The top panels in (e&#x02013;h) show <italic>Square1</italic> sample clusters with k-means partitioning for K = 2&#x02013;5 displayed on PC1 (21.8%) and PC2 (19.1%). The bottom panels show consensus heatmaps for K = 2&#x02013;5 with 80% sample subsampling and k-means as the base method.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f2"/></fig><fig id="f3"><label>Figure 3</label><caption><title>CC shows stable clusters in a simulated dataset (Sim25) that carries gene-gene correlation but lacks known clusters.</title><p>(a) sample-sample correlation heatmaps, showing weak or inconsistent clustering, (b) 80% gene-subsampling consensus heatmaps, (c) 80% sample-subsampling consensus heatmaps. Sim25 is chosen as a representative random dataset from the pcNormal null distribution. For each K in 2&#x02013;5 (show from left to right), the order of samples on all three heatmaps is the one obtained from average-linkage hierarchical clustering on the gene-subsampling consensus matrix (in b).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f3"/></fig><fig id="f4"><label>Figure 4</label><caption><title>Difficulties in finding the true K.</title><p>The four columns from left to right are for (1) a randomly generated unimodal dataset, (2) a 2-way pull-apart dataset with degree of pull-apart <italic>a</italic> = 0.08, (3) a 3-way pull-apart dataset with <italic>a</italic> = 0.12, and (4) a 4-way pull-apart dataset with <italic>a</italic> = 0.12. The first row (a1&#x02013;a4): CDF plots from the consensus matrices. CDF curves for K = 2&#x02013;6 are shown in black, red, green, blue and cyan, respectively. The second row (b1&#x02013;b4): &#x00394;(K) plots across K = 2&#x02013;6. An elbow occurs at K = 4 in all plots suggesting an optimal K of 4. The third row (c1&#x02013;c4): GAP plots across K = 2&#x02013;6. In all four plots the optimal K value according to the original interpretation is 3. The fourth row (d1&#x02013;d4): CLEST plots across K = 2&#x02013;6. The decision criterion involving <italic>d<sup>k</sup></italic> suggest an optimal K of 1, 2, 3, and 5 in these four cases, respectively.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f4"/></fig><fig id="f5"><label>Figure 5</label><caption><title>Consensus clustering diagnostic plots on the Alizadeh et al. dataset.</title><p>The dataset contains 62 samples from three histopathologic classes (DLBCL, CLL, FL) and 2,093 probes. Shown are heatmaps for sample-sample correlation coefficient matrix (a,c) and CC matrix (b, d) for K = 2 (a&#x02013;b) and K = 3 (c&#x02013;d). The consensus heatmap for K = 2 shows crisp clusters, while at K = 3 additional structure is seen. The CDF plot shows a flat middle segment for K = 2 (e), however the &#x00394;(<italic>K</italic>) plot has an elbow at K = 4 (f).</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f5"/></fig><fig id="f6"><label>Figure 6</label><caption><title>The ability to identify K is better for PAC than other methods.</title><p>Identifiability graphs for (a) PAC, (b) &#x00394;(K), (c) CLEST, (d) GAP-PC with the original decision rule, (e) GAP-PC with a modified decision rule, and (f) silhouette width. The x-axis shows a, the degree of the pull-apart signal, i.e., the cluster strength. The y-axis shows K, the true number of clusters. The colors in the bars indicate estimated K values for the corresponding (K, <italic>a</italic>) pair. The length of each color in a given bar is proportional to the frequency of inferring a particular K value in the set of 50 simulations.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f6"/></fig><fig id="f7"><label>Figure 7</label><caption><title>Overall summary: PAC outperforms other methods.</title><p>The overall accuracy of each tested method is defined as the percentage of correct K calls averaged over K = 2&#x02013;6 and <italic>a</italic> in [0,0.8]. For LCE only the best of 12 parameter combinations (shown in <xref ref-type="supplementary-material" rid="s1">Supplementary Figure 5b</xref>) is shown.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f7"/></fig><fig id="f8"><label>Figure 8</label><caption><title>The gene-sample signatures from the learning set (Sim25) are preserved even when the test sets have no known clusters.</title><p>(a) The heatmap of four sample groups (1&#x02013;4) and four groups of most discriminant genes (A&#x02013;D) discovered by k-means clustering of Sim25 with K = 4. (b) Heatmaps for nine datasets similarly simulated as Sim25. The x-axis shows the samples as partitioned into 4 clusters with k-means, and the y-axis shows the same "most discriminant genes" from Sim25. These nine null test datasets were able to show the same placement of the gene-sample blocks as in Sim25.</p></caption><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="srep06207-f8"/></fig></floats-group></article>